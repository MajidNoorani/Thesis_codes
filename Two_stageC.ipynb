{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries.\n",
    "import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import keras\n",
    "from keras import regularizers\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.initializers import glorot_normal\n",
    "# import sounddevice as sd\n",
    "import tensorflow as tf\n",
    "from mir_eval import separation \n",
    "from pystoi.stoi import stoi \n",
    "import h5py\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries.\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "import time\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "# from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_DNNC(y_true, y_pred, Lambda = 0.05):\n",
    "    loss = K.sum(K.square(y_true - y_pred)) \n",
    "    - Lambda * K.sum(K.square(y_true[0:257,:] - y_pred[0:257,:]) + K.square(y_true[257:,:] - y_pred[257:,:]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(wave,angle):\n",
    "    recon1 = wave*np.cos(angle)+wave*np.sin(angle)*1j\n",
    "#     recon = np.sqrt(np.power(10, wave))\n",
    "#     recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=160, win_length=400, window='hann')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_series = str(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 295522)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('TIMIT/Organized/Two_stage_sec_set/predicted_sec_set.hdf5','r')\n",
    "X = h5f['predicted_sec_set'][:]\n",
    "h5f.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 295522)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('TIMIT/Organized/concatenated/clean'  + data_series + '_sec.hdf5','r')\n",
    "y = h5f['clean'  + data_series + '_sec'][:]\n",
    "h5f.close()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55383, 257)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('TIMIT/Organized/concatenated/valid_mixed'  + data_series + '.hdf5','r')\n",
    "X_valid = h5f['valid_mixed'  + data_series ][:]\n",
    "h5f.close()\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55383, 514)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('TIMIT/Organized/concatenated/valid_clean'  + data_series + '.hdf5','r')\n",
    "y_valid = h5f['valid_clean'  + data_series ][:]\n",
    "h5f.close()\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.121167555"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing.normalize(X, norm='l2', axis=0, copy=True)\n",
    "y_train = preprocessing.normalize(y, norm='l2', axis=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = preprocessing.normalize(X_valid, norm='l2', axis=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_A = load_model('Models/Two_stage/trained_model19.h5')\n",
    "prediction = estimator_A.predict(X_valid)\n",
    "prediction0 = np.multiply(X_valid, prediction[:,0:257])\n",
    "prediction1 = np.multiply(X_valid, prediction[:,257:])\n",
    "X_valid = np.concatenate((prediction0, prediction1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = preprocessing.normalize(X_valid, norm='l2', axis=0, copy=True)\n",
    "y_valid = preprocessing.normalize(y_valid, norm='l2', axis=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 295522)\n",
      "(514, 295522)\n",
      "(55383, 514)\n",
      "(55383, 514)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim= 514\n",
    "y_dim = 514\n",
    "h_C = [1028, 1028, 1028]\n",
    "def DNN_C():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(h_C[0], input_dim = X_dim, kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(h_C[1], kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(h_C[2], kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.add(Dense(y_dim, kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_DNNC, optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^^^^^^^^^^^^^^^^^^ Training on mixed5^^^^^^^^^^^^^^^^^^^^^\n",
      "Train on 295522 samples, validate on 55383 samples\n",
      "Epoch 1/200\n",
      "295522/295522 [==============================] - 26s 88us/step - loss: 5380.3147 - val_loss: 1268.3572\n",
      "Epoch 2/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1738.0682 - val_loss: 178.6418\n",
      "Epoch 3/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1497.9548 - val_loss: 67.3805\n",
      "Epoch 4/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1458.3881 - val_loss: 77.9630\n",
      "Epoch 5/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1438.8661 - val_loss: 73.1857\n",
      "Epoch 6/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1426.8275 - val_loss: 70.0564\n",
      "Epoch 7/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1417.4443 - val_loss: 94.8682\n",
      "Epoch 8/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1410.3849 - val_loss: 79.4999\n",
      "Epoch 9/200\n",
      "295522/295522 [==============================] - 26s 89us/step - loss: 1403.4441 - val_loss: 57.4320\n",
      "Epoch 10/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1397.6258 - val_loss: 78.7456\n",
      "Epoch 11/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1392.7185 - val_loss: 76.7533\n",
      "Epoch 12/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1388.3869 - val_loss: 93.8341\n",
      "Epoch 13/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1384.2452 - val_loss: 79.3103\n",
      "Epoch 14/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1380.0830 - val_loss: 99.8360\n",
      "Epoch 15/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1376.5104 - val_loss: 93.1160\n",
      "Epoch 16/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1372.9107 - val_loss: 95.4235\n",
      "Epoch 17/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1369.8011 - val_loss: 79.2191\n",
      "Epoch 18/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1366.5114 - val_loss: 93.8859\n",
      "Epoch 19/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1362.7816 - val_loss: 54.4510\n",
      "Epoch 20/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1360.0664 - val_loss: 88.2939\n",
      "Epoch 21/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1357.2457 - val_loss: 91.7688\n",
      "Epoch 22/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1355.2260 - val_loss: 96.4071\n",
      "Epoch 23/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1351.8593 - val_loss: 91.9796\n",
      "Epoch 24/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1349.4359 - val_loss: 105.0232\n",
      "Epoch 25/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1346.4961 - val_loss: 71.2700\n",
      "Epoch 26/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1344.1861 - val_loss: 87.8854\n",
      "Epoch 27/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1341.2425 - val_loss: 137.4807\n",
      "Epoch 28/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1338.9137 - val_loss: 114.0201\n",
      "Epoch 29/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1336.7585 - val_loss: 78.3679\n",
      "Epoch 30/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1334.4445 - val_loss: 136.4911\n",
      "Epoch 31/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1332.1242 - val_loss: 95.2931\n",
      "Epoch 32/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1329.5815 - val_loss: 160.9178\n",
      "Epoch 33/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1327.4786 - val_loss: 154.7008\n",
      "Epoch 34/200\n",
      "295522/295522 [==============================] - 26s 89us/step - loss: 1325.2437 - val_loss: 112.0143\n",
      "Epoch 35/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1322.7961 - val_loss: 49.3192\n",
      "Epoch 36/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1321.0371 - val_loss: 100.4056\n",
      "Epoch 37/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1318.7436 - val_loss: 88.0137\n",
      "Epoch 38/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1317.4419 - val_loss: 93.4058\n",
      "Epoch 39/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1314.5631 - val_loss: 113.1780\n",
      "Epoch 40/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1312.6010 - val_loss: 86.6959\n",
      "Epoch 41/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1310.5571 - val_loss: 83.5126\n",
      "Epoch 42/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1308.7245 - val_loss: 130.6023\n",
      "Epoch 43/200\n",
      "295522/295522 [==============================] - 23s 79us/step - loss: 1306.6712 - val_loss: 264.6384\n",
      "Epoch 44/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1304.7341 - val_loss: 111.4300\n",
      "Epoch 45/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1303.0826 - val_loss: 203.7206\n",
      "Epoch 46/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1300.2932 - val_loss: 112.4495\n",
      "Epoch 47/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1298.5435 - val_loss: 128.3745\n",
      "Epoch 48/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1296.9931 - val_loss: 92.5948\n",
      "Epoch 49/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1294.9959 - val_loss: 296.2266\n",
      "Epoch 50/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1292.8950 - val_loss: 101.0211\n",
      "Epoch 51/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1291.7529 - val_loss: 96.8215\n",
      "Epoch 52/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1289.3599 - val_loss: 122.6989\n",
      "Epoch 53/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1287.6019 - val_loss: 364.2437\n",
      "Epoch 54/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1285.9453 - val_loss: 103.0106\n",
      "Epoch 55/200\n",
      "295522/295522 [==============================] - 26s 86us/step - loss: 1284.1562 - val_loss: 57.3471\n",
      "Epoch 56/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1282.8305 - val_loss: 185.0613\n",
      "Epoch 57/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1281.1042 - val_loss: 173.1874\n",
      "Epoch 58/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1279.2919 - val_loss: 75.7761\n",
      "Epoch 59/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1277.3691 - val_loss: 129.1780\n",
      "Epoch 60/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1276.1756 - val_loss: 341.5604\n",
      "Epoch 61/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1274.1073 - val_loss: 85.9299\n",
      "Epoch 62/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1272.4492 - val_loss: 166.8246\n",
      "Epoch 63/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1271.6670 - val_loss: 129.8563\n",
      "Epoch 64/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1269.8501 - val_loss: 104.7108\n",
      "Epoch 65/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1268.4641 - val_loss: 281.5642\n",
      "Epoch 66/200\n",
      "295522/295522 [==============================] - 26s 86us/step - loss: 1266.6738 - val_loss: 78.4069\n",
      "Epoch 67/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1264.9163 - val_loss: 159.6819\n",
      "Epoch 68/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1263.6285 - val_loss: 134.0714\n",
      "Epoch 69/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1262.3637 - val_loss: 129.6984\n",
      "Epoch 70/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1260.5418 - val_loss: 263.9452\n",
      "Epoch 71/200\n",
      "295522/295522 [==============================] - 26s 88us/step - loss: 1259.9771 - val_loss: 63.7113\n",
      "Epoch 72/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1258.5366 - val_loss: 157.7297\n",
      "Epoch 73/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1257.4278 - val_loss: 82.9020\n",
      "Epoch 74/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1255.5421 - val_loss: 84.5655\n",
      "Epoch 75/200\n",
      "295522/295522 [==============================] - 26s 88us/step - loss: 1253.8398 - val_loss: 75.0951\n",
      "Epoch 76/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1253.3726 - val_loss: 120.4102\n",
      "Epoch 77/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1251.9385 - val_loss: 436.1895\n",
      "Epoch 78/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1250.3630 - val_loss: 398.5532\n",
      "Epoch 79/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1249.0299 - val_loss: 159.7744\n",
      "Epoch 80/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1248.0417 - val_loss: 270.7434\n",
      "Epoch 81/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1246.9618 - val_loss: 101.7157\n",
      "Epoch 82/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1245.3670 - val_loss: 111.9342\n",
      "Epoch 83/200\n",
      "295522/295522 [==============================] - 26s 88us/step - loss: 1244.1743 - val_loss: 63.7907\n",
      "Epoch 84/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1242.9469 - val_loss: 324.4315\n",
      "Epoch 85/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1241.6755 - val_loss: 82.5059\n",
      "Epoch 86/200\n",
      "295522/295522 [==============================] - 23s 79us/step - loss: 1241.2593 - val_loss: 36.1714\n",
      "Epoch 87/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1239.5820 - val_loss: 690.3569\n",
      "Epoch 88/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1238.5611 - val_loss: 328.1013\n",
      "Epoch 89/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1237.0621 - val_loss: 139.2717\n",
      "Epoch 90/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1236.9642 - val_loss: 117.6777\n",
      "Epoch 91/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1235.1957 - val_loss: 73.1848\n",
      "Epoch 92/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1233.9532 - val_loss: 105.2349\n",
      "Epoch 93/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1233.2152 - val_loss: 687.4383\n",
      "Epoch 94/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1232.3206 - val_loss: 383.1043\n",
      "Epoch 95/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1230.7193 - val_loss: 50.8812\n",
      "Epoch 96/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1229.6791 - val_loss: 105.9606\n",
      "Epoch 97/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1229.2459 - val_loss: 226.4698\n",
      "Epoch 98/200\n",
      "295522/295522 [==============================] - 26s 90us/step - loss: 1227.6784 - val_loss: 62.9451\n",
      "Epoch 99/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1227.0482 - val_loss: 202.2641\n",
      "Epoch 100/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1225.5645 - val_loss: 393.3465\n",
      "Epoch 101/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1225.0375 - val_loss: 124.4641\n",
      "Epoch 102/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1223.8065 - val_loss: 124.6138\n",
      "Epoch 103/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1223.0533 - val_loss: 413.8492\n",
      "Epoch 104/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1222.5193 - val_loss: 138.0468\n",
      "Epoch 105/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1221.2983 - val_loss: 123.2141\n",
      "Epoch 106/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1220.8096 - val_loss: 147.5944\n",
      "Epoch 107/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1219.6942 - val_loss: 174.9792\n",
      "Epoch 108/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1218.4493 - val_loss: 101.4254\n",
      "Epoch 109/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1217.9411 - val_loss: 91.8577\n",
      "Epoch 110/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1216.9965 - val_loss: 557.1387\n",
      "Epoch 111/200\n",
      "295522/295522 [==============================] - 26s 89us/step - loss: 1215.8409 - val_loss: 305.6010\n",
      "Epoch 112/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1214.9760 - val_loss: 452.3025\n",
      "Epoch 113/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1214.0231 - val_loss: 205.9749\n",
      "Epoch 114/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1213.5421 - val_loss: 61.9517\n",
      "Epoch 115/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1212.0099 - val_loss: 132.5154\n",
      "Epoch 116/200\n",
      "295522/295522 [==============================] - 26s 86us/step - loss: 1211.9334 - val_loss: 184.7373\n",
      "Epoch 117/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1210.8125 - val_loss: 156.2605\n",
      "Epoch 118/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1210.1217 - val_loss: 96.6526\n",
      "Epoch 119/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1209.4828 - val_loss: 32.1234\n",
      "Epoch 120/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1208.4866 - val_loss: 158.4664\n",
      "Epoch 121/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1207.6404 - val_loss: 43.9319\n",
      "Epoch 122/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1206.7388 - val_loss: 712.4088\n",
      "Epoch 123/200\n",
      "295522/295522 [==============================] - 26s 89us/step - loss: 1205.8688 - val_loss: 191.9854\n",
      "Epoch 124/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1205.1400 - val_loss: 60.0013\n",
      "Epoch 125/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1204.1962 - val_loss: 77.3764\n",
      "Epoch 126/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1203.5156 - val_loss: 253.7154\n",
      "Epoch 127/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1202.4065 - val_loss: 36.3291\n",
      "Epoch 128/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1202.0870 - val_loss: 278.7149\n",
      "Epoch 129/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1201.5648 - val_loss: 88.4851\n",
      "Epoch 130/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1201.0023 - val_loss: 85.4321\n",
      "Epoch 131/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1199.7710 - val_loss: 307.1048\n",
      "Epoch 132/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1199.5923 - val_loss: 345.7561\n",
      "Epoch 133/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1198.1135 - val_loss: 55.6425\n",
      "Epoch 134/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1197.3046 - val_loss: 92.7910\n",
      "Epoch 135/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1197.0921 - val_loss: 172.0001\n",
      "Epoch 136/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1196.6611 - val_loss: 95.7918\n",
      "Epoch 137/200\n",
      "295522/295522 [==============================] - 23s 79us/step - loss: 1195.4957 - val_loss: 427.2822\n",
      "Epoch 138/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1194.3657 - val_loss: 73.2746\n",
      "Epoch 139/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1193.8296 - val_loss: 87.5791\n",
      "Epoch 140/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1193.3575 - val_loss: 111.4721\n",
      "Epoch 141/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1192.4040 - val_loss: 542.4139\n",
      "Epoch 142/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295522/295522 [==============================] - 25s 83us/step - loss: 1191.7685 - val_loss: 147.3500\n",
      "Epoch 143/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1191.4238 - val_loss: 83.0429\n",
      "Epoch 144/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1190.6132 - val_loss: 72.6543\n",
      "Epoch 145/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1189.6052 - val_loss: 394.0444\n",
      "Epoch 146/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1189.3451 - val_loss: 128.2043\n",
      "Epoch 147/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1188.5902 - val_loss: 348.3904\n",
      "Epoch 148/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1187.4650 - val_loss: 172.8352\n",
      "Epoch 149/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1187.2429 - val_loss: 232.4129\n",
      "Epoch 150/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1186.7851 - val_loss: 168.2574\n",
      "Epoch 151/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1185.6327 - val_loss: 707.4340\n",
      "Epoch 152/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1185.5353 - val_loss: 299.5310\n",
      "Epoch 153/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1184.7012 - val_loss: 154.3008\n",
      "Epoch 154/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1184.0829 - val_loss: 127.1463\n",
      "Epoch 155/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1183.2198 - val_loss: 255.3554\n",
      "Epoch 156/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1182.8470 - val_loss: 101.3727\n",
      "Epoch 157/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1182.4279 - val_loss: 77.7610\n",
      "Epoch 158/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1181.2224 - val_loss: 90.5273\n",
      "Epoch 159/200\n",
      "295522/295522 [==============================] - 25s 83us/step - loss: 1180.5467 - val_loss: 269.1107\n",
      "Epoch 160/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1180.2294 - val_loss: 74.7966\n",
      "Epoch 161/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1180.3607 - val_loss: 150.6396\n",
      "Epoch 162/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1178.6403 - val_loss: 125.1965\n",
      "Epoch 163/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1178.2815 - val_loss: 129.8109\n",
      "Epoch 164/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1178.1254 - val_loss: 337.1059\n",
      "Epoch 165/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1176.5391 - val_loss: 119.1408\n",
      "Epoch 166/200\n",
      "295522/295522 [==============================] - 27s 90us/step - loss: 1176.7764 - val_loss: 206.9201\n",
      "Epoch 167/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1176.1413 - val_loss: 236.0485\n",
      "Epoch 168/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1175.3045 - val_loss: 193.0095\n",
      "Epoch 169/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1174.9107 - val_loss: 60.4617\n",
      "Epoch 170/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1174.9548 - val_loss: 89.2328\n",
      "Epoch 171/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1173.6114 - val_loss: 252.6755\n",
      "Epoch 172/200\n",
      "295522/295522 [==============================] - 24s 80us/step - loss: 1173.0716 - val_loss: 283.5297\n",
      "Epoch 173/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1173.1114 - val_loss: 546.0649\n",
      "Epoch 174/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1171.3545 - val_loss: 137.4579\n",
      "Epoch 175/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1171.6210 - val_loss: 77.9862\n",
      "Epoch 176/200\n",
      "295522/295522 [==============================] - 26s 86us/step - loss: 1170.9646 - val_loss: 107.8140\n",
      "Epoch 177/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1170.9618 - val_loss: 124.6798\n",
      "Epoch 178/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1169.9576 - val_loss: 729.3618\n",
      "Epoch 179/200\n",
      "295522/295522 [==============================] - 26s 88us/step - loss: 1170.0537 - val_loss: 434.4603\n",
      "Epoch 180/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1168.9123 - val_loss: 209.9695\n",
      "Epoch 181/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1168.3465 - val_loss: 301.2002\n",
      "Epoch 182/200\n",
      "295522/295522 [==============================] - 25s 85us/step - loss: 1168.3729 - val_loss: 252.1197\n",
      "Epoch 183/200\n",
      "295522/295522 [==============================] - 24s 83us/step - loss: 1167.1850 - val_loss: 272.8556\n",
      "Epoch 184/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1166.7189 - val_loss: 61.1334\n",
      "Epoch 185/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1166.2198 - val_loss: 106.8491\n",
      "Epoch 186/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1166.0496 - val_loss: 37.3342\n",
      "Epoch 187/200\n",
      "295522/295522 [==============================] - 26s 88us/step - loss: 1166.0067 - val_loss: 200.3872\n",
      "Epoch 188/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1164.6762 - val_loss: 197.1330\n",
      "Epoch 189/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1163.8901 - val_loss: 305.2243\n",
      "Epoch 190/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1163.6792 - val_loss: 150.6757\n",
      "Epoch 191/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1163.3294 - val_loss: 154.2204\n",
      "Epoch 192/200\n",
      "295522/295522 [==============================] - 26s 87us/step - loss: 1162.6399 - val_loss: 129.6560\n",
      "Epoch 193/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1162.0924 - val_loss: 135.7289\n",
      "Epoch 194/200\n",
      "295522/295522 [==============================] - 23s 79us/step - loss: 1161.8382 - val_loss: 406.3093\n",
      "Epoch 195/200\n",
      "295522/295522 [==============================] - 24s 82us/step - loss: 1161.4051 - val_loss: 67.5227\n",
      "Epoch 196/200\n",
      "295522/295522 [==============================] - 26s 89us/step - loss: 1161.2309 - val_loss: 246.8491\n",
      "Epoch 197/200\n",
      "295522/295522 [==============================] - 25s 84us/step - loss: 1160.4152 - val_loss: 119.1919\n",
      "Epoch 198/200\n",
      "295522/295522 [==============================] - 25s 86us/step - loss: 1159.8524 - val_loss: 147.7955\n",
      "Epoch 199/200\n",
      "295522/295522 [==============================] - 24s 81us/step - loss: 1159.9117 - val_loss: 521.0996\n",
      "Epoch 200/200\n",
      "295522/295522 [==============================] - 27s 91us/step - loss: 1158.5626 - val_loss: 65.9531\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "tic = time.time()\n",
    "estimator_C = KerasRegressor(build_fn=DNN_C, epochs = 200, batch_size=batch_size, shuffle = True, verbose=1)\n",
    "# kfold = KFold(n_splits=2, random_state=None)\n",
    "# kfold = KFold(n_splits=5)\n",
    "# results = cross_val_score(estimator_A, X.T, y.T, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "# toc_fold = time.time()\n",
    "\n",
    "print(\"^^^^^^^^^^^^^^^^^^^^^ Training on mixed5^^^^^^^^^^^^^^^^^^^^^\")\n",
    "# filepath=\"half_trained.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 0, save_best_only = True, mode = 'min') \n",
    "# callbacks_list = [checkpoint]\n",
    "history = estimator_C.fit(X.T, y.T, validation_data=(X_valid, y_valid), shuffle = True, batch_size=batch_size,  steps_per_epoch=None)\n",
    "toc_adam = time.time()\n",
    "# print(\"---------------------------second part of training---------------------------\")\n",
    "# new_model = load_model(filepath)\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save?_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "# new_model.optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# new_model.fit(X.T, Mask.T, shuffle = True, epochs = 100, batch_size=100,  steps_per_epoch=None, callbacks=callbacks_list)\n",
    "# estimator_A.fit (X.T, y.T, shuffle = True, batch_size=100,  steps_per_epoch=None, callbacks = [changer])\n",
    "# prediction = estimator_A.predict(X_test)\n",
    "\n",
    "toc_fit = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_C.model.save('Models/Two_stage/Model0_C0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time=  82.56514866749446\n"
     ]
    }
   ],
   "source": [
    "print('Time= ', (toc_fit - tic)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_series = str(4)\n",
    "test_point_start0 = 0\n",
    "test_point_stop0 = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 1000)\n",
      "(257, 1000)\n",
      "(514, 1000)\n",
      "(257, 1000)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('TIMIT/Organized/Two_stage_sec_set/predicted_sec_set.hdf5','r')\n",
    "test_input = h5f['predicted_sec_set'][:, test_point_start0 : test_point_stop0]\n",
    "h5f.close()\n",
    "print(test_input.shape)\n",
    "h5f = h5py.File('TIMIT/Organized/concatenated/phase' + data_series + '_sec.hdf5','r')\n",
    "angle_mixed = h5f['phase' + data_series + '_sec'][:, test_point_start0 : test_point_stop0]\n",
    "h5f.close()\n",
    "print(angle_mixed.shape)\n",
    "h5f = h5py.File('TIMIT/Organized/concatenated/clean' + data_series + '_sec.hdf5','r')\n",
    "test_target = h5f['clean' + data_series + '_sec'][:, test_point_start0 : test_point_stop0]\n",
    "h5f.close()\n",
    "print(test_target.shape)\n",
    "h5f = h5py.File('TIMIT/Organized/concatenated/phase' + data_series + '_sec.hdf5','r')\n",
    "angle_clean = h5f['phase' + data_series + '_sec'][:, test_point_start0 : test_point_stop0]\n",
    "h5f.close()\n",
    "print(angle_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = preprocessing.normalize(test_input, norm='l2', axis=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(test_input[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimator_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-93d191f296d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'estimator_C' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = estimator_C.predict(test_input.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
