{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Thesis_Func as Func\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# from shapely.geometry import Polygon, LineString\n",
    "# import descartes\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import wave\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from natsort import natsorted\n",
    "\n",
    "# import keras\n",
    "from keras.models import load_model ,Model\n",
    "from keras import optimizers\n",
    "# from keras.layers import Lambda, Input, Dense, Dropout\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Lambda\n",
    "# from tensorflow.keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "# from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] initialization\n",
    "system_Name = 'labtop'\n",
    "if system_Name == 'labtop':\n",
    "    timit_addr = 'E:\\\\Arshad\\\\Database\\\\TIMIT'\n",
    "    label_directory = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_label'\n",
    "    name_directory = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_name'\n",
    "    MRCG_directory = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG'\n",
    "    vad_model_save_addr = 'E:\\\\Arshad\\\\Thesis\\\\model\\\\VAD\\\\'\n",
    "    enhance_model_save_addr = 'E:\\\\Arshad\\\\Thesis\\\\model\\\\Enhance\\\\'\n",
    "    \n",
    "    \n",
    "    train_label_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_label\\\\silence_add_train_label.h5'\n",
    "    test_label_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_label\\\\silence_add_test_label.h5'\n",
    "    \n",
    "\n",
    "    train_silence_add_clean_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\train\\\\train_silence_add_clean.h5'\n",
    "    test_silence_add_clean_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\test\\\\test_silence_add_clean.h5'\n",
    "        \n",
    "        \n",
    "    train_data_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\train\\\\train_silence_add_factory1_SNR 0.h5'\n",
    "    train_data_addr2 = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\train\\\\train_silence_add_babble_SNR 0.h5'\n",
    "    \n",
    "    \n",
    "    test_data_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\test\\\\test_silence_add_factory1_SNR 0.h5'\n",
    "    test_data_addr2 = 'E:\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\test\\\\test_silence_add_babble_SNR 0.h5'\n",
    "    \n",
    "    \n",
    "    train_predict_label_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\predict_label\\\\4_batch256_train_silence_add_factory1_SNR 0.h5'\n",
    "    train_predict_label_addr2 = 'E:\\\\Arshad\\\\Database\\\\my database\\\\predict_label\\\\4_batch256_train_silence_add_babble_SNR 0.h5'\n",
    "    predict_label_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\predict_label\\\\'\n",
    "    \n",
    "    test_predict_label_addr = 'E:\\\\Arshad\\\\Database\\\\my database\\\\predict_label\\\\4_batch256_test_silence_add_factory1_SNR 0.h5'\n",
    "    test_predict_label_addr2 = 'E:\\\\Arshad\\\\Database\\\\my database\\\\predict_label\\\\4_batch256_test_silence_add_babble_SNR 0.h5'\n",
    "    \n",
    "elif system_Name == 'pc':\n",
    "    timit_addr = 'E:\\\\iman\\\\Arshad\\\\Database\\\\TIMIT'\n",
    "    label_directory = 'E:\\\\iman\\\\Arshad\\\\Database\\\\my database\\\\timit_label'\n",
    "    name_directory = 'E:\\\\iman\\\\Arshad\\\\Database\\\\my database\\\\timit_name'\n",
    "    MRCG_directory = 'E:\\\\iman\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG'\n",
    "\n",
    "    train_data_addr = 'E:\\\\iman\\\\Arshad\\\\Database\\\\my database\\\\timit_MRCG\\\\train\\\\train_data_M5.h5'\n",
    "    train_label_addr = 'E:\\\\iman\\\\Arshad\\\\Database\\\\my database\\\\timit_label\\\\train_label_2x.h5'\n",
    "    model_save_addr = 'E:\\\\iman\\\\Arshad\\\\Thesis\\\\model\\\\VAD\\\\'\n",
    "\n",
    "elif system_Name == 'gpu1':\n",
    "    train_silence_add_clean_addr = '/home/aut_speech/iman/Arshad/Database/my database/timit_MRCG/train/train_silence_add_clean.h5'\n",
    "    vad_model_save_addr = '/home/aut_speech/iman/Arshad/Thesis/model/VAD/'\n",
    "    enhance_model_save_addr = '/home/aut_speech/iman/Arshad/Thesis/model/enhance/'\n",
    "    \n",
    "    \n",
    "    train_data_addr = '/home/aut_speech/iman/Arshad/Database/my database/timit_MRCG/train/train_silence_add_factory1_SNR 0.h5'\n",
    "    train_data_addr2 = '/home/aut_speech/iman/Arshad/Database/my database/timit_MRCG/train/train_silence_add_babble_SNR 0.h5'\n",
    "    train_label_addr = '/home/aut_speech/iman/Arshad/Database/my database/timit_label/silence_add_train_label.h5'\n",
    "    \n",
    "elif system_Name == 'gpu2':\n",
    "    need_to_be_set = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg_jae(Requirements,all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, batch_size, mini_batch_size, train_data_len, window_size):\n",
    "#     all_noisy_train_data_addr='all_noisy_train_data_addr'\n",
    "#     clean_train_data_addr='clean_train_data_addr'\n",
    "#     train_label_addr='train_label_addr'\n",
    "#     batch_size='batch_size'\n",
    "#     mini_batch_size='mini_batch_size'\n",
    "#     train_data_len='train_data_len'\n",
    "#     window_size='window_size'\n",
    "    global I\n",
    "    h5f = {}\n",
    "    for i in range(len(all_noisy_train_data_addr)):\n",
    "        h5f[i] = h5py.File(all_noisy_train_data_addr[i],'r')\n",
    "#         print(h5f[i]['train_data'].shape)\n",
    "    h5f_clean_train_data = h5py.File(clean_train_data_addr,'r')\n",
    "    h5f_train_label = h5py.File(train_label_addr,'r')\n",
    "    \n",
    "    indx = np.arange(0, train_data_len, mini_batch_size)\n",
    "    file_Nu = list(range(len(all_noisy_train_data_addr)))\n",
    "    all_data_mini_batch_flag = np.asarray([[n, i] for n in file_Nu for i in indx])\n",
    "    if 'train' in Requirements:\n",
    "        all_data_mini_batch_flag = shuffle(all_data_mini_batch_flag, random_state=0)\n",
    "#     print(all_data_mini_batch_flag[:10])\n",
    "#     print(all_data_mini_batch_flag.shape)\n",
    "    \n",
    "    first_mini_batch_noisy = 'True'\n",
    "    first_mini_batch_clean = 'True'\n",
    "    first_mini_batch_label = 'True'\n",
    "    while True:             \n",
    "        for I in all_data_mini_batch_flag:\n",
    "#             print(I)\n",
    "            if 'noisy' in Requirements:\n",
    "                if train_data_len < mini_batch_size + window_size + I[1]:        \n",
    "                    temp = h5f[I[0]]['train_data'][:,I[1]-window_size:]\n",
    "                    temp = np.concatenate((temp,np.repeat(np.reshape(temp[:,0:1],(96,1)),[window_size],axis=1)), axis=1)\n",
    "                elif I[1] < window_size:\n",
    "                    temp = h5f[I[0]]['train_data'][:,I[1]:I[1]+mini_batch_size+window_size] \n",
    "                    temp = np.concatenate((np.repeat(np.reshape(temp[:,0:1],(96,1)),[window_size],axis=1),temp), axis=1)\n",
    "                else:                               \n",
    "                    temp = h5f[I[0]]['train_data'][:,I[1]-window_size:I[1]+mini_batch_size+window_size]\n",
    "                \n",
    "                if first_mini_batch_noisy == 'True':\n",
    "                    first_mini_batch_noisy = 'False'\n",
    "                    r = np.reshape(temp[:,:window_size*2+1], (96*(window_size*2+1),1))\n",
    "                    in_data = (r-np.min(r))/(np.max(r)-np.min(r)+0.00001)\n",
    "                    for i in range(1,temp.shape[1]-2*window_size):\n",
    "                        r = np.reshape(temp[:,i:i+window_size*2+1].T,(96*(window_size*2+1),1))\n",
    "                        in_data = np.concatenate((in_data,(r-np.min(r))/(np.max(r)-np.min(r)+0.00001)),axis = 1)   \n",
    "                else:\n",
    "                    for i in range(0,temp.shape[1]-2*window_size):\n",
    "                        r = np.reshape(temp[:,i:i+window_size*2+1].T,(96*(window_size*2+1),1))\n",
    "                        in_data = np.concatenate((in_data,(r-np.min(r))/(np.max(r)-np.min(r)+0.00001)),axis = 1)\n",
    "            \n",
    "            if 'clean' in Requirements or 'clean_target' in Requirements:\n",
    "                if train_data_len < mini_batch_size + window_size + I[1]: \n",
    "                    temp2 = h5f_clean_train_data['train_data'][:,I[1]-window_size:]\n",
    "                    temp2 = np.concatenate((temp2,np.repeat(np.reshape(temp2[:,0:1],(96,1)),[window_size],axis=1)), axis=1)\n",
    "                elif I[1] < window_size:\n",
    "                    temp2 = h5f_clean_train_data['train_data'][:,I[1]:I[1]+mini_batch_size+window_size] \n",
    "                    temp2 = np.concatenate((np.repeat(np.reshape(temp2[:,0:1],(96,1)),[window_size],axis=1),temp2), axis=1)\n",
    "                else:                               \n",
    "                    temp2 = h5f_clean_train_data['train_data'][:,I[1]-window_size:I[1]+mini_batch_size+window_size]\n",
    "                if first_mini_batch_clean == 'True':\n",
    "                    first_mini_batch_clean = 'False'\n",
    "                    s = np.reshape(temp2[:,:window_size*2+1], (96*(window_size*2+1),1))\n",
    "                    out_data = (s-np.min(s))/(np.max(s)-np.min(s)+0.00001)\n",
    "                    for i in range(1,temp2.shape[1]-2*window_size):\n",
    "                        s = np.reshape(temp2[:,i:i+window_size*2+1].T,(96*(window_size*2+1),1))\n",
    "                        out_data = np.concatenate((out_data,(s-np.min(s))/(np.max(s)-np.min(s)+0.00001)),axis = 1)\n",
    "                else:\n",
    "                    for i in range(0,temp2.shape[1]-2*window_size):\n",
    "                        s = np.reshape(temp2[:,i:i+window_size*2+1].T,(96*(window_size*2+1),1))\n",
    "                        out_data = np.concatenate((out_data,(s-np.min(s))/(np.max(s)-np.min(s)+0.00001)),axis = 1)\n",
    "            \n",
    "            \n",
    "            if 'label' in Requirements:\n",
    "                if train_data_len < mini_batch_size + I[1]: \n",
    "                    temp3 = h5f_train_label['train_label'][:,I[1]:]\n",
    "                else:                               \n",
    "                    temp3 = h5f_train_label['train_label'][:,I[1]:I[1]+mini_batch_size]\n",
    "                if first_mini_batch_label == 'True':\n",
    "                    first_mini_batch_label = 'False'\n",
    "                    y_int = temp3\n",
    "                else:\n",
    "                    y_int = np.concatenate((y_int,temp3), axis = 1)\n",
    "                    \n",
    "            if (batch_size - in_data.shape[1]) < mini_batch_size:\n",
    "                if 'noisy' in Requirements:\n",
    "                    in_data_scale = in_data.T\n",
    "                if 'clean' in Requirements or 'clean_target' in Requirements:\n",
    "                    out_data_scale = out_data.T\n",
    "                if 'label' in Requirements:\n",
    "                    train_label_batch = to_categorical(y_int.T)\n",
    "                    \n",
    "                first_mini_batch_noisy = 'True'\n",
    "                first_mini_batch_clean = 'True'\n",
    "                first_mini_batch_label = 'True'\n",
    "#                 preprocessing.normalize(train_data_batch,norm='l2',)\n",
    "#                 in_data_scale = preprocessing.scale(in_data.T)\n",
    "#                 out_data_scale = preprocessing.scale(out_data.T)                    \n",
    "#                 print(in_data_scale)\n",
    "#                 print(out_data_scale)\n",
    "#                 print(np.argwhere(np.isnan(in_data_scale)))\n",
    "#                 print(np.argwhere(np.isnan(out_data_scale)))\n",
    "                if ('label' in Requirements and train_label_batch.shape[1]==2) or 'label' not in Requirements:\n",
    "                    out = []\n",
    "                    if 'noisy' in Requirements:\n",
    "                        out.append(in_data_scale)\n",
    "                    if 'clean' in Requirements:\n",
    "                        out.append(out_data_scale)\n",
    "                    if 'clean_target' in Requirements:\n",
    "                        out.append(out_data_scale[:,96*window_size:96*(window_size+1)])\n",
    "                    if 'label' in Requirements:\n",
    "                        out.append(train_label_batch)\n",
    "                    yield out\n",
    "                else:\n",
    "                    print('data_fail  ','index == ' , I)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape (512, 2016)\n",
      "(512, 2)\n"
     ]
    }
   ],
   "source": [
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_label_addr = train_label_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 512\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 50\n",
    "data_dg_joint=dg_jae(['train','noisy','label'],all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, \n",
    "                     batch_size, mini_batch_size, train_data_len, window_size)\n",
    "for i in range(1):\n",
    "    q,e= next(data_dg_joint)\n",
    "    print('q shape' , q.shape)\n",
    "    print(e.shape)\n",
    "#     print(e.shape)\n",
    "#     print(w.shape)\n",
    "#     print(q)\n",
    "#     print(w)\n",
    "#     plt.matshow(q.T);\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "#     plt.matshow(w.T);\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "#     plt.matshow(e.T);\n",
    "#     plt.colorbar()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0719 15:26:44.611736  7944 deprecation_wrapper.py:119] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0719 15:26:45.582132  7944 deprecation_wrapper.py:119] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0719 15:26:45.897939  7944 deprecation_wrapper.py:119] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0719 15:26:46.188755  7944 deprecation_wrapper.py:119] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0719 15:26:46.792382  7944 deprecation_wrapper.py:119] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0719 15:26:46.891319  7944 deprecation_wrapper.py:119] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   16/14680 [..............................] - ETA: 5:00:31 - loss: 0.0322 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-9bba15d87042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_Nu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                           verbose=1)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mtoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DNN enhancement separate training\n",
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_label_addr = train_label_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 128\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 10\n",
    "h = [512, 512]\n",
    "\n",
    "\n",
    "X_dim = 96*(window_size*2+1)\n",
    "y_dim = 96\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim=X_dim, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(h[1], kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(y_dim, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "opt = optimizers.adam(lr=0.00001)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "tic = time.clock()\n",
    "model.fit_generator(generator=dg_jae(['train','noisy','clean_target'],all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, batch_size, mini_batch_size, train_data_len, window_size),\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                          epochs=epochs_Nu,\n",
    "                          verbose=1)\n",
    "toc = time.clock()\n",
    "print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "\n",
    "model.save(enhance_model_save_addr + '55_MRCG96_win10_batch256_mini128_e50_512sig_512sig_sig.h5')\n",
    "\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data_addr = [train_data_addr] # NEED TO REWRITE\n",
    "# all_noisy_train_data_addr = [train_data_addr]\n",
    "# test_data_len = 685845 * len(all_test_data_addr)\n",
    "test_data_len = 1879087 * len(all_test_data_addr)\n",
    "# test_data_len = ???\n",
    "batch_size = 256\n",
    "steps = int((test_data_len/batch_size)-1)\n",
    "window_size = 10\n",
    "\n",
    "model = load_model(model_save_addr + '4_MRCG96_win10_batch256_mini128_e200_512sigmoid_512sigmoid.h5')\n",
    "\n",
    "tic = time.clock()\n",
    "ynew = model.predict_generator(dg_jae(all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, batch_size, mini_batch_size, train_data_len, window_size),steps)\n",
    "print(ynew.shape)\n",
    "\n",
    "toc = time.clock()\n",
    "print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "\n",
    "with h5py.File(predict_label_addr +'4_batch'+str(batch_size)+'_'+all_test_data_addr[0].split('\\\\')[-1].split('.')[0]+'.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('label', data=ynew)\n",
    "    h5f.close()\n",
    "\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 16:41:00.886944  8392 training_utils.py:1101] Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [array([[0.7538522 , 0.74719505, 0.75829417, ..., 0.6194392 , 0.61854727,\n        0.61719367],\n       [0.74719505, 0.84315685, 0.84289994, ..., 0.61595412, 0.61595412,\n        0.61595412],\n       [0.75829417, 0.8458044 , 0.85107519, ..., 0.61557532, 0.61557532,\n        0.61557532],\n       ...,\n       [0.60862391, 0.67869013, 0.69121866, ..., 0.37482666, 0.37482666,\n        0.37482666],\n       [0.61729137, 0.69065343, 0.70625734, ..., 0.40065116, 0.40065116,\n        0.40065116],\n       [0.61868178, 0.69330231, 0.70800897, ..., 0.41375352, 0.41375352,\n        0.41375352]]), array([[0.64170196, 0.64468494, 0.64567785, ..., 0.63860712, 0.63980698,\n        0.64194542],\n       [0.64468494, 0.72551845, 0.74917899, ..., 0.64408613, 0.64408613,\n        0.64408613],\n       [0.64567785, 0.71928315, 0.74560663, ..., 0.64515613, 0.64515613,\n        0.64515613],\n       ...,\n       [0.53349712, 0.59846053, 0.60972573, ..., 0.48778199, 0.48778199,\n        0.48778199],\n       [0.52895562, 0.60077039, 0.60385357, ..., 0.49930872, 0.49930872,\n        0.49930872],\n       [0.50851802, 0.60621678, 0.61187989, ..., 0.50351829, 0.50351829,\n        0.50351829]])]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-63ace5d60a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_Nu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                           verbose=1)\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[0mtoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m       \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[1;34m(generator, mode)\u001b[0m\n\u001b[0;32m    370\u001b[0m       raise ValueError('Output of generator should be '\n\u001b[0;32m    371\u001b[0m                        \u001b[1;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m                        'or `(x, y)`. Found: ' + str(generator_output))\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [array([[0.7538522 , 0.74719505, 0.75829417, ..., 0.6194392 , 0.61854727,\n        0.61719367],\n       [0.74719505, 0.84315685, 0.84289994, ..., 0.61595412, 0.61595412,\n        0.61595412],\n       [0.75829417, 0.8458044 , 0.85107519, ..., 0.61557532, 0.61557532,\n        0.61557532],\n       ...,\n       [0.60862391, 0.67869013, 0.69121866, ..., 0.37482666, 0.37482666,\n        0.37482666],\n       [0.61729137, 0.69065343, 0.70625734, ..., 0.40065116, 0.40065116,\n        0.40065116],\n       [0.61868178, 0.69330231, 0.70800897, ..., 0.41375352, 0.41375352,\n        0.41375352]]), array([[0.64170196, 0.64468494, 0.64567785, ..., 0.63860712, 0.63980698,\n        0.64194542],\n       [0.64468494, 0.72551845, 0.74917899, ..., 0.64408613, 0.64408613,\n        0.64408613],\n       [0.64567785, 0.71928315, 0.74560663, ..., 0.64515613, 0.64515613,\n        0.64515613],\n       ...,\n       [0.53349712, 0.59846053, 0.60972573, ..., 0.48778199, 0.48778199,\n        0.48778199],\n       [0.52895562, 0.60077039, 0.60385357, ..., 0.49930872, 0.49930872,\n        0.49930872],\n       [0.50851802, 0.60621678, 0.61187989, ..., 0.50351829, 0.50351829,\n        0.50351829]])]"
     ]
    }
   ],
   "source": [
    "#Auto encoder enhancement separate training\n",
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_label_addr = train_label_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 128\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 10\n",
    "h = [512, 512]\n",
    "\n",
    "X_dim = 96*(window_size*2+1)\n",
    "y_dim = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# network parameters\n",
    "input_shape = (X_dim, )\n",
    "output_shape = X_dim\n",
    "intermediate_dim = 512\n",
    "latent_dim = 64\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = tf.keras.layers.Input(shape=input_shape, name='encoder_input')\n",
    "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = tf.keras.models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# encoder.summary()\n",
    "# tf.keras.utils.plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = tf.keras.layers.Dense(output_shape, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = tf.keras.models.Model(latent_inputs, outputs, name='decoder')\n",
    "# decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "# true_output = tf.keras.layers.Input(shape=output_shape, name='true_output')\n",
    "vae = tf.keras.models.Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "models = (encoder, decoder)\n",
    "# data = (x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
    "\n",
    "# reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n",
    "\n",
    "# reconstruction_loss *= input_shape\n",
    "\n",
    "kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
    "kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "# opt = optimizers.adam(lr=0.00001)\n",
    "vae.compile(optimizer='adam', metrics=['accuracy'])\n",
    "# vae.summary()\n",
    "\n",
    "\n",
    "\n",
    "tic = time.clock()\n",
    "vae.fit_generator(generator=dg_jae(['noisy','clean'],all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, batch_size, mini_batch_size, train_data_len, window_size),\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                          epochs=epochs_Nu,\n",
    "                          verbose=1)\n",
    "toc = time.clock()\n",
    "print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "\n",
    "vae.save(enhance_model_save_addr + '55_MRCG96_win10_batch128_mini128_e50_512sig_512sig_sig.h5')\n",
    "\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch_NU: 1/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 10/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 20/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 30/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 40/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 50/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 60/14680 Minibatch Loss: nan  nan\n",
      "Step 1: Minibatch_NU: 70/14680 Minibatch Loss: nan  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e544432646df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0min_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dg_joint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[1;31m#         print(in_data,'------')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m#         print(denoise_data,'------')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-2792eb38f98f>\u001b[0m in \u001b[0;36mdg_train_jae\u001b[1;34m(all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, batch_size, mini_batch_size, train_data_len, window_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrain_data_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mmini_batch_size\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mtemp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f_clean_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfirst_mini_batch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'True'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# Patch up the output for NumPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Single-field recarray convention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_label_addr = train_label_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 128\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 50\n",
    "# q = next(dg_train_ae())\n",
    "# print(q)\n",
    "# print(type(q))\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "num_steps = 30000\n",
    "display_step = 10\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 512 # 1st layer num features\n",
    "num_hidden_2 = 512 # 2nd layer num features \n",
    "num_hidden_3 = 64 # 3nd layer num features (the latent dim)\n",
    "num_input = 96*(window_size*2+1) # MNIST data input (img shape: 28*28)\n",
    "\n",
    "num_hidden_1_vad = 96*(window_size*2+1)\n",
    "num_hidden_2_vad = 96*(window_size*2+1)\n",
    "labels_dim = 2\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "true_clean_X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "# true_clean_X = clean_X\n",
    "# output = tf.placeholder(\"float\", [None, labels_dim], name='labels')\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'encoder_h_logvar': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_3])),\n",
    "    'encoder_h_mean': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_3])),\n",
    "    \n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_3, num_hidden_2])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h_logvar': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "    'decoder_h_mean': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "    'decoder_h_output': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "    \n",
    "    'vad_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1_vad])),\n",
    "    'vad_h2': tf.Variable(tf.random_normal([num_hidden_1_vad, num_hidden_2_vad])),\n",
    "    'vad_hout': tf.Variable(tf.random_normal([num_hidden_2_vad, 2]))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'encoder_b_logvar': tf.Variable(tf.random_normal([num_hidden_3])),\n",
    "    'encoder_b_mean': tf.Variable(tf.random_normal([num_hidden_3])),\n",
    "    \n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b_logvar': tf.Variable(tf.random_normal([num_input])),\n",
    "    'decoder_b_mean': tf.Variable(tf.random_normal([num_input])),\n",
    "    'decoder_b_output': tf.Variable(tf.random_normal([num_input])),\n",
    "    \n",
    "    'vad_b1': tf.Variable(tf.random_normal([num_hidden_1_vad])),\n",
    "    'vad_b2': tf.Variable(tf.random_normal([num_hidden_2_vad])),\n",
    "    'vad_bout': tf.Variable(tf.random_normal([labels_dim]))\n",
    "}\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(X):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    \n",
    "    layer_z_logvar = tf.maximum(tf.add(tf.matmul(layer_2, weights['encoder_h_logvar']),\n",
    "                                   biases['encoder_b_logvar']),10e-9)\n",
    "    layer_z_mean = tf.add(tf.matmul(layer_2, weights['encoder_h_mean']),\n",
    "                                   biases['encoder_b_mean'])\n",
    "    z = Lambda(sampling, output_shape=(num_hidden_3,), name='z')([layer_z_mean, layer_z_logvar])\n",
    "    return z, layer_z_logvar, layer_z_mean\n",
    "    \n",
    "\n",
    "# Building the decoder\n",
    "def decoder(z):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(z, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    \n",
    "    layer_x_logvar = tf.maximum(tf.add(tf.matmul(layer_2, weights['decoder_h_logvar']),\n",
    "                                   biases['decoder_b_logvar']),10e-9)\n",
    "    layer_x_mean = tf.add(tf.matmul(layer_2, weights['decoder_h_mean']),\n",
    "                                   biases['decoder_b_mean'])\n",
    "    denoise_X = Lambda(sampling, output_shape=(num_input,), name='result_x')([layer_x_mean, layer_x_logvar])\n",
    "#     denoise_X = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['decoder_h_output']), biases['decoder_b_output']))\n",
    "    return denoise_X, layer_x_logvar ,layer_x_mean\n",
    "\n",
    "def vad(MRCG,output):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(MRCG, weights['vad_h1']),\n",
    "                                   biases['vad_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['vad_h2']),\n",
    "                                   biases['vad_b2']))\n",
    "    output = tf.nn.softmax(tf.add(tf.matmul(layer_2, weights['vad_hout']),\n",
    "                                   biases['vad_bout']))\n",
    "    return output\n",
    "\n",
    "# Construct model\n",
    "z, z_logvar, z_mean = encoder(X)\n",
    "pred_clean_X, x_logvar ,x_mean = decoder(z)\n",
    "# pred_clean_X = decoder(z)\n",
    "# pred_label = vad(denoise_X, output)\n",
    "\n",
    "# Prediction\n",
    "# y_pred = result_x\n",
    "# Targets (Labels) are the input data.\n",
    "# y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "# calculate binary cross entropy\n",
    "def binary_cross_entropy(actual, predicted):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_score += actual[i] * log(1e-15 + predicted[i])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    return -mean_sum_score\n",
    "\n",
    "def categorical_cross_entropy(actual, predicted):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_score += actual[i][j] * log(1e-15 + predicted[i][j])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    return -mean_sum_score\n",
    "\n",
    "# loss_VAD = tf.keras.losses.categorical_crossentropy(output,pred_label)    \n",
    "loss_SE =  tf.reduce_mean(K.sum(1 + z_logvar - K.square(z_mean) - K.exp(z_logvar))- K.sum(0.5*x_logvar + K.square(pred_clean_X-x_mean)/(2*K.exp(x_logvar))))\n",
    "loss_SE2 = tf.reduce_mean(tf.pow(true_clean_X - pred_clean_X, 2))\n",
    "\n",
    "# final_loss = tf.reduce_mean(loss1+loss2)\n",
    "optimizer_SE = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_SE)\n",
    "optimizer_SE2 = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_SE2)\n",
    "# optimizer_VAD = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_VAD)\n",
    "# final_train_op = tf.group([optimizer_SE, optimizer_VAD])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([X,true_clean_X])\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "# print(dataset.output_types)\n",
    "# print(dataset.output_shapes)\n",
    "data_dg_joint=dg_train_jae(all_noisy_train_data_addr, clean_train_data_addr, train_label_addr, batch_size, mini_batch_size, train_data_len, window_size)\n",
    "\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1,epochs_Nu+1):\n",
    "        tic = time.clock()\n",
    "        for j in range(1,steps_per_epoch+1):\n",
    "            in_data,clean_data,labels = next(data_dg_joint)\n",
    "    #         print(in_data,'------')\n",
    "    #         print(denoise_data,'------')\n",
    "            a,b,c = sess.run([optimizer_SE,loss_SE,loss_SE2], feed_dict={ X: in_data, true_clean_X: clean_data })\n",
    "            \n",
    "    #         print(sess.run(el).shape)\n",
    "            if j % display_step == 0 or j == 1:\n",
    "                print('Step %i: Minibatch_NU: %i/%i Minibatch Loss: %f  %f' % (i, j, steps_per_epoch, b,c))\n",
    "        toc = time.clock()\n",
    "        print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "#         saver.save(sess, 'my_test_model')\n",
    "        saver.save(sess, enhance_model_save_addr+'model_ae_'+str(i)+'.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 18:11:10.742700    92 deprecation.py:506] From c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          401920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 403,972\n",
      "Trainable params: 403,972\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               402192    \n",
      "=================================================================\n",
      "Total params: 403,728\n",
      "Trainable params: 403,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 18:11:11.686905    92 training_utils.py:1101] Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 403972      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 784)          403728      encoder[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_decoder/dense_2/Mat [(None, 784)]        0           dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_decoder/dense_2/Bia [(None, 784)]        0           tf_op_layer_decoder/dense_2/MatMu\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/zeros [(None, 784)]        0           tf_op_layer_decoder/dense_2/BiasA\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/Great [(None, 784)]        0           tf_op_layer_decoder/dense_2/BiasA\n",
      "                                                                 tf_op_layer_logistic_loss/zeros_l\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/Selec [(None, 784)]        0           tf_op_layer_logistic_loss/Greater\n",
      "                                                                 tf_op_layer_decoder/dense_2/BiasA\n",
      "                                                                 tf_op_layer_logistic_loss/zeros_l\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/mul ( [(None, 784)]        0           tf_op_layer_decoder/dense_2/BiasA\n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/sub ( [(None, 784)]        0           tf_op_layer_logistic_loss/Select[\n",
      "                                                                 tf_op_layer_logistic_loss/mul[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/Neg ( [(None, 784)]        0           tf_op_layer_decoder/dense_2/BiasA\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/Selec [(None, 784)]        0           tf_op_layer_logistic_loss/Greater\n",
      "                                                                 tf_op_layer_logistic_loss/Neg[0][\n",
      "                                                                 tf_op_layer_decoder/dense_2/BiasA\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/Exp ( [(None, 784)]        0           tf_op_layer_logistic_loss/Select_\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss/Log1p [(None, 784)]        0           tf_op_layer_logistic_loss/Exp[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss (Tens [(None, 784)]        0           tf_op_layer_logistic_loss/sub[0][\n",
      "                                                                 tf_op_layer_logistic_loss/Log1p[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_2 (TensorFlowO [(None,)]            0           tf_op_layer_logistic_loss[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_2 (TensorFlowOp [(None,)]            0           tf_op_layer_Mean_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_10 (TensorFlowO [(None, 2)]          0           z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_2 (TensorFlo [(None, 2)]          0           z_mean[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_5 (TensorFlowOp [(None, 2)]          0           tf_op_layer_add_10[0][0]         \n",
      "                                                                 tf_op_layer_Square_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_2 (TensorFlowOp [(None, 2)]          0           z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_6 (TensorFlowOp [(None, 2)]          0           tf_op_layer_sub_5[0][0]          \n",
      "                                                                 tf_op_layer_Exp_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None,)]            0           tf_op_layer_sub_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlowOp [(None,)]            0           tf_op_layer_Sum_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_11 (TensorFlowO [(None,)]            0           tf_op_layer_mul_2[0][0]          \n",
      "                                                                 tf_op_layer_mul_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_3 (TensorFlowO [()]                 0           tf_op_layer_add_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              ()                   0           tf_op_layer_Mean_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 807,700\n",
      "Trainable params: 807,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = tf.keras.layers.Input(shape=input_shape, name='encoder_input')\n",
    "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = tf.keras.models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "# tf.keras.utils.plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = tf.keras.layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = tf.keras.models.Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "# tf.keras.utils.plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = tf.keras.models.Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "# reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "\n",
    "kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
    "kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "# tf.keras.utils.plot_model(vae, to_file='vae_mlp.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aut_speech/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Step 1: Minibatch_NU: 1/14680 Minibatch Loss: -inf  107762756406325805056.000000\n",
      "Step 1: Minibatch_NU: 100/14680 Minibatch Loss: nan  3.271753\n",
      "Step 1: Minibatch_NU: 200/14680 Minibatch Loss: nan  3.223250\n",
      "Step 1: Minibatch_NU: 300/14680 Minibatch Loss: nan  3.166612\n",
      "Step 1: Minibatch_NU: 400/14680 Minibatch Loss: nan  3.108583\n",
      "Step 1: Minibatch_NU: 500/14680 Minibatch Loss: nan  3.076584\n",
      "Step 1: Minibatch_NU: 600/14680 Minibatch Loss: nan  3.002919\n",
      "Step 1: Minibatch_NU: 700/14680 Minibatch Loss: nan  3.040681\n",
      "Step 1: Minibatch_NU: 800/14680 Minibatch Loss: nan  2.920647\n",
      "Step 1: Minibatch_NU: 900/14680 Minibatch Loss: nan  2.928104\n",
      "Step 1: Minibatch_NU: 1000/14680 Minibatch Loss: nan  2.878370\n",
      "Step 1: Minibatch_NU: 1100/14680 Minibatch Loss: nan  2.815583\n",
      "Step 1: Minibatch_NU: 1200/14680 Minibatch Loss: nan  2.753686\n",
      "Step 1: Minibatch_NU: 1300/14680 Minibatch Loss: nan  2.744031\n",
      "Step 1: Minibatch_NU: 1400/14680 Minibatch Loss: nan  2.725977\n",
      "Step 1: Minibatch_NU: 1500/14680 Minibatch Loss: nan  2.650580\n",
      "Step 1: Minibatch_NU: 1600/14680 Minibatch Loss: nan  2.601226\n",
      "Step 1: Minibatch_NU: 1700/14680 Minibatch Loss: nan  2.553920\n",
      "Step 1: Minibatch_NU: 1800/14680 Minibatch Loss: nan  2.529398\n",
      "Step 1: Minibatch_NU: 1900/14680 Minibatch Loss: nan  2.485018\n",
      "Step 1: Minibatch_NU: 2000/14680 Minibatch Loss: nan  2.453361\n",
      "Step 1: Minibatch_NU: 2100/14680 Minibatch Loss: nan  2.396412\n",
      "Step 1: Minibatch_NU: 2200/14680 Minibatch Loss: nan  2.352000\n",
      "Step 1: Minibatch_NU: 2300/14680 Minibatch Loss: nan  2.300067\n",
      "Step 1: Minibatch_NU: 2400/14680 Minibatch Loss: nan  2.243458\n",
      "Step 1: Minibatch_NU: 2500/14680 Minibatch Loss: nan  2.204293\n",
      "Step 1: Minibatch_NU: 2600/14680 Minibatch Loss: nan  2.177269\n",
      "Step 1: Minibatch_NU: 2700/14680 Minibatch Loss: nan  2.148882\n",
      "Step 1: Minibatch_NU: 2800/14680 Minibatch Loss: nan  2.102031\n",
      "Step 1: Minibatch_NU: 2900/14680 Minibatch Loss: nan  2.097197\n",
      "Step 1: Minibatch_NU: 3000/14680 Minibatch Loss: nan  2.018251\n",
      "Step 1: Minibatch_NU: 3100/14680 Minibatch Loss: nan  2.003027\n",
      "Step 1: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.989456\n",
      "Step 1: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.924494\n",
      "Step 1: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.868938\n",
      "Step 1: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.840634\n",
      "Step 1: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.832829\n",
      "Step 1: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.770221\n",
      "Step 1: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.732886\n",
      "Step 1: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.724248\n",
      "Step 1: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.694680\n",
      "Step 1: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.661228\n",
      "Step 1: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.674638\n",
      "Step 1: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.620135\n",
      "Step 1: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.580210\n",
      "Step 1: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.543933\n",
      "Step 1: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.544633\n",
      "Step 1: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.498577\n",
      "Step 1: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.474923\n",
      "Step 1: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.447997\n",
      "Step 1: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.430907\n",
      "Step 1: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.399642\n",
      "Step 1: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.403577\n",
      "Step 1: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.378827\n",
      "Step 1: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.365675\n",
      "Step 1: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.332269\n",
      "Step 1: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.303649\n",
      "Step 1: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.301264\n",
      "Step 1: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.280337\n",
      "Step 1: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.282886\n",
      "Step 1: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.256943\n",
      "Step 1: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.235145\n",
      "Step 1: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.222779\n",
      "Step 1: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.218957\n",
      "Step 1: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.199808\n",
      "Step 1: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.192734\n",
      "Step 1: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.177432\n",
      "Step 1: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.185058\n",
      "Step 1: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.149715\n",
      "Step 1: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.147272\n",
      "Step 1: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.135381\n",
      "Step 1: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.133370\n",
      "Step 1: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.126079\n",
      "Step 1: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.111438\n",
      "Step 1: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.103539\n",
      "Step 1: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.111341\n",
      "Step 1: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.093633\n",
      "Step 1: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.084355\n",
      "Step 1: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.080499\n",
      "Step 1: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.071925\n",
      "Step 1: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.077173\n",
      "Step 1: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.068352\n",
      "Step 1: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.067131\n",
      "Step 1: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.058301\n",
      "Step 1: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.054212\n",
      "Step 1: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.053543\n",
      "Step 1: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.052091\n",
      "Step 1: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.047855\n",
      "Step 1: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.042419\n",
      "Step 1: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.039121\n",
      "Step 1: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.043579\n",
      "Step 1: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.040053\n",
      "Step 1: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.039904\n",
      "Step 1: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.028648\n",
      "Step 1: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.031217\n",
      "Step 1: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.027840\n",
      "Step 1: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.028485\n",
      "Step 1: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.027933\n",
      "Step 1: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.031639\n",
      "Step 1: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.019966\n",
      "Step 1: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.018600\n",
      "Step 1: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.017585\n",
      "Step 1: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.016779\n",
      "Step 1: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.029402\n",
      "Step 1: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.022427\n",
      "Step 1: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.022813\n",
      "Step 1: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.017712\n",
      "Step 1: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015600\n",
      "Step 1: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.017782\n",
      "Step 1: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.015551\n",
      "Step 1: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.017525\n",
      "Step 1: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.010746\n",
      "Step 1: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.017263\n",
      "Step 1: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.013161\n",
      "Step 1: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.020124\n",
      "Step 1: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009794\n",
      "Step 1: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.016639\n",
      "Step 1: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.016381\n",
      "Step 1: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.007900\n",
      "Step 1: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.014825\n",
      "Step 1: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.015798\n",
      "Step 1: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.010013\n",
      "Step 1: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.012729\n",
      "Step 1: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012959\n",
      "Step 1: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.013425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.015605\n",
      "Step 1: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012521\n",
      "Step 1: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.010690\n",
      "Step 1: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007951\n",
      "Step 1: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.015579\n",
      "Step 1: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.014862\n",
      "Step 1: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010539\n",
      "Step 1: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.007576\n",
      "Step 1: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009257\n",
      "Step 1: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.024642\n",
      "Step 1: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.008274\n",
      "Step 1: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.006975\n",
      "Step 1: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.015193\n",
      "Step 1: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.007600\n",
      "Step 1: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006598\n",
      "Step 1: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.013272\n",
      "Step 1: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.008033\n",
      "Step 1: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.013825\n",
      "Step 1: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012903\n",
      "Step 1: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.014289\n",
      "Step 1: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.010521\n",
      "Step 1: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.016076\n",
      "time =  0:07:39.775192\n",
      "Step 2: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.008960\n",
      "Step 2: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009998\n",
      "Step 2: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.012407\n",
      "Step 2: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009534\n",
      "Step 2: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011362\n",
      "Step 2: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.006256\n",
      "Step 2: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.015855\n",
      "Step 2: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009487\n",
      "Step 2: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009285\n",
      "Step 2: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011085\n",
      "Step 2: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.007724\n",
      "Step 2: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.013716\n",
      "Step 2: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.016011\n",
      "Step 2: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.005095\n",
      "Step 2: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.007604\n",
      "Step 2: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012592\n",
      "Step 2: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.005329\n",
      "Step 2: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009323\n",
      "Step 2: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.013927\n",
      "Step 2: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.004344\n",
      "Step 2: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.022993\n",
      "Step 2: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009951\n",
      "Step 2: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.010675\n",
      "Step 2: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.011607\n",
      "Step 2: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.011676\n",
      "Step 2: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.014894\n",
      "Step 2: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.002920\n",
      "Step 2: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.011886\n",
      "Step 2: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007439\n",
      "Step 2: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.013492\n",
      "Step 2: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.015711\n",
      "Step 2: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.007048\n",
      "Step 2: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.012491\n",
      "Step 2: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.016114\n",
      "Step 2: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.011404\n",
      "Step 2: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.009055\n",
      "Step 2: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.011437\n",
      "Step 2: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.011891\n",
      "Step 2: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.013157\n",
      "Step 2: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.011021\n",
      "Step 2: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.010669\n",
      "Step 2: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.005865\n",
      "Step 2: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010601\n",
      "Step 2: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.009255\n",
      "Step 2: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012072\n",
      "Step 2: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.009472\n",
      "Step 2: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.016333\n",
      "Step 2: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.003750\n",
      "Step 2: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009737\n",
      "Step 2: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.013688\n",
      "Step 2: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.003850\n",
      "Step 2: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.015879\n",
      "Step 2: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010261\n",
      "Step 2: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.013467\n",
      "Step 2: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.010908\n",
      "Step 2: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.014855\n",
      "Step 2: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014361\n",
      "Step 2: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.016571\n",
      "Step 2: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.012337\n",
      "Step 2: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.011284\n",
      "Step 2: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.014000\n",
      "Step 2: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013320\n",
      "Step 2: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.009065\n",
      "Step 2: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009693\n",
      "Step 2: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.004554\n",
      "Step 2: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.013642\n",
      "Step 2: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.015907\n",
      "Step 2: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.012336\n",
      "Step 2: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010556\n",
      "Step 2: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.006551\n",
      "Step 2: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.011625\n",
      "Step 2: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009564\n",
      "Step 2: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.005925\n",
      "Step 2: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.007114\n",
      "Step 2: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011985\n",
      "Step 2: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.007421\n",
      "Step 2: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.011599\n",
      "Step 2: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.014762\n",
      "Step 2: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.010715\n",
      "Step 2: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012352\n",
      "Step 2: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.010111\n",
      "Step 2: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.013440\n",
      "Step 2: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.010569\n",
      "Step 2: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.010951\n",
      "Step 2: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009298\n",
      "Step 2: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.012197\n",
      "Step 2: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010502\n",
      "Step 2: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.012032\n",
      "Step 2: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010200\n",
      "Step 2: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.012524\n",
      "Step 2: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.011277\n",
      "Step 2: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.015255\n",
      "Step 2: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.009058\n",
      "Step 2: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.016754\n",
      "Step 2: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.006781\n",
      "Step 2: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.007671\n",
      "Step 2: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.009765\n",
      "Step 2: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007701\n",
      "Step 2: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.008408\n",
      "Step 2: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.013273\n",
      "Step 2: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.013822\n",
      "Step 2: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.007962\n",
      "Step 2: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.003613\n",
      "Step 2: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.014505\n",
      "Step 2: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009927\n",
      "Step 2: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008513\n",
      "Step 2: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.012080\n",
      "Step 2: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.008831\n",
      "Step 2: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.009810\n",
      "Step 2: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.013886\n",
      "Step 2: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.004717\n",
      "Step 2: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011016\n",
      "Step 2: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.003941\n",
      "Step 2: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.004939\n",
      "Step 2: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011854\n",
      "Step 2: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.012794\n",
      "Step 2: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.011976\n",
      "Step 2: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.014594\n",
      "Step 2: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.008782\n",
      "Step 2: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.000535\n",
      "Step 2: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.006323\n",
      "Step 2: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.010099\n",
      "Step 2: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.009282\n",
      "Step 2: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006678\n",
      "Step 2: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.004986\n",
      "Step 2: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.005930\n",
      "Step 2: Minibatch_NU: 12700/14680 Minibatch Loss: nan  0.999338\n",
      "Step 2: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.010895\n",
      "Step 2: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.020408\n",
      "Step 2: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.012092\n",
      "Step 2: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.014106\n",
      "Step 2: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.010087\n",
      "Step 2: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.011763\n",
      "Step 2: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.007367\n",
      "Step 2: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.008760\n",
      "Step 2: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011058\n",
      "Step 2: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.006835\n",
      "Step 2: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010355\n",
      "Step 2: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.010837\n",
      "Step 2: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.012234\n",
      "Step 2: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.008402\n",
      "Step 2: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008052\n",
      "Step 2: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009053\n",
      "Step 2: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.006145\n",
      "Step 2: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.007633\n",
      "Step 2: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009213\n",
      "time =  0:07:07.787773\n",
      "Step 3: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.011157\n",
      "Step 3: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.013846\n",
      "Step 3: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.011036\n",
      "Step 3: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.012499\n",
      "Step 3: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.004314\n",
      "Step 3: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.010028\n",
      "Step 3: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.007710\n",
      "Step 3: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.017154\n",
      "Step 3: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009844\n",
      "Step 3: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.001502\n",
      "Step 3: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.014902\n",
      "Step 3: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005103\n",
      "Step 3: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009309\n",
      "Step 3: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.011839\n",
      "Step 3: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.015164\n",
      "Step 3: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.029077\n",
      "Step 3: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.008573\n",
      "Step 3: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.011867\n",
      "Step 3: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.006871\n",
      "Step 3: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011603\n",
      "Step 3: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.007225\n",
      "Step 3: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009201\n",
      "Step 3: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.006590\n",
      "Step 3: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.010341\n",
      "Step 3: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.015991\n",
      "Step 3: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.015574\n",
      "Step 3: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.005227\n",
      "Step 3: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007480\n",
      "Step 3: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.002367\n",
      "Step 3: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011384\n",
      "Step 3: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.008992\n",
      "Step 3: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.016934\n",
      "Step 3: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009389\n",
      "Step 3: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.014386\n",
      "Step 3: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.007344\n",
      "Step 3: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.011567\n",
      "Step 3: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.016329\n",
      "Step 3: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.014693\n",
      "Step 3: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009376\n",
      "Step 3: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009683\n",
      "Step 3: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011826\n",
      "Step 3: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.004624\n",
      "Step 3: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.014892\n",
      "Step 3: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.005865\n",
      "Step 3: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.011199\n",
      "Step 3: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010326\n",
      "Step 3: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007982\n",
      "Step 3: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.010017\n",
      "Step 3: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.006220\n",
      "Step 3: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.013698\n",
      "Step 3: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.011586\n",
      "Step 3: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.005430\n",
      "Step 3: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.008499\n",
      "Step 3: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.008427\n",
      "Step 3: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.009001\n",
      "Step 3: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.017152\n",
      "Step 3: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008648\n",
      "Step 3: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011746\n",
      "Step 3: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.016681\n",
      "Step 3: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.011685\n",
      "Step 3: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009590\n",
      "Step 3: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.007444\n",
      "Step 3: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.007337\n",
      "Step 3: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011097\n",
      "Step 3: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.010648\n",
      "Step 3: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.011266\n",
      "Step 3: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.008071\n",
      "Step 3: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009515\n",
      "Step 3: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.003071\n",
      "Step 3: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.010080\n",
      "Step 3: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.006181\n",
      "Step 3: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.004955\n",
      "Step 3: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009247\n",
      "Step 3: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.006212\n",
      "Step 3: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.007955\n",
      "Step 3: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.009722\n",
      "Step 3: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009142\n",
      "Step 3: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.006345\n",
      "Step 3: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008787\n",
      "Step 3: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009931\n",
      "Step 3: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011287\n",
      "Step 3: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.000161\n",
      "Step 3: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.003860\n",
      "Step 3: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.011724\n",
      "Step 3: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009162\n",
      "Step 3: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.008703\n",
      "Step 3: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008737\n",
      "Step 3: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010236\n",
      "Step 3: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.011446\n",
      "Step 3: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.014720\n",
      "Step 3: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.009071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.011501\n",
      "Step 3: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.012460\n",
      "Step 3: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.008793\n",
      "Step 3: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.008892\n",
      "Step 3: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.007416\n",
      "Step 3: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.016094\n",
      "Step 3: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.005330\n",
      "Step 3: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.012208\n",
      "Step 3: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.017573\n",
      "Step 3: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.016448\n",
      "Step 3: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.009878\n",
      "Step 3: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.003949\n",
      "Step 3: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.016798\n",
      "Step 3: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009572\n",
      "Step 3: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010476\n",
      "Step 3: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.010548\n",
      "Step 3: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.013102\n",
      "Step 3: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009316\n",
      "Step 3: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.007537\n",
      "Step 3: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010571\n",
      "Step 3: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.009850\n",
      "Step 3: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.007435\n",
      "Step 3: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.006127\n",
      "Step 3: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.007077\n",
      "Step 3: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.010109\n",
      "Step 3: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.009846\n",
      "Step 3: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.013313\n",
      "Step 3: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.004371\n",
      "Step 3: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009102\n",
      "Step 3: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.019949\n",
      "Step 3: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.013724\n",
      "Step 3: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.007792\n",
      "Step 3: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.007274\n",
      "Step 3: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.010916\n",
      "Step 3: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.008670\n",
      "Step 3: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.004676\n",
      "Step 3: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.012172\n",
      "Step 3: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.012018\n",
      "Step 3: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.013883\n",
      "Step 3: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.013109\n",
      "Step 3: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.011907\n",
      "Step 3: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.011191\n",
      "Step 3: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012262\n",
      "Step 3: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.018273\n",
      "Step 3: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.013754\n",
      "Step 3: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.005230\n",
      "Step 3: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.011810\n",
      "Step 3: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.006833\n",
      "Step 3: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.014450\n",
      "Step 3: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.003462\n",
      "Step 3: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.001429\n",
      "Step 3: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.010113\n",
      "Step 3: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012589\n",
      "Step 3: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008277\n",
      "Step 3: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.016845\n",
      "Step 3: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.012392\n",
      "time =  0:06:56.404656\n",
      "Step 4: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.002741\n",
      "Step 4: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012212\n",
      "Step 4: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.005108\n",
      "Step 4: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.003367\n",
      "Step 4: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007964\n",
      "Step 4: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009462\n",
      "Step 4: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.017032\n",
      "Step 4: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.007602\n",
      "Step 4: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.003606\n",
      "Step 4: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011429\n",
      "Step 4: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008158\n",
      "Step 4: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.012707\n",
      "Step 4: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.008982\n",
      "Step 4: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.009858\n",
      "Step 4: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.010943\n",
      "Step 4: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009992\n",
      "Step 4: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.006116\n",
      "Step 4: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.008325\n",
      "Step 4: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.005192\n",
      "Step 4: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.010232\n",
      "Step 4: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.012106\n",
      "Step 4: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008947\n",
      "Step 4: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.011137\n",
      "Step 4: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.007928\n",
      "Step 4: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.013192\n",
      "Step 4: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.002895\n",
      "Step 4: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.010217\n",
      "Step 4: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.010700\n",
      "Step 4: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.016767\n",
      "Step 4: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.006537\n",
      "Step 4: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.012904\n",
      "Step 4: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.008487\n",
      "Step 4: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011511\n",
      "Step 4: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.012145\n",
      "Step 4: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009950\n",
      "Step 4: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.006942\n",
      "Step 4: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.005379\n",
      "Step 4: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.007418\n",
      "Step 4: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.008252\n",
      "Step 4: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.017107\n",
      "Step 4: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009489\n",
      "Step 4: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.007746\n",
      "Step 4: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.006590\n",
      "Step 4: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.020320\n",
      "Step 4: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008370\n",
      "Step 4: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.008361\n",
      "Step 4: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.008403\n",
      "Step 4: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.007749\n",
      "Step 4: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.003952\n",
      "Step 4: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.012765\n",
      "Step 4: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005019\n",
      "Step 4: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.005366\n",
      "Step 4: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.011001\n",
      "Step 4: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.013194\n",
      "Step 4: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.007083\n",
      "Step 4: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.009331\n",
      "Step 4: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008135\n",
      "Step 4: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.006717\n",
      "Step 4: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.012352\n",
      "Step 4: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.007404\n",
      "Step 4: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.011639\n",
      "Step 4: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013467\n",
      "Step 4: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.018390\n",
      "Step 4: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.018755\n",
      "Step 4: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.009552\n",
      "Step 4: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.018238\n",
      "Step 4: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.006353\n",
      "Step 4: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.012231\n",
      "Step 4: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.012275\n",
      "Step 4: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.011241\n",
      "Step 4: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.016827\n",
      "Step 4: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.014909\n",
      "Step 4: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.007784\n",
      "Step 4: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.013786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.012174\n",
      "Step 4: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.008856\n",
      "Step 4: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.007446\n",
      "Step 4: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.011291\n",
      "Step 4: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008396\n",
      "Step 4: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.005291\n",
      "Step 4: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007434\n",
      "Step 4: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.007703\n",
      "Step 4: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009344\n",
      "Step 4: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.007304\n",
      "Step 4: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.011160\n",
      "Step 4: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.009784\n",
      "Step 4: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.014878\n",
      "Step 4: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.007647\n",
      "Step 4: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.003031\n",
      "Step 4: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.014017\n",
      "Step 4: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.014053\n",
      "Step 4: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.009929\n",
      "Step 4: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.015874\n",
      "Step 4: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.007136\n",
      "Step 4: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.013120\n",
      "Step 4: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010272\n",
      "Step 4: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.004114\n",
      "Step 4: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007107\n",
      "Step 4: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.008877\n",
      "Step 4: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.003515\n",
      "Step 4: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008173\n",
      "Step 4: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.007742\n",
      "Step 4: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.006637\n",
      "Step 4: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.003589\n",
      "Step 4: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.005701\n",
      "Step 4: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.013113\n",
      "Step 4: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.014290\n",
      "Step 4: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.011652\n",
      "Step 4: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.015419\n",
      "Step 4: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.013381\n",
      "Step 4: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.005703\n",
      "Step 4: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011731\n",
      "Step 4: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011564\n",
      "Step 4: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.011675\n",
      "Step 4: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010039\n",
      "Step 4: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.010266\n",
      "Step 4: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.003819\n",
      "Step 4: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.005732\n",
      "Step 4: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.006497\n",
      "Step 4: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.013285\n",
      "Step 4: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008928\n",
      "Step 4: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.003362\n",
      "Step 4: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.008124\n",
      "Step 4: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.008727\n",
      "Step 4: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.012789\n",
      "Step 4: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.008516\n",
      "Step 4: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012113\n",
      "Step 4: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.013435\n",
      "Step 4: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007164\n",
      "Step 4: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.013210\n",
      "Step 4: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007041\n",
      "Step 4: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.009508\n",
      "Step 4: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.010100\n",
      "Step 4: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.003759\n",
      "Step 4: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010577\n",
      "Step 4: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.001371\n",
      "Step 4: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.007838\n",
      "Step 4: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.011928\n",
      "Step 4: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.015075\n",
      "Step 4: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.007565\n",
      "Step 4: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.007576\n",
      "Step 4: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.008257\n",
      "Step 4: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.002080\n",
      "Step 4: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.011634\n",
      "Step 4: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010947\n",
      "Step 4: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009945\n",
      "Step 4: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.007794\n",
      "time =  0:06:53.627095\n",
      "Step 5: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.008314\n",
      "Step 5: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.010476\n",
      "Step 5: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.013127\n",
      "Step 5: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.006877\n",
      "Step 5: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.013676\n",
      "Step 5: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009402\n",
      "Step 5: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.009159\n",
      "Step 5: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.005493\n",
      "Step 5: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009804\n",
      "Step 5: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.012331\n",
      "Step 5: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.012073\n",
      "Step 5: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.012778\n",
      "Step 5: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.010304\n",
      "Step 5: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007022\n",
      "Step 5: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006723\n",
      "Step 5: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012519\n",
      "Step 5: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.008042\n",
      "Step 5: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009267\n",
      "Step 5: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.014150\n",
      "Step 5: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.007991\n",
      "Step 5: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.008618\n",
      "Step 5: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.016214\n",
      "Step 5: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009601\n",
      "Step 5: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.014105\n",
      "Step 5: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.006112\n",
      "Step 5: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.019689\n",
      "Step 5: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.014210\n",
      "Step 5: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007606\n",
      "Step 5: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.004957\n",
      "Step 5: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.006486\n",
      "Step 5: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.008284\n",
      "Step 5: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.009304\n",
      "Step 5: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.010225\n",
      "Step 5: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.006672\n",
      "Step 5: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.011305\n",
      "Step 5: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.013801\n",
      "Step 5: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.010879\n",
      "Step 5: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.009320\n",
      "Step 5: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.008487\n",
      "Step 5: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.011223\n",
      "Step 5: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009513\n",
      "Step 5: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.011561\n",
      "Step 5: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.012349\n",
      "Step 5: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.011956\n",
      "Step 5: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009032\n",
      "Step 5: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010319\n",
      "Step 5: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.005631\n",
      "Step 5: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006946\n",
      "Step 5: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.013171\n",
      "Step 5: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.016629\n",
      "Step 5: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.008409\n",
      "Step 5: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.012145\n",
      "Step 5: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.002899\n",
      "Step 5: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.011764\n",
      "Step 5: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.012969\n",
      "Step 5: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.010696\n",
      "Step 5: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.007574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.009105\n",
      "Step 5: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.011212\n",
      "Step 5: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.011585\n",
      "Step 5: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.012930\n",
      "Step 5: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013505\n",
      "Step 5: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.008071\n",
      "Step 5: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.003212\n",
      "Step 5: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.012395\n",
      "Step 5: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010625\n",
      "Step 5: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.012898\n",
      "Step 5: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.014282\n",
      "Step 5: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010598\n",
      "Step 5: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.006944\n",
      "Step 5: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.006594\n",
      "Step 5: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.014804\n",
      "Step 5: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.015171\n",
      "Step 5: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.010451\n",
      "Step 5: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.007681\n",
      "Step 5: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.013110\n",
      "Step 5: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.004876\n",
      "Step 5: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008693\n",
      "Step 5: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.011927\n",
      "Step 5: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009361\n",
      "Step 5: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011190\n",
      "Step 5: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008186\n",
      "Step 5: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.005706\n",
      "Step 5: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.014504\n",
      "Step 5: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.008678\n",
      "Step 5: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.001573\n",
      "Step 5: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008381\n",
      "Step 5: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.008043\n",
      "Step 5: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.008078\n",
      "Step 5: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.010081\n",
      "Step 5: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.002618\n",
      "Step 5: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.007401\n",
      "Step 5: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.004264\n",
      "Step 5: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.000417\n",
      "Step 5: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.006895\n",
      "Step 5: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.013917\n",
      "Step 5: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010341\n",
      "Step 5: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010443\n",
      "Step 5: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.009013\n",
      "Step 5: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.012328\n",
      "Step 5: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.011752\n",
      "Step 5: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.011212\n",
      "Step 5: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.007758\n",
      "Step 5: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.015581\n",
      "Step 5: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.007071\n",
      "Step 5: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.011578\n",
      "Step 5: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.013799\n",
      "Step 5: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.005751\n",
      "Step 5: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.002793\n",
      "Step 5: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.006791\n",
      "Step 5: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.006884\n",
      "Step 5: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.010999\n",
      "Step 5: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010078\n",
      "Step 5: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.012426\n",
      "Step 5: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.015571\n",
      "Step 5: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011792\n",
      "Step 5: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.006938\n",
      "Step 5: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.007982\n",
      "Step 5: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.007574\n",
      "Step 5: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.008561\n",
      "Step 5: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008173\n",
      "Step 5: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.015810\n",
      "Step 5: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011549\n",
      "Step 5: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012658\n",
      "Step 5: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.004105\n",
      "Step 5: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.004869\n",
      "Step 5: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.011033\n",
      "Step 5: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011232\n",
      "Step 5: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009441\n",
      "Step 5: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.015113\n",
      "Step 5: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007280\n",
      "Step 5: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010772\n",
      "Step 5: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.005527\n",
      "Step 5: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.007651\n",
      "Step 5: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.003269\n",
      "Step 5: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011264\n",
      "Step 5: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011702\n",
      "Step 5: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.018389\n",
      "Step 5: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010258\n",
      "Step 5: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.004531\n",
      "Step 5: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.014377\n",
      "Step 5: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.004104\n",
      "Step 5: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008713\n",
      "Step 5: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012577\n",
      "Step 5: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.007859\n",
      "Step 5: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.014616\n",
      "Step 5: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010011\n",
      "time =  0:06:51.601467\n",
      "WARNING:tensorflow:From /home/aut_speech/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Step 6: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.008733\n",
      "Step 6: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.010074\n",
      "Step 6: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.012167\n",
      "Step 6: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008106\n",
      "Step 6: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.014911\n",
      "Step 6: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.005221\n",
      "Step 6: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.011304\n",
      "Step 6: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.016556\n",
      "Step 6: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.012577\n",
      "Step 6: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010233\n",
      "Step 6: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.014619\n",
      "Step 6: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.006612\n",
      "Step 6: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.010411\n",
      "Step 6: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.009644\n",
      "Step 6: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.010140\n",
      "Step 6: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.015016\n",
      "Step 6: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.007463\n",
      "Step 6: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012132\n",
      "Step 6: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010146\n",
      "Step 6: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011589\n",
      "Step 6: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.009414\n",
      "Step 6: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009271\n",
      "Step 6: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009754\n",
      "Step 6: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009450\n",
      "Step 6: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.013725\n",
      "Step 6: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.010451\n",
      "Step 6: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008018\n",
      "Step 6: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.013547\n",
      "Step 6: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007066\n",
      "Step 6: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.006444\n",
      "Step 6: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011690\n",
      "Step 6: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.004143\n",
      "Step 6: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009521\n",
      "Step 6: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.009172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.011883\n",
      "Step 6: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.007942\n",
      "Step 6: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006019\n",
      "Step 6: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.011800\n",
      "Step 6: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.015151\n",
      "Step 6: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.007698\n",
      "Step 6: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.014280\n",
      "Step 6: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.010615\n",
      "Step 6: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.016457\n",
      "Step 6: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.010936\n",
      "Step 6: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009531\n",
      "Step 6: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.005822\n",
      "Step 6: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.008973\n",
      "Step 6: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.004765\n",
      "Step 6: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.010657\n",
      "Step 6: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.015240\n",
      "Step 6: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005410\n",
      "Step 6: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.012786\n",
      "Step 6: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.009310\n",
      "Step 6: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.014968\n",
      "Step 6: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.006028\n",
      "Step 6: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.010655\n",
      "Step 6: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014119\n",
      "Step 6: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.012362\n",
      "Step 6: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.012012\n",
      "Step 6: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009282\n",
      "Step 6: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.006075\n",
      "Step 6: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013511\n",
      "Step 6: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.011278\n",
      "Step 6: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.002206\n",
      "Step 6: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.003032\n",
      "Step 6: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.008578\n",
      "Step 6: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.009361\n",
      "Step 6: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009664\n",
      "Step 6: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.005048\n",
      "Step 6: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.006540\n",
      "Step 6: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.009069\n",
      "Step 6: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.013653\n",
      "Step 6: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009389\n",
      "Step 6: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.007675\n",
      "Step 6: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011826\n",
      "Step 6: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.014680\n",
      "Step 6: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.010383\n",
      "Step 6: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.014453\n",
      "Step 6: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.010249\n",
      "Step 6: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.005825\n",
      "Step 6: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.010275\n",
      "Step 6: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.005384\n",
      "Step 6: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008741\n",
      "Step 6: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.014390\n",
      "Step 6: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.007259\n",
      "Step 6: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.014628\n",
      "Step 6: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.012384\n",
      "Step 6: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011824\n",
      "Step 6: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007428\n",
      "Step 6: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.007131\n",
      "Step 6: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.017260\n",
      "Step 6: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.017656\n",
      "Step 6: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.009576\n",
      "Step 6: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.014690\n",
      "Step 6: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.011671\n",
      "Step 6: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.006672\n",
      "Step 6: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.007936\n",
      "Step 6: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.012577\n",
      "Step 6: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.010585\n",
      "Step 6: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008918\n",
      "Step 6: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008762\n",
      "Step 6: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.018552\n",
      "Step 6: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.008223\n",
      "Step 6: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.008947\n",
      "Step 6: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.004696\n",
      "Step 6: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.011736\n",
      "Step 6: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.008184\n",
      "Step 6: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.008600\n",
      "Step 6: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.013134\n",
      "Step 6: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.013013\n",
      "Step 6: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.002930\n",
      "Step 6: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.007281\n",
      "Step 6: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.009408\n",
      "Step 6: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009980\n",
      "Step 6: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.017879\n",
      "Step 6: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.006188\n",
      "Step 6: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.014884\n",
      "Step 6: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.010452\n",
      "Step 6: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.008114\n",
      "Step 6: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.018151\n",
      "Step 6: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.009894\n",
      "Step 6: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.012131\n",
      "Step 6: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.012488\n",
      "Step 6: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.004286\n",
      "Step 6: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.005534\n",
      "Step 6: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.009836\n",
      "Step 6: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.004277\n",
      "Step 6: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011161\n",
      "Step 6: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.004526\n",
      "Step 6: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.007308\n",
      "Step 6: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.022565\n",
      "Step 6: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.013925\n",
      "Step 6: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.012728\n",
      "Step 6: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.011472\n",
      "Step 6: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.007861\n",
      "Step 6: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010621\n",
      "Step 6: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.013476\n",
      "Step 6: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.019169\n",
      "Step 6: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.004260\n",
      "Step 6: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.009939\n",
      "Step 6: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008228\n",
      "Step 6: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.006874\n",
      "Step 6: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.013089\n",
      "Step 6: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.014813\n",
      "Step 6: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.011790\n",
      "Step 6: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.015967\n",
      "Step 6: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009698\n",
      "time =  0:06:59.661321\n",
      "Step 7: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.010209\n",
      "Step 7: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.010720\n",
      "Step 7: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.007522\n",
      "Step 7: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.017989\n",
      "Step 7: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007031\n",
      "Step 7: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.004805\n",
      "Step 7: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.002618\n",
      "Step 7: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008201\n",
      "Step 7: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010195\n",
      "Step 7: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013363\n",
      "Step 7: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.014349\n",
      "Step 7: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.017024\n",
      "Step 7: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.007344\n",
      "Step 7: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.003169\n",
      "Step 7: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.014616\n",
      "Step 7: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012394\n",
      "Step 7: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.005581\n",
      "Step 7: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.007293\n",
      "Step 7: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.003166\n",
      "Step 7: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.014418\n",
      "Step 7: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.012754\n",
      "Step 7: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.008946\n",
      "Step 7: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009775\n",
      "Step 7: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.012877\n",
      "Step 7: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009295\n",
      "Step 7: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.011817\n",
      "Step 7: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.008524\n",
      "Step 7: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.006099\n",
      "Step 7: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.013346\n",
      "Step 7: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011079\n",
      "Step 7: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.007917\n",
      "Step 7: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009281\n",
      "Step 7: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.011688\n",
      "Step 7: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.003365\n",
      "Step 7: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.004148\n",
      "Step 7: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.005034\n",
      "Step 7: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.012784\n",
      "Step 7: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010069\n",
      "Step 7: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.012525\n",
      "Step 7: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.006293\n",
      "Step 7: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.011507\n",
      "Step 7: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.007789\n",
      "Step 7: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.017881\n",
      "Step 7: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012501\n",
      "Step 7: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.008627\n",
      "Step 7: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.011892\n",
      "Step 7: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006125\n",
      "Step 7: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.002619\n",
      "Step 7: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.005772\n",
      "Step 7: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.008914\n",
      "Step 7: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.018289\n",
      "Step 7: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010409\n",
      "Step 7: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.003373\n",
      "Step 7: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.005979\n",
      "Step 7: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013633\n",
      "Step 7: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.007515\n",
      "Step 7: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.014270\n",
      "Step 7: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.003153\n",
      "Step 7: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009517\n",
      "Step 7: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007416\n",
      "Step 7: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.010934\n",
      "Step 7: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010491\n",
      "Step 7: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.001533\n",
      "Step 7: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.007463\n",
      "Step 7: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.001474\n",
      "Step 7: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011148\n",
      "Step 7: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.007968\n",
      "Step 7: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.013948\n",
      "Step 7: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.007483\n",
      "Step 7: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.005301\n",
      "Step 7: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010230\n",
      "Step 7: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.012317\n",
      "Step 7: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.005686\n",
      "Step 7: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.004760\n",
      "Step 7: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.013083\n",
      "Step 7: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.007631\n",
      "Step 7: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.012824\n",
      "Step 7: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.012220\n",
      "Step 7: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.005087\n",
      "Step 7: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.006898\n",
      "Step 7: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.012706\n",
      "Step 7: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009617\n",
      "Step 7: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.010639\n",
      "Step 7: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009304\n",
      "Step 7: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.005918\n",
      "Step 7: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.003675\n",
      "Step 7: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011442\n",
      "Step 7: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.006269\n",
      "Step 7: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.008250\n",
      "Step 7: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.007450\n",
      "Step 7: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.008713\n",
      "Step 7: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.014413\n",
      "Step 7: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.011729\n",
      "Step 7: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.011852\n",
      "Step 7: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.009888\n",
      "Step 7: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.011706\n",
      "Step 7: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008649\n",
      "Step 7: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.006836\n",
      "Step 7: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.006529\n",
      "Step 7: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.011088\n",
      "Step 7: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.008681\n",
      "Step 7: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.010568\n",
      "Step 7: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.007244\n",
      "Step 7: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009903\n",
      "Step 7: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008626\n",
      "Step 7: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.006097\n",
      "Step 7: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.014078\n",
      "Step 7: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.011549\n",
      "Step 7: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.005572\n",
      "Step 7: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.005773\n",
      "Step 7: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.013125\n",
      "Step 7: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.006969\n",
      "Step 7: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.011024\n",
      "Step 7: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011971\n",
      "Step 7: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009054\n",
      "Step 7: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.007126\n",
      "Step 7: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.013117\n",
      "Step 7: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.004016\n",
      "Step 7: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.004783\n",
      "Step 7: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.010739\n",
      "Step 7: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.007340\n",
      "Step 7: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.016573\n",
      "Step 7: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012233\n",
      "Step 7: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009867\n",
      "Step 7: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.004716\n",
      "Step 7: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012076\n",
      "Step 7: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.014558\n",
      "Step 7: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007116\n",
      "Step 7: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.013150\n",
      "Step 7: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.000864\n",
      "Step 7: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.006567\n",
      "Step 7: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.010963\n",
      "Step 7: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.010369\n",
      "Step 7: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010992\n",
      "Step 7: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010751\n",
      "Step 7: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010197\n",
      "Step 7: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.007328\n",
      "Step 7: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.008353\n",
      "Step 7: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.005283\n",
      "Step 7: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.011205\n",
      "Step 7: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012777\n",
      "Step 7: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.005257\n",
      "Step 7: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013658\n",
      "Step 7: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.011870\n",
      "Step 7: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.014218\n",
      "Step 7: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.012343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  0:06:58.889028\n",
      "Step 8: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.019206\n",
      "Step 8: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.008137\n",
      "Step 8: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006002\n",
      "Step 8: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.014046\n",
      "Step 8: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.012974\n",
      "Step 8: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.006618\n",
      "Step 8: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.008518\n",
      "Step 8: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.010089\n",
      "Step 8: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010422\n",
      "Step 8: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.008837\n",
      "Step 8: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.010496\n",
      "Step 8: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008006\n",
      "Step 8: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.013776\n",
      "Step 8: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.005685\n",
      "Step 8: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013668\n",
      "Step 8: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.003321\n",
      "Step 8: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.009007\n",
      "Step 8: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.003999\n",
      "Step 8: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.008318\n",
      "Step 8: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.005910\n",
      "Step 8: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.012184\n",
      "Step 8: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.011128\n",
      "Step 8: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.008618\n",
      "Step 8: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.006327\n",
      "Step 8: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.010968\n",
      "Step 8: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.012120\n",
      "Step 8: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.016557\n",
      "Step 8: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.005754\n",
      "Step 8: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.010707\n",
      "Step 8: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.007030\n",
      "Step 8: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.007349\n",
      "Step 8: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.004719\n",
      "Step 8: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.005417\n",
      "Step 8: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.005792\n",
      "Step 8: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.013955\n",
      "Step 8: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.008352\n",
      "Step 8: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006945\n",
      "Step 8: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.011980\n",
      "Step 8: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010325\n",
      "Step 8: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009421\n",
      "Step 8: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011572\n",
      "Step 8: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009516\n",
      "Step 8: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010910\n",
      "Step 8: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.010673\n",
      "Step 8: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.010473\n",
      "Step 8: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.015548\n",
      "Step 8: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010632\n",
      "Step 8: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.012506\n",
      "Step 8: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.003524\n",
      "Step 8: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.006940\n",
      "Step 8: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.010757\n",
      "Step 8: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.012031\n",
      "Step 8: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.009086\n",
      "Step 8: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.013316\n",
      "Step 8: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.011403\n",
      "Step 8: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.005377\n",
      "Step 8: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.011338\n",
      "Step 8: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.009511\n",
      "Step 8: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008036\n",
      "Step 8: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.010354\n",
      "Step 8: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.011011\n",
      "Step 8: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.009369\n",
      "Step 8: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.004984\n",
      "Step 8: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011633\n",
      "Step 8: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.010251\n",
      "Step 8: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.003265\n",
      "Step 8: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.017879\n",
      "Step 8: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009044\n",
      "Step 8: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.007194\n",
      "Step 8: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009799\n",
      "Step 8: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.011294\n",
      "Step 8: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.006387\n",
      "Step 8: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009821\n",
      "Step 8: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009071\n",
      "Step 8: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.017399\n",
      "Step 8: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.008711\n",
      "Step 8: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.016405\n",
      "Step 8: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008809\n",
      "Step 8: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009666\n",
      "Step 8: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.010983\n",
      "Step 8: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009269\n",
      "Step 8: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.014147\n",
      "Step 8: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.010804\n",
      "Step 8: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.011039\n",
      "Step 8: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.013448\n",
      "Step 8: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.005852\n",
      "Step 8: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010171\n",
      "Step 8: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.012117\n",
      "Step 8: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010613\n",
      "Step 8: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009106\n",
      "Step 8: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.006647\n",
      "Step 8: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.011880\n",
      "Step 8: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.016161\n",
      "Step 8: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.004159\n",
      "Step 8: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.006744\n",
      "Step 8: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.007824\n",
      "Step 8: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.005339\n",
      "Step 8: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.011565\n",
      "Step 8: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.014562\n",
      "Step 8: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.003335\n",
      "Step 8: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010930\n",
      "Step 8: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.012824\n",
      "Step 8: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.007296\n",
      "Step 8: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.008243\n",
      "Step 8: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.016962\n",
      "Step 8: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008661\n",
      "Step 8: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.008192\n",
      "Step 8: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.014827\n",
      "Step 8: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.012944\n",
      "Step 8: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.002388\n",
      "Step 8: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.009982\n",
      "Step 8: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.009899\n",
      "Step 8: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010125\n",
      "Step 8: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.008391\n",
      "Step 8: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.008359\n",
      "Step 8: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.014077\n",
      "Step 8: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.009384\n",
      "Step 8: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009800\n",
      "Step 8: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.005032\n",
      "Step 8: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.012188\n",
      "Step 8: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.014439\n",
      "Step 8: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011279\n",
      "Step 8: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.013293\n",
      "Step 8: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.009951\n",
      "Step 8: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.010432\n",
      "Step 8: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012465\n",
      "Step 8: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.006658\n",
      "Step 8: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.014430\n",
      "Step 8: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011593\n",
      "Step 8: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.010445\n",
      "Step 8: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.011896\n",
      "Step 8: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.014776\n",
      "Step 8: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.008089\n",
      "Step 8: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.009802\n",
      "Step 8: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.005352\n",
      "Step 8: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.005393\n",
      "Step 8: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.006964\n",
      "Step 8: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.020647\n",
      "Step 8: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.005748\n",
      "Step 8: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.013925\n",
      "Step 8: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.011603\n",
      "Step 8: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.009038\n",
      "Step 8: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.010701\n",
      "Step 8: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.009912\n",
      "Step 8: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.006021\n",
      "Step 8: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008270\n",
      "time =  0:06:55.431825\n",
      "Step 9: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.016970\n",
      "Step 9: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.011495\n",
      "Step 9: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006122\n",
      "Step 9: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.001782\n",
      "Step 9: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.003960\n",
      "Step 9: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.010566\n",
      "Step 9: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.019836\n",
      "Step 9: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.014411\n",
      "Step 9: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.013462\n",
      "Step 9: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.006330\n",
      "Step 9: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.010455\n",
      "Step 9: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008318\n",
      "Step 9: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.013912\n",
      "Step 9: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.006328\n",
      "Step 9: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.012840\n",
      "Step 9: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012719\n",
      "Step 9: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.006137\n",
      "Step 9: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.005834\n",
      "Step 9: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.008663\n",
      "Step 9: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.012407\n",
      "Step 9: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.017364\n",
      "Step 9: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010049\n",
      "Step 9: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009719\n",
      "Step 9: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.011110\n",
      "Step 9: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.011788\n",
      "Step 9: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.014724\n",
      "Step 9: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.005735\n",
      "Step 9: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007514\n",
      "Step 9: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007885\n",
      "Step 9: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008238\n",
      "Step 9: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011203\n",
      "Step 9: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.008685\n",
      "Step 9: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.014383\n",
      "Step 9: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008716\n",
      "Step 9: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.015628\n",
      "Step 9: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.003303\n",
      "Step 9: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006173\n",
      "Step 9: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.008843\n",
      "Step 9: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.012700\n",
      "Step 9: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.010763\n",
      "Step 9: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.008273\n",
      "Step 9: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.010947\n",
      "Step 9: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.013191\n",
      "Step 9: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.006272\n",
      "Step 9: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009983\n",
      "Step 9: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.008956\n",
      "Step 9: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010537\n",
      "Step 9: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.005367\n",
      "Step 9: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.005823\n",
      "Step 9: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008301\n",
      "Step 9: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.010359\n",
      "Step 9: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.008298\n",
      "Step 9: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.006162\n",
      "Step 9: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.009415\n",
      "Step 9: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.012515\n",
      "Step 9: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.009764\n",
      "Step 9: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.002465\n",
      "Step 9: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.009505\n",
      "Step 9: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008961\n",
      "Step 9: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008395\n",
      "Step 9: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.011470\n",
      "Step 9: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008412\n",
      "Step 9: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.018221\n",
      "Step 9: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008752\n",
      "Step 9: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.014502\n",
      "Step 9: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.009019\n",
      "Step 9: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.015221\n",
      "Step 9: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009387\n",
      "Step 9: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.011692\n",
      "Step 9: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.007808\n",
      "Step 9: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.003647\n",
      "Step 9: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.007375\n",
      "Step 9: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009769\n",
      "Step 9: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.003930\n",
      "Step 9: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.005874\n",
      "Step 9: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.007589\n",
      "Step 9: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.012490\n",
      "Step 9: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009081\n",
      "Step 9: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008613\n",
      "Step 9: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012160\n",
      "Step 9: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.004614\n",
      "Step 9: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.014189\n",
      "Step 9: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009616\n",
      "Step 9: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.009398\n",
      "Step 9: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005051\n",
      "Step 9: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007048\n",
      "Step 9: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.013666\n",
      "Step 9: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010574\n",
      "Step 9: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010677\n",
      "Step 9: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.008020\n",
      "Step 9: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.005887\n",
      "Step 9: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.016216\n",
      "Step 9: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.018609\n",
      "Step 9: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.009254\n",
      "Step 9: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.016685\n",
      "Step 9: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.012779\n",
      "Step 9: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.015068\n",
      "Step 9: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010009\n",
      "Step 9: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.006394\n",
      "Step 9: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.010511\n",
      "Step 9: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010116\n",
      "Step 9: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006139\n",
      "Step 9: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.011379\n",
      "Step 9: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.007297\n",
      "Step 9: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.012355\n",
      "Step 9: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010452\n",
      "Step 9: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.011473\n",
      "Step 9: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.004361\n",
      "Step 9: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009255\n",
      "Step 9: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.013919\n",
      "Step 9: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.006674\n",
      "Step 9: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.009302\n",
      "Step 9: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.012247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.007197\n",
      "Step 9: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.006757\n",
      "Step 9: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.013731\n",
      "Step 9: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010697\n",
      "Step 9: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.013363\n",
      "Step 9: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011261\n",
      "Step 9: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.007844\n",
      "Step 9: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.010640\n",
      "Step 9: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.008919\n",
      "Step 9: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011203\n",
      "Step 9: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.011947\n",
      "Step 9: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009645\n",
      "Step 9: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.004701\n",
      "Step 9: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.008437\n",
      "Step 9: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.006381\n",
      "Step 9: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.006017\n",
      "Step 9: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008559\n",
      "Step 9: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008098\n",
      "Step 9: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008180\n",
      "Step 9: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.011199\n",
      "Step 9: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.015348\n",
      "Step 9: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.011761\n",
      "Step 9: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011426\n",
      "Step 9: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011052\n",
      "Step 9: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.009690\n",
      "Step 9: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.002503\n",
      "Step 9: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.014480\n",
      "Step 9: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.011720\n",
      "Step 9: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.015529\n",
      "Step 9: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.010163\n",
      "Step 9: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013275\n",
      "Step 9: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.004023\n",
      "Step 9: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.010863\n",
      "Step 9: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008342\n",
      "time =  0:06:56.477644\n",
      "Step 10: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.006550\n",
      "Step 10: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.004827\n",
      "Step 10: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.016611\n",
      "Step 10: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009452\n",
      "Step 10: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.006935\n",
      "Step 10: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.003638\n",
      "Step 10: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.010025\n",
      "Step 10: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008424\n",
      "Step 10: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.008488\n",
      "Step 10: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.006104\n",
      "Step 10: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.012618\n",
      "Step 10: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.006164\n",
      "Step 10: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.012235\n",
      "Step 10: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007719\n",
      "Step 10: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.009797\n",
      "Step 10: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.010780\n",
      "Step 10: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.006842\n",
      "Step 10: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.008968\n",
      "Step 10: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.009124\n",
      "Step 10: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011785\n",
      "Step 10: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.005366\n",
      "Step 10: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.013384\n",
      "Step 10: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009668\n",
      "Step 10: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.006443\n",
      "Step 10: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.012251\n",
      "Step 10: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.013979\n",
      "Step 10: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.007365\n",
      "Step 10: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.004066\n",
      "Step 10: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.008908\n",
      "Step 10: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.007063\n",
      "Step 10: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.005090\n",
      "Step 10: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.011100\n",
      "Step 10: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.010992\n",
      "Step 10: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008762\n",
      "Step 10: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.011921\n",
      "Step 10: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.007986\n",
      "Step 10: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.017777\n",
      "Step 10: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010290\n",
      "Step 10: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.005897\n",
      "Step 10: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.012524\n",
      "Step 10: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011838\n",
      "Step 10: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.013840\n",
      "Step 10: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.007486\n",
      "Step 10: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.008191\n",
      "Step 10: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.007334\n",
      "Step 10: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010719\n",
      "Step 10: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009372\n",
      "Step 10: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.011853\n",
      "Step 10: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009524\n",
      "Step 10: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.005731\n",
      "Step 10: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005695\n",
      "Step 10: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.008901\n",
      "Step 10: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.012932\n",
      "Step 10: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.002899\n",
      "Step 10: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.007129\n",
      "Step 10: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.011370\n",
      "Step 10: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008329\n",
      "Step 10: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.006733\n",
      "Step 10: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008791\n",
      "Step 10: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009859\n",
      "Step 10: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.008808\n",
      "Step 10: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.011082\n",
      "Step 10: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.013555\n",
      "Step 10: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008671\n",
      "Step 10: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.006465\n",
      "Step 10: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.014542\n",
      "Step 10: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010243\n",
      "Step 10: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.012014\n",
      "Step 10: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.008955\n",
      "Step 10: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009182\n",
      "Step 10: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.007580\n",
      "Step 10: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.012810\n",
      "Step 10: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.005165\n",
      "Step 10: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.012712\n",
      "Step 10: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009913\n",
      "Step 10: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010844\n",
      "Step 10: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009725\n",
      "Step 10: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009624\n",
      "Step 10: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008128\n",
      "Step 10: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.006877\n",
      "Step 10: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007591\n",
      "Step 10: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008653\n",
      "Step 10: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.012181\n",
      "Step 10: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.011644\n",
      "Step 10: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.013748\n",
      "Step 10: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.009109\n",
      "Step 10: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008704\n",
      "Step 10: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.013246\n",
      "Step 10: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.011421\n",
      "Step 10: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009373\n",
      "Step 10: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.012113\n",
      "Step 10: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.014484\n",
      "Step 10: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.006402\n",
      "Step 10: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.005122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.012017\n",
      "Step 10: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010930\n",
      "Step 10: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012134\n",
      "Step 10: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.015779\n",
      "Step 10: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.012797\n",
      "Step 10: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.007741\n",
      "Step 10: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009636\n",
      "Step 10: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.009068\n",
      "Step 10: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.003478\n",
      "Step 10: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.010764\n",
      "Step 10: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.008405\n",
      "Step 10: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009169\n",
      "Step 10: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.015790\n",
      "Step 10: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.006671\n",
      "Step 10: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.010183\n",
      "Step 10: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011012\n",
      "Step 10: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.006825\n",
      "Step 10: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011985\n",
      "Step 10: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.008427\n",
      "Step 10: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.008013\n",
      "Step 10: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.003486\n",
      "Step 10: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011379\n",
      "Step 10: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010102\n",
      "Step 10: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.006883\n",
      "Step 10: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.003525\n",
      "Step 10: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.015231\n",
      "Step 10: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.012997\n",
      "Step 10: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.012761\n",
      "Step 10: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.015938\n",
      "Step 10: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.013140\n",
      "Step 10: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.002633\n",
      "Step 10: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.010878\n",
      "Step 10: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.010086\n",
      "Step 10: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.016832\n",
      "Step 10: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.010591\n",
      "Step 10: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009272\n",
      "Step 10: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008510\n",
      "Step 10: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.005635\n",
      "Step 10: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.004833\n",
      "Step 10: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009432\n",
      "Step 10: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.019503\n",
      "Step 10: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009981\n",
      "Step 10: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.006429\n",
      "Step 10: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.012758\n",
      "Step 10: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.003564\n",
      "Step 10: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.010368\n",
      "Step 10: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.010704\n",
      "Step 10: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007578\n",
      "Step 10: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.011441\n",
      "Step 10: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.015170\n",
      "Step 10: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010726\n",
      "Step 10: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.008412\n",
      "Step 10: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009808\n",
      "time =  0:07:00.875971\n",
      "Step 11: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.006400\n",
      "Step 11: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012511\n",
      "Step 11: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.013960\n",
      "Step 11: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008773\n",
      "Step 11: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007355\n",
      "Step 11: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009294\n",
      "Step 11: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.011096\n",
      "Step 11: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008867\n",
      "Step 11: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.008494\n",
      "Step 11: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.006071\n",
      "Step 11: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008250\n",
      "Step 11: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008977\n",
      "Step 11: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.015314\n",
      "Step 11: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.017229\n",
      "Step 11: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.016442\n",
      "Step 11: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012582\n",
      "Step 11: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.009693\n",
      "Step 11: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012240\n",
      "Step 11: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.013315\n",
      "Step 11: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.012585\n",
      "Step 11: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011551\n",
      "Step 11: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008139\n",
      "Step 11: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.004725\n",
      "Step 11: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.010026\n",
      "Step 11: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.001404\n",
      "Step 11: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.005640\n",
      "Step 11: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.006482\n",
      "Step 11: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.011052\n",
      "Step 11: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.008441\n",
      "Step 11: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008241\n",
      "Step 11: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009240\n",
      "Step 11: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.012359\n",
      "Step 11: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.006574\n",
      "Step 11: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.011731\n",
      "Step 11: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.006768\n",
      "Step 11: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.010513\n",
      "Step 11: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.011212\n",
      "Step 11: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.016090\n",
      "Step 11: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.007766\n",
      "Step 11: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.010739\n",
      "Step 11: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.006216\n",
      "Step 11: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.011520\n",
      "Step 11: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.004665\n",
      "Step 11: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.018085\n",
      "Step 11: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.006968\n",
      "Step 11: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.009963\n",
      "Step 11: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007367\n",
      "Step 11: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.008060\n",
      "Step 11: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.002944\n",
      "Step 11: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.010013\n",
      "Step 11: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.015141\n",
      "Step 11: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007418\n",
      "Step 11: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.007279\n",
      "Step 11: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.007393\n",
      "Step 11: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.002254\n",
      "Step 11: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006382\n",
      "Step 11: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014983\n",
      "Step 11: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010345\n",
      "Step 11: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.015537\n",
      "Step 11: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.014622\n",
      "Step 11: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.015970\n",
      "Step 11: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.009874\n",
      "Step 11: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012507\n",
      "Step 11: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.012130\n",
      "Step 11: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.007989\n",
      "Step 11: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.011225\n",
      "Step 11: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.009345\n",
      "Step 11: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.016623\n",
      "Step 11: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010558\n",
      "Step 11: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.011315\n",
      "Step 11: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.010492\n",
      "Step 11: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.011401\n",
      "Step 11: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.007481\n",
      "Step 11: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.011575\n",
      "Step 11: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.006904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.004971\n",
      "Step 11: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.007275\n",
      "Step 11: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.007953\n",
      "Step 11: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.006899\n",
      "Step 11: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009299\n",
      "Step 11: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007509\n",
      "Step 11: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008748\n",
      "Step 11: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008522\n",
      "Step 11: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.008481\n",
      "Step 11: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.007300\n",
      "Step 11: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.009077\n",
      "Step 11: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.014290\n",
      "Step 11: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.007058\n",
      "Step 11: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.002764\n",
      "Step 11: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.008586\n",
      "Step 11: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.013894\n",
      "Step 11: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.011239\n",
      "Step 11: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.013310\n",
      "Step 11: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.015412\n",
      "Step 11: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.009290\n",
      "Step 11: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010713\n",
      "Step 11: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012706\n",
      "Step 11: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.012123\n",
      "Step 11: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.010233\n",
      "Step 11: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.007581\n",
      "Step 11: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.004492\n",
      "Step 11: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006513\n",
      "Step 11: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.010762\n",
      "Step 11: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011267\n",
      "Step 11: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009403\n",
      "Step 11: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010488\n",
      "Step 11: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.010983\n",
      "Step 11: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015656\n",
      "Step 11: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.004622\n",
      "Step 11: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.004971\n",
      "Step 11: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008267\n",
      "Step 11: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.006613\n",
      "Step 11: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.001587\n",
      "Step 11: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.002835\n",
      "Step 11: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.007400\n",
      "Step 11: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.004629\n",
      "Step 11: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.013077\n",
      "Step 11: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009842\n",
      "Step 11: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.012753\n",
      "Step 11: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.006353\n",
      "Step 11: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.011383\n",
      "Step 11: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.003625\n",
      "Step 11: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.014432\n",
      "Step 11: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.005212\n",
      "Step 11: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009383\n",
      "Step 11: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.007386\n",
      "Step 11: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.014904\n",
      "Step 11: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.010926\n",
      "Step 11: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.008997\n",
      "Step 11: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.010968\n",
      "Step 11: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007949\n",
      "Step 11: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008078\n",
      "Step 11: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.006560\n",
      "Step 11: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009487\n",
      "Step 11: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.003641\n",
      "Step 11: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.014573\n",
      "Step 11: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.007456\n",
      "Step 11: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.014039\n",
      "Step 11: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.005262\n",
      "Step 11: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.015153\n",
      "Step 11: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.004706\n",
      "Step 11: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.011594\n",
      "Step 11: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.007606\n",
      "Step 11: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008711\n",
      "Step 11: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.014150\n",
      "Step 11: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.010207\n",
      "Step 11: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.006805\n",
      "time =  0:07:01.023914\n",
      "Step 12: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.013005\n",
      "Step 12: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.010939\n",
      "Step 12: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.001400\n",
      "Step 12: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009946\n",
      "Step 12: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010371\n",
      "Step 12: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009699\n",
      "Step 12: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.006628\n",
      "Step 12: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.011405\n",
      "Step 12: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.008292\n",
      "Step 12: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010613\n",
      "Step 12: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.006564\n",
      "Step 12: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005003\n",
      "Step 12: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.011890\n",
      "Step 12: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.010747\n",
      "Step 12: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.008600\n",
      "Step 12: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.005388\n",
      "Step 12: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.011914\n",
      "Step 12: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.005200\n",
      "Step 12: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.011990\n",
      "Step 12: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.005915\n",
      "Step 12: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.014590\n",
      "Step 12: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009788\n",
      "Step 12: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.012205\n",
      "Step 12: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.010297\n",
      "Step 12: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.010984\n",
      "Step 12: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009501\n",
      "Step 12: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.010395\n",
      "Step 12: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.016801\n",
      "Step 12: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.004612\n",
      "Step 12: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.013468\n",
      "Step 12: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.006325\n",
      "Step 12: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.012508\n",
      "Step 12: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011094\n",
      "Step 12: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.011785\n",
      "Step 12: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.010093\n",
      "Step 12: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.004695\n",
      "Step 12: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.014892\n",
      "Step 12: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.007490\n",
      "Step 12: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.012975\n",
      "Step 12: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.007538\n",
      "Step 12: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.007270\n",
      "Step 12: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.010734\n",
      "Step 12: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.009724\n",
      "Step 12: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.006822\n",
      "Step 12: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008660\n",
      "Step 12: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.012019\n",
      "Step 12: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010776\n",
      "Step 12: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006506\n",
      "Step 12: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.004720\n",
      "Step 12: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.007555\n",
      "Step 12: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.011480\n",
      "Step 12: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007776\n",
      "Step 12: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010955\n",
      "Step 12: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.007127\n",
      "Step 12: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.015119\n",
      "Step 12: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.008911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.004229\n",
      "Step 12: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.012502\n",
      "Step 12: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.011210\n",
      "Step 12: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008183\n",
      "Step 12: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007402\n",
      "Step 12: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.004218\n",
      "Step 12: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012744\n",
      "Step 12: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011715\n",
      "Step 12: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.011945\n",
      "Step 12: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007952\n",
      "Step 12: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.014697\n",
      "Step 12: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.010223\n",
      "Step 12: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.004364\n",
      "Step 12: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008630\n",
      "Step 12: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.002403\n",
      "Step 12: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010564\n",
      "Step 12: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.015147\n",
      "Step 12: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.010341\n",
      "Step 12: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.017572\n",
      "Step 12: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.006092\n",
      "Step 12: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.006117\n",
      "Step 12: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.010222\n",
      "Step 12: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.021918\n",
      "Step 12: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012933\n",
      "Step 12: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.010838\n",
      "Step 12: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.003395\n",
      "Step 12: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.012994\n",
      "Step 12: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.005502\n",
      "Step 12: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014630\n",
      "Step 12: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.002615\n",
      "Step 12: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.007466\n",
      "Step 12: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010659\n",
      "Step 12: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.011110\n",
      "Step 12: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.011169\n",
      "Step 12: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.008642\n",
      "Step 12: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.005329\n",
      "Step 12: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.012374\n",
      "Step 12: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.011164\n",
      "Step 12: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.003136\n",
      "Step 12: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010379\n",
      "Step 12: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.004969\n",
      "Step 12: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.011521\n",
      "Step 12: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.011156\n",
      "Step 12: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.005991\n",
      "Step 12: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009328\n",
      "Step 12: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.012386\n",
      "Step 12: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.011479\n",
      "Step 12: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011310\n",
      "Step 12: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.012492\n",
      "Step 12: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009536\n",
      "Step 12: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.011767\n",
      "Step 12: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.005899\n",
      "Step 12: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009221\n",
      "Step 12: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011635\n",
      "Step 12: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010518\n",
      "Step 12: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.005602\n",
      "Step 12: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.008009\n",
      "Step 12: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009600\n",
      "Step 12: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009486\n",
      "Step 12: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.016525\n",
      "Step 12: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010238\n",
      "Step 12: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.006612\n",
      "Step 12: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.015294\n",
      "Step 12: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.016787\n",
      "Step 12: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.010966\n",
      "Step 12: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.004293\n",
      "Step 12: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011553\n",
      "Step 12: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.020024\n",
      "Step 12: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.007666\n",
      "Step 12: Minibatch_NU: 12500/14680 Minibatch Loss: nan  0.999769\n",
      "Step 12: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.010632\n",
      "Step 12: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.007668\n",
      "Step 12: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011197\n",
      "Step 12: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.018129\n",
      "Step 12: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007835\n",
      "Step 12: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.007827\n",
      "Step 12: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.008201\n",
      "Step 12: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.013855\n",
      "Step 12: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010224\n",
      "Step 12: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.007009\n",
      "Step 12: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.014692\n",
      "Step 12: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.004889\n",
      "Step 12: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.011612\n",
      "Step 12: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006871\n",
      "Step 12: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.009567\n",
      "Step 12: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012956\n",
      "Step 12: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.007885\n",
      "Step 12: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013945\n",
      "Step 12: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.005220\n",
      "Step 12: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.012969\n",
      "Step 12: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010507\n",
      "time =  0:06:55.477195\n",
      "Step 13: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.007640\n",
      "Step 13: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.010150\n",
      "Step 13: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.007713\n",
      "Step 13: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.007661\n",
      "Step 13: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007221\n",
      "Step 13: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008289\n",
      "Step 13: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.014317\n",
      "Step 13: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.013059\n",
      "Step 13: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.005980\n",
      "Step 13: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011911\n",
      "Step 13: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.014895\n",
      "Step 13: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.010143\n",
      "Step 13: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.005788\n",
      "Step 13: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.005611\n",
      "Step 13: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006760\n",
      "Step 13: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.005932\n",
      "Step 13: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.008628\n",
      "Step 13: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.005568\n",
      "Step 13: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.012718\n",
      "Step 13: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.010010\n",
      "Step 13: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.008288\n",
      "Step 13: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.011165\n",
      "Step 13: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.012256\n",
      "Step 13: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009911\n",
      "Step 13: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.016070\n",
      "Step 13: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.015923\n",
      "Step 13: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.007721\n",
      "Step 13: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009082\n",
      "Step 13: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.013016\n",
      "Step 13: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010677\n",
      "Step 13: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.008757\n",
      "Step 13: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.012312\n",
      "Step 13: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011244\n",
      "Step 13: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008056\n",
      "Step 13: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.014275\n",
      "Step 13: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.012132\n",
      "Step 13: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.010405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.017645\n",
      "Step 13: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009569\n",
      "Step 13: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.010167\n",
      "Step 13: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011779\n",
      "Step 13: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009007\n",
      "Step 13: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.013246\n",
      "Step 13: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.004732\n",
      "Step 13: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009011\n",
      "Step 13: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.011136\n",
      "Step 13: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010792\n",
      "Step 13: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.009721\n",
      "Step 13: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.011651\n",
      "Step 13: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008872\n",
      "Step 13: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.007422\n",
      "Step 13: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007675\n",
      "Step 13: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.013692\n",
      "Step 13: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.011661\n",
      "Step 13: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.009404\n",
      "Step 13: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.020161\n",
      "Step 13: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.009646\n",
      "Step 13: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010988\n",
      "Step 13: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.012612\n",
      "Step 13: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.004683\n",
      "Step 13: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009047\n",
      "Step 13: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008447\n",
      "Step 13: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.005062\n",
      "Step 13: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011460\n",
      "Step 13: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.012022\n",
      "Step 13: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.008396\n",
      "Step 13: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.009598\n",
      "Step 13: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.010216\n",
      "Step 13: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.003636\n",
      "Step 13: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.005710\n",
      "Step 13: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.010663\n",
      "Step 13: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009223\n",
      "Step 13: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.003823\n",
      "Step 13: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.008744\n",
      "Step 13: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011276\n",
      "Step 13: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.005302\n",
      "Step 13: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.012303\n",
      "Step 13: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.012570\n",
      "Step 13: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008270\n",
      "Step 13: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012269\n",
      "Step 13: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007201\n",
      "Step 13: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.015658\n",
      "Step 13: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.006152\n",
      "Step 13: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.012309\n",
      "Step 13: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014295\n",
      "Step 13: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.013718\n",
      "Step 13: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.001862\n",
      "Step 13: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010440\n",
      "Step 13: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.004734\n",
      "Step 13: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.007214\n",
      "Step 13: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.006966\n",
      "Step 13: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.008446\n",
      "Step 13: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.004723\n",
      "Step 13: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.013811\n",
      "Step 13: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.005964\n",
      "Step 13: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.012815\n",
      "Step 13: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.008411\n",
      "Step 13: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.012099\n",
      "Step 13: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.010316\n",
      "Step 13: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008621\n",
      "Step 13: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.006644\n",
      "Step 13: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.005781\n",
      "Step 13: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.015993\n",
      "Step 13: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.013898\n",
      "Step 13: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009876\n",
      "Step 13: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008394\n",
      "Step 13: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.010997\n",
      "Step 13: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.012578\n",
      "Step 13: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.011033\n",
      "Step 13: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.005989\n",
      "Step 13: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.012973\n",
      "Step 13: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.013614\n",
      "Step 13: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.004763\n",
      "Step 13: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.017049\n",
      "Step 13: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.014887\n",
      "Step 13: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.008638\n",
      "Step 13: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.013736\n",
      "Step 13: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.005831\n",
      "Step 13: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.013457\n",
      "Step 13: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.008555\n",
      "Step 13: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.015876\n",
      "Step 13: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.006681\n",
      "Step 13: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.010237\n",
      "Step 13: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012494\n",
      "Step 13: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006431\n",
      "Step 13: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.008577\n",
      "Step 13: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.010461\n",
      "Step 13: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.007419\n",
      "Step 13: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.012359\n",
      "Step 13: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008167\n",
      "Step 13: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.016235\n",
      "Step 13: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010255\n",
      "Step 13: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.005032\n",
      "Step 13: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012469\n",
      "Step 13: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.009092\n",
      "Step 13: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.005483\n",
      "Step 13: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.002481\n",
      "Step 13: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.006613\n",
      "Step 13: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.005027\n",
      "Step 13: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.013167\n",
      "Step 13: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.014675\n",
      "Step 13: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.011859\n",
      "Step 13: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.012724\n",
      "Step 13: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.003716\n",
      "Step 13: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.016162\n",
      "Step 13: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009887\n",
      "Step 13: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.013906\n",
      "time =  0:06:58.070526\n",
      "Step 14: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.017766\n",
      "Step 14: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012210\n",
      "Step 14: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006732\n",
      "Step 14: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009617\n",
      "Step 14: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007985\n",
      "Step 14: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.012778\n",
      "Step 14: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.009374\n",
      "Step 14: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.006062\n",
      "Step 14: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.007577\n",
      "Step 14: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013187\n",
      "Step 14: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.013113\n",
      "Step 14: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008839\n",
      "Step 14: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.006723\n",
      "Step 14: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.005835\n",
      "Step 14: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.012773\n",
      "Step 14: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.017996\n",
      "Step 14: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012084\n",
      "Step 14: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.009542\n",
      "Step 14: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.006139\n",
      "Step 14: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011214\n",
      "Step 14: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.016764\n",
      "Step 14: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.013329\n",
      "Step 14: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.007634\n",
      "Step 14: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008693\n",
      "Step 14: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.014985\n",
      "Step 14: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.006453\n",
      "Step 14: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009260\n",
      "Step 14: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.012578\n",
      "Step 14: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.013166\n",
      "Step 14: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.013602\n",
      "Step 14: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.012499\n",
      "Step 14: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011278\n",
      "Step 14: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.007933\n",
      "Step 14: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.010184\n",
      "Step 14: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.018080\n",
      "Step 14: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.013443\n",
      "Step 14: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.012228\n",
      "Step 14: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.012771\n",
      "Step 14: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.010974\n",
      "Step 14: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.005160\n",
      "Step 14: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.010360\n",
      "Step 14: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.007636\n",
      "Step 14: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.013816\n",
      "Step 14: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.010786\n",
      "Step 14: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.006338\n",
      "Step 14: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.003906\n",
      "Step 14: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.004436\n",
      "Step 14: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.002703\n",
      "Step 14: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.014705\n",
      "Step 14: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.014569\n",
      "Step 14: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.006802\n",
      "Step 14: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.019004\n",
      "Step 14: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.012716\n",
      "Step 14: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.010681\n",
      "Step 14: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.001989\n",
      "Step 14: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014190\n",
      "Step 14: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.006146\n",
      "Step 14: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008135\n",
      "Step 14: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008809\n",
      "Step 14: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.005391\n",
      "Step 14: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013625\n",
      "Step 14: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.014072\n",
      "Step 14: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008132\n",
      "Step 14: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.005448\n",
      "Step 14: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.013744\n",
      "Step 14: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010472\n",
      "Step 14: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.006474\n",
      "Step 14: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.009407\n",
      "Step 14: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009368\n",
      "Step 14: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.018918\n",
      "Step 14: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010456\n",
      "Step 14: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.017231\n",
      "Step 14: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009335\n",
      "Step 14: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009932\n",
      "Step 14: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.021233\n",
      "Step 14: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.010062\n",
      "Step 14: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.015427\n",
      "Step 14: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.010899\n",
      "Step 14: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.007056\n",
      "Step 14: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.008023\n",
      "Step 14: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.013698\n",
      "Step 14: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.015046\n",
      "Step 14: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013162\n",
      "Step 14: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.017545\n",
      "Step 14: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.010331\n",
      "Step 14: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008188\n",
      "Step 14: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011696\n",
      "Step 14: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.013798\n",
      "Step 14: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009074\n",
      "Step 14: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.007451\n",
      "Step 14: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.002915\n",
      "Step 14: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.014329\n",
      "Step 14: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.010925\n",
      "Step 14: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.002557\n",
      "Step 14: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.005138\n",
      "Step 14: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.013754\n",
      "Step 14: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008971\n",
      "Step 14: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.005494\n",
      "Step 14: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.013546\n",
      "Step 14: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010171\n",
      "Step 14: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.005128\n",
      "Step 14: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.006417\n",
      "Step 14: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.013012\n",
      "Step 14: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.003508\n",
      "Step 14: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.018007\n",
      "Step 14: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.015003\n",
      "Step 14: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.014121\n",
      "Step 14: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.008317\n",
      "Step 14: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.008920\n",
      "Step 14: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.009672\n",
      "Step 14: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.013488\n",
      "Step 14: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.009355\n",
      "Step 14: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.006672\n",
      "Step 14: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.008594\n",
      "Step 14: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.012239\n",
      "Step 14: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.011823\n",
      "Step 14: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009242\n",
      "Step 14: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010690\n",
      "Step 14: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009941\n",
      "Step 14: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.007724\n",
      "Step 14: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.017985\n",
      "Step 14: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.007304\n",
      "Step 14: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.006396\n",
      "Step 14: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.010892\n",
      "Step 14: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012688\n",
      "Step 14: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012745\n",
      "Step 14: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.004247\n",
      "Step 14: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011439\n",
      "Step 14: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008678\n",
      "Step 14: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008251\n",
      "Step 14: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010766\n",
      "Step 14: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.006501\n",
      "Step 14: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.010877\n",
      "Step 14: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.005072\n",
      "Step 14: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010391\n",
      "Step 14: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.006623\n",
      "Step 14: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.004551\n",
      "Step 14: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.011425\n",
      "Step 14: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.017703\n",
      "Step 14: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.014663\n",
      "Step 14: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.008327\n",
      "Step 14: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.001541\n",
      "Step 14: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012573\n",
      "Step 14: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.009971\n",
      "Step 14: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.013336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.013442\n",
      "time =  0:06:56.698695\n",
      "Step 15: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.007522\n",
      "Step 15: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.007033\n",
      "Step 15: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.007526\n",
      "Step 15: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009286\n",
      "Step 15: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010262\n",
      "Step 15: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008845\n",
      "Step 15: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.007367\n",
      "Step 15: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.011048\n",
      "Step 15: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.013540\n",
      "Step 15: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.005492\n",
      "Step 15: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009701\n",
      "Step 15: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008870\n",
      "Step 15: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.014543\n",
      "Step 15: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.013853\n",
      "Step 15: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013020\n",
      "Step 15: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.010483\n",
      "Step 15: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.008787\n",
      "Step 15: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.011168\n",
      "Step 15: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.006872\n",
      "Step 15: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.006581\n",
      "Step 15: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.006736\n",
      "Step 15: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.012203\n",
      "Step 15: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.010303\n",
      "Step 15: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009462\n",
      "Step 15: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008482\n",
      "Step 15: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009850\n",
      "Step 15: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.005889\n",
      "Step 15: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.010899\n",
      "Step 15: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.015489\n",
      "Step 15: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010233\n",
      "Step 15: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.007681\n",
      "Step 15: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.015207\n",
      "Step 15: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.004732\n",
      "Step 15: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.018453\n",
      "Step 15: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.000619\n",
      "Step 15: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.012587\n",
      "Step 15: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006467\n",
      "Step 15: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.008886\n",
      "Step 15: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.004715\n",
      "Step 15: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009566\n",
      "Step 15: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.012238\n",
      "Step 15: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.002406\n",
      "Step 15: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010221\n",
      "Step 15: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.008067\n",
      "Step 15: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.007770\n",
      "Step 15: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.003648\n",
      "Step 15: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010717\n",
      "Step 15: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.005758\n",
      "Step 15: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.008630\n",
      "Step 15: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.004587\n",
      "Step 15: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.014867\n",
      "Step 15: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.010661\n",
      "Step 15: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.006796\n",
      "Step 15: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.009513\n",
      "Step 15: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.006944\n",
      "Step 15: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006190\n",
      "Step 15: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.011725\n",
      "Step 15: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.014253\n",
      "Step 15: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010914\n",
      "Step 15: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.007400\n",
      "Step 15: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009866\n",
      "Step 15: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006441\n",
      "Step 15: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012395\n",
      "Step 15: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009741\n",
      "Step 15: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.003588\n",
      "Step 15: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007707\n",
      "Step 15: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.003832\n",
      "Step 15: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.006152\n",
      "Step 15: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.006732\n",
      "Step 15: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.004388\n",
      "Step 15: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.012045\n",
      "Step 15: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.006607\n",
      "Step 15: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009343\n",
      "Step 15: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009383\n",
      "Step 15: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.007300\n",
      "Step 15: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.007181\n",
      "Step 15: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.013104\n",
      "Step 15: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.002399\n",
      "Step 15: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.007997\n",
      "Step 15: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.010504\n",
      "Step 15: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011654\n",
      "Step 15: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.009022\n",
      "Step 15: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009108\n",
      "Step 15: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.009053\n",
      "Step 15: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014735\n",
      "Step 15: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.015220\n",
      "Step 15: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010063\n",
      "Step 15: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010140\n",
      "Step 15: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.014041\n",
      "Step 15: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.012488\n",
      "Step 15: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.013923\n",
      "Step 15: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.005646\n",
      "Step 15: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.006371\n",
      "Step 15: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.007468\n",
      "Step 15: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.014495\n",
      "Step 15: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010590\n",
      "Step 15: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010332\n",
      "Step 15: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007303\n",
      "Step 15: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.005707\n",
      "Step 15: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.014215\n",
      "Step 15: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010227\n",
      "Step 15: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006914\n",
      "Step 15: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.013520\n",
      "Step 15: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.014148\n",
      "Step 15: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.010582\n",
      "Step 15: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009886\n",
      "Step 15: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.013598\n",
      "Step 15: Minibatch_NU: 10700/14680 Minibatch Loss: nan  0.999819\n",
      "Step 15: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.014470\n",
      "Step 15: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.008504\n",
      "Step 15: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008002\n",
      "Step 15: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.006759\n",
      "Step 15: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011296\n",
      "Step 15: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.016864\n",
      "Step 15: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010078\n",
      "Step 15: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011234\n",
      "Step 15: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.006562\n",
      "Step 15: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.008056\n",
      "Step 15: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010534\n",
      "Step 15: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.010791\n",
      "Step 15: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.006816\n",
      "Step 15: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.013654\n",
      "Step 15: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.028430\n",
      "Step 15: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.008424\n",
      "Step 15: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.015399\n",
      "Step 15: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.013203\n",
      "Step 15: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.005082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.007793\n",
      "Step 15: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.004514\n",
      "Step 15: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.013159\n",
      "Step 15: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.014245\n",
      "Step 15: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.017286\n",
      "Step 15: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009334\n",
      "Step 15: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.006963\n",
      "Step 15: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.014260\n",
      "Step 15: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.005806\n",
      "Step 15: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008859\n",
      "Step 15: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.012299\n",
      "Step 15: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.016806\n",
      "Step 15: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.016620\n",
      "Step 15: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008939\n",
      "Step 15: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012060\n",
      "Step 15: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008361\n",
      "Step 15: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008386\n",
      "Step 15: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008235\n",
      "Step 15: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.015589\n",
      "Step 15: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.016683\n",
      "time =  0:06:58.105131\n",
      "Step 16: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.014834\n",
      "Step 16: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.011304\n",
      "Step 16: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.010710\n",
      "Step 16: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.010949\n",
      "Step 16: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.009908\n",
      "Step 16: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.012726\n",
      "Step 16: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.005368\n",
      "Step 16: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.007381\n",
      "Step 16: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010689\n",
      "Step 16: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013089\n",
      "Step 16: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008392\n",
      "Step 16: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005826\n",
      "Step 16: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009646\n",
      "Step 16: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.006259\n",
      "Step 16: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006647\n",
      "Step 16: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.010224\n",
      "Step 16: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012510\n",
      "Step 16: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.010774\n",
      "Step 16: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.009037\n",
      "Step 16: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.015988\n",
      "Step 16: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.012582\n",
      "Step 16: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008967\n",
      "Step 16: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.004700\n",
      "Step 16: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.013891\n",
      "Step 16: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.015896\n",
      "Step 16: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.007330\n",
      "Step 16: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.011448\n",
      "Step 16: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009744\n",
      "Step 16: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007248\n",
      "Step 16: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010342\n",
      "Step 16: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.006435\n",
      "Step 16: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.004338\n",
      "Step 16: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.010155\n",
      "Step 16: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008812\n",
      "Step 16: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.010742\n",
      "Step 16: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.007231\n",
      "Step 16: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.012175\n",
      "Step 16: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.002394\n",
      "Step 16: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.015733\n",
      "Step 16: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009254\n",
      "Step 16: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009575\n",
      "Step 16: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.011220\n",
      "Step 16: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.007289\n",
      "Step 16: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.005395\n",
      "Step 16: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.007134\n",
      "Step 16: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.009219\n",
      "Step 16: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009500\n",
      "Step 16: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.005260\n",
      "Step 16: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.005687\n",
      "Step 16: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.009437\n",
      "Step 16: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.006688\n",
      "Step 16: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.002383\n",
      "Step 16: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.013005\n",
      "Step 16: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.013398\n",
      "Step 16: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.010122\n",
      "Step 16: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013135\n",
      "Step 16: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.010163\n",
      "Step 16: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011670\n",
      "Step 16: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.003430\n",
      "Step 16: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008767\n",
      "Step 16: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.008515\n",
      "Step 16: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.007266\n",
      "Step 16: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.006528\n",
      "Step 16: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.013426\n",
      "Step 16: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.009993\n",
      "Step 16: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.004407\n",
      "Step 16: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.013160\n",
      "Step 16: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.013177\n",
      "Step 16: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.011420\n",
      "Step 16: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009663\n",
      "Step 16: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.005133\n",
      "Step 16: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.013443\n",
      "Step 16: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009461\n",
      "Step 16: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009388\n",
      "Step 16: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.008353\n",
      "Step 16: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.014612\n",
      "Step 16: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.012239\n",
      "Step 16: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.011660\n",
      "Step 16: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008659\n",
      "Step 16: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012086\n",
      "Step 16: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009306\n",
      "Step 16: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.011836\n",
      "Step 16: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.013852\n",
      "Step 16: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.015310\n",
      "Step 16: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.006899\n",
      "Step 16: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.014427\n",
      "Step 16: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008116\n",
      "Step 16: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.005021\n",
      "Step 16: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010909\n",
      "Step 16: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.005844\n",
      "Step 16: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.012962\n",
      "Step 16: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.014206\n",
      "Step 16: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.015137\n",
      "Step 16: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.008776\n",
      "Step 16: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.008778\n",
      "Step 16: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.003891\n",
      "Step 16: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012477\n",
      "Step 16: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008285\n",
      "Step 16: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.002361\n",
      "Step 16: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.012017\n",
      "Step 16: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.012565\n",
      "Step 16: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.007212\n",
      "Step 16: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.004594\n",
      "Step 16: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.017256\n",
      "Step 16: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.007984\n",
      "Step 16: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.006899\n",
      "Step 16: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007236\n",
      "Step 16: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.006811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.011936\n",
      "Step 16: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.007882\n",
      "Step 16: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008879\n",
      "Step 16: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.013921\n",
      "Step 16: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.015723\n",
      "Step 16: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009968\n",
      "Step 16: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010732\n",
      "Step 16: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.013059\n",
      "Step 16: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.008199\n",
      "Step 16: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.014140\n",
      "Step 16: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011652\n",
      "Step 16: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.008750\n",
      "Step 16: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.007113\n",
      "Step 16: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011935\n",
      "Step 16: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.007051\n",
      "Step 16: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012687\n",
      "Step 16: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.002587\n",
      "Step 16: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.015158\n",
      "Step 16: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.007910\n",
      "Step 16: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011595\n",
      "Step 16: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.013385\n",
      "Step 16: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.002910\n",
      "Step 16: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007083\n",
      "Step 16: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.011503\n",
      "Step 16: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.003442\n",
      "Step 16: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.005191\n",
      "Step 16: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.004512\n",
      "Step 16: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.008382\n",
      "Step 16: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008107\n",
      "Step 16: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.006218\n",
      "Step 16: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.014440\n",
      "Step 16: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.009495\n",
      "Step 16: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008414\n",
      "Step 16: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.005099\n",
      "Step 16: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.010198\n",
      "Step 16: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012308\n",
      "Step 16: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.005203\n",
      "Step 16: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.015861\n",
      "Step 16: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.014908\n",
      "time =  0:06:59.523100\n",
      "Step 17: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.011668\n",
      "Step 17: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.013699\n",
      "Step 17: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.014961\n",
      "Step 17: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008693\n",
      "Step 17: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.009233\n",
      "Step 17: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009007\n",
      "Step 17: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.009623\n",
      "Step 17: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008987\n",
      "Step 17: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.006299\n",
      "Step 17: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010907\n",
      "Step 17: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009256\n",
      "Step 17: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.007729\n",
      "Step 17: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.003351\n",
      "Step 17: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.015670\n",
      "Step 17: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.010940\n",
      "Step 17: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.006835\n",
      "Step 17: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.007047\n",
      "Step 17: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009478\n",
      "Step 17: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.014154\n",
      "Step 17: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.009067\n",
      "Step 17: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011443\n",
      "Step 17: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.014133\n",
      "Step 17: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.019604\n",
      "Step 17: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.012673\n",
      "Step 17: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.006568\n",
      "Step 17: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.013313\n",
      "Step 17: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.005871\n",
      "Step 17: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.006062\n",
      "Step 17: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.008743\n",
      "Step 17: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011913\n",
      "Step 17: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.007555\n",
      "Step 17: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.009824\n",
      "Step 17: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.006963\n",
      "Step 17: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.006708\n",
      "Step 17: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.010394\n",
      "Step 17: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.017824\n",
      "Step 17: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006837\n",
      "Step 17: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.007725\n",
      "Step 17: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009882\n",
      "Step 17: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.011355\n",
      "Step 17: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.014872\n",
      "Step 17: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.013693\n",
      "Step 17: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.011592\n",
      "Step 17: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.014205\n",
      "Step 17: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.007592\n",
      "Step 17: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.008419\n",
      "Step 17: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009549\n",
      "Step 17: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.016193\n",
      "Step 17: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.011405\n",
      "Step 17: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.004977\n",
      "Step 17: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.006609\n",
      "Step 17: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.012817\n",
      "Step 17: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.015539\n",
      "Step 17: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010803\n",
      "Step 17: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.011244\n",
      "Step 17: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013188\n",
      "Step 17: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.006878\n",
      "Step 17: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011799\n",
      "Step 17: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.013602\n",
      "Step 17: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009858\n",
      "Step 17: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007305\n",
      "Step 17: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006397\n",
      "Step 17: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.005879\n",
      "Step 17: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.012292\n",
      "Step 17: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.013860\n",
      "Step 17: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.009130\n",
      "Step 17: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011511\n",
      "Step 17: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.000271\n",
      "Step 17: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.016706\n",
      "Step 17: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009658\n",
      "Step 17: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.012697\n",
      "Step 17: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.004890\n",
      "Step 17: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.006806\n",
      "Step 17: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009881\n",
      "Step 17: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.008472\n",
      "Step 17: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.003761\n",
      "Step 17: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.013230\n",
      "Step 17: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.012068\n",
      "Step 17: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.004802\n",
      "Step 17: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.005303\n",
      "Step 17: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007148\n",
      "Step 17: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.009225\n",
      "Step 17: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.012696\n",
      "Step 17: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.011968\n",
      "Step 17: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009848\n",
      "Step 17: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007991\n",
      "Step 17: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.005868\n",
      "Step 17: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.005677\n",
      "Step 17: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.016214\n",
      "Step 17: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.009839\n",
      "Step 17: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.004183\n",
      "Step 17: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.012063\n",
      "Step 17: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.005572\n",
      "Step 17: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.013219\n",
      "Step 17: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.013734\n",
      "Step 17: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.015365\n",
      "Step 17: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.011875\n",
      "Step 17: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.016041\n",
      "Step 17: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.009531\n",
      "Step 17: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.001861\n",
      "Step 17: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.009627\n",
      "Step 17: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.006628\n",
      "Step 17: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.012315\n",
      "Step 17: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.013944\n",
      "Step 17: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.012333\n",
      "Step 17: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.012599\n",
      "Step 17: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009038\n",
      "Step 17: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009024\n",
      "Step 17: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.007825\n",
      "Step 17: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008465\n",
      "Step 17: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.015395\n",
      "Step 17: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.008611\n",
      "Step 17: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.007754\n",
      "Step 17: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.013624\n",
      "Step 17: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.008127\n",
      "Step 17: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010732\n",
      "Step 17: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.011739\n",
      "Step 17: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011407\n",
      "Step 17: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.014405\n",
      "Step 17: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.013147\n",
      "Step 17: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.013096\n",
      "Step 17: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.008165\n",
      "Step 17: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.008073\n",
      "Step 17: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.008180\n",
      "Step 17: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.014790\n",
      "Step 17: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.009800\n",
      "Step 17: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.004259\n",
      "Step 17: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009356\n",
      "Step 17: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009029\n",
      "Step 17: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007150\n",
      "Step 17: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.015749\n",
      "Step 17: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.004825\n",
      "Step 17: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012068\n",
      "Step 17: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.007168\n",
      "Step 17: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.007697\n",
      "Step 17: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011054\n",
      "Step 17: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.009361\n",
      "Step 17: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.008745\n",
      "Step 17: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.008872\n",
      "Step 17: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.007162\n",
      "Step 17: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.009135\n",
      "Step 17: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008867\n",
      "Step 17: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009100\n",
      "Step 17: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.014546\n",
      "Step 17: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.007542\n",
      "Step 17: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.007450\n",
      "time =  0:07:00.493088\n",
      "Step 18: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.006802\n",
      "Step 18: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.016030\n",
      "Step 18: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.012050\n",
      "Step 18: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.010786\n",
      "Step 18: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.009290\n",
      "Step 18: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008514\n",
      "Step 18: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.002829\n",
      "Step 18: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.015417\n",
      "Step 18: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.005430\n",
      "Step 18: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.003992\n",
      "Step 18: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.011273\n",
      "Step 18: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005801\n",
      "Step 18: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.005225\n",
      "Step 18: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.010314\n",
      "Step 18: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.009988\n",
      "Step 18: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.008646\n",
      "Step 18: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.009056\n",
      "Step 18: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.007235\n",
      "Step 18: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.009321\n",
      "Step 18: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.010961\n",
      "Step 18: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010768\n",
      "Step 18: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.014860\n",
      "Step 18: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009387\n",
      "Step 18: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.005874\n",
      "Step 18: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.015949\n",
      "Step 18: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.003216\n",
      "Step 18: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.014024\n",
      "Step 18: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.012024\n",
      "Step 18: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.013408\n",
      "Step 18: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.003861\n",
      "Step 18: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.008709\n",
      "Step 18: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.012314\n",
      "Step 18: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.014289\n",
      "Step 18: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.013428\n",
      "Step 18: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009519\n",
      "Step 18: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.010733\n",
      "Step 18: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.008854\n",
      "Step 18: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.009285\n",
      "Step 18: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009426\n",
      "Step 18: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009146\n",
      "Step 18: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.008517\n",
      "Step 18: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.010227\n",
      "Step 18: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.005561\n",
      "Step 18: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.009297\n",
      "Step 18: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.010968\n",
      "Step 18: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.014241\n",
      "Step 18: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.008777\n",
      "Step 18: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.008101\n",
      "Step 18: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009790\n",
      "Step 18: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.007921\n",
      "Step 18: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005197\n",
      "Step 18: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.008598\n",
      "Step 18: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.013903\n",
      "Step 18: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.016199\n",
      "Step 18: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.012220\n",
      "Step 18: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.011390\n",
      "Step 18: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008079\n",
      "Step 18: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011035\n",
      "Step 18: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.011297\n",
      "Step 18: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.006785\n",
      "Step 18: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.015462\n",
      "Step 18: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.011403\n",
      "Step 18: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012569\n",
      "Step 18: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.010008\n",
      "Step 18: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.014121\n",
      "Step 18: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.015982\n",
      "Step 18: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.005533\n",
      "Step 18: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009894\n",
      "Step 18: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.009366\n",
      "Step 18: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.011289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.009642\n",
      "Step 18: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010560\n",
      "Step 18: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.007426\n",
      "Step 18: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.005527\n",
      "Step 18: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.010597\n",
      "Step 18: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.006327\n",
      "Step 18: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.007247\n",
      "Step 18: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009158\n",
      "Step 18: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.015036\n",
      "Step 18: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.016528\n",
      "Step 18: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007944\n",
      "Step 18: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.009785\n",
      "Step 18: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008314\n",
      "Step 18: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.010315\n",
      "Step 18: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.011582\n",
      "Step 18: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007439\n",
      "Step 18: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.007389\n",
      "Step 18: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.013836\n",
      "Step 18: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010035\n",
      "Step 18: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.013433\n",
      "Step 18: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.005029\n",
      "Step 18: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.009885\n",
      "Step 18: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.013353\n",
      "Step 18: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.009318\n",
      "Step 18: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.010799\n",
      "Step 18: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.013165\n",
      "Step 18: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.007173\n",
      "Step 18: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.009255\n",
      "Step 18: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.004190\n",
      "Step 18: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008582\n",
      "Step 18: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.012293\n",
      "Step 18: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.012389\n",
      "Step 18: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.002762\n",
      "Step 18: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011794\n",
      "Step 18: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009727\n",
      "Step 18: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.006925\n",
      "Step 18: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.010371\n",
      "Step 18: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.007736\n",
      "Step 18: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.005230\n",
      "Step 18: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.022092\n",
      "Step 18: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010283\n",
      "Step 18: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011111\n",
      "Step 18: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.004621\n",
      "Step 18: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.017363\n",
      "Step 18: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009432\n",
      "Step 18: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007979\n",
      "Step 18: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.012537\n",
      "Step 18: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.007557\n",
      "Step 18: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011466\n",
      "Step 18: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009583\n",
      "Step 18: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008027\n",
      "Step 18: Minibatch_NU: 12100/14680 Minibatch Loss: nan  0.999378\n",
      "Step 18: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.004888\n",
      "Step 18: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.007970\n",
      "Step 18: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.012885\n",
      "Step 18: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.014922\n",
      "Step 18: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.017990\n",
      "Step 18: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011102\n",
      "Step 18: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007826\n",
      "Step 18: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009075\n",
      "Step 18: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.009458\n",
      "Step 18: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.009124\n",
      "Step 18: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.008798\n",
      "Step 18: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.008226\n",
      "Step 18: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010342\n",
      "Step 18: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.017053\n",
      "Step 18: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.009384\n",
      "Step 18: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.005650\n",
      "Step 18: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.006838\n",
      "Step 18: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006038\n",
      "Step 18: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.004628\n",
      "Step 18: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012305\n",
      "Step 18: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.011817\n",
      "Step 18: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.011681\n",
      "Step 18: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.012851\n",
      "Step 18: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.007138\n",
      "Step 18: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.007910\n",
      "time =  0:06:57.442165\n",
      "Step 19: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.011702\n",
      "Step 19: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.007458\n",
      "Step 19: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006160\n",
      "Step 19: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.012663\n",
      "Step 19: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.018696\n",
      "Step 19: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.017193\n",
      "Step 19: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.016628\n",
      "Step 19: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.006515\n",
      "Step 19: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.012579\n",
      "Step 19: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.004306\n",
      "Step 19: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.015477\n",
      "Step 19: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.012548\n",
      "Step 19: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.007252\n",
      "Step 19: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.012666\n",
      "Step 19: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013321\n",
      "Step 19: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009604\n",
      "Step 19: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.007876\n",
      "Step 19: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009535\n",
      "Step 19: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.013204\n",
      "Step 19: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.012646\n",
      "Step 19: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010219\n",
      "Step 19: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010103\n",
      "Step 19: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.010338\n",
      "Step 19: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.007156\n",
      "Step 19: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.006503\n",
      "Step 19: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009026\n",
      "Step 19: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.014359\n",
      "Step 19: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.004013\n",
      "Step 19: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009858\n",
      "Step 19: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.006117\n",
      "Step 19: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.015694\n",
      "Step 19: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.013087\n",
      "Step 19: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.010039\n",
      "Step 19: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.011382\n",
      "Step 19: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.016545\n",
      "Step 19: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.010457\n",
      "Step 19: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.008602\n",
      "Step 19: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.014082\n",
      "Step 19: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010289\n",
      "Step 19: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.007239\n",
      "Step 19: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.015501\n",
      "Step 19: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.006585\n",
      "Step 19: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.009804\n",
      "Step 19: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.011252\n",
      "Step 19: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.015845\n",
      "Step 19: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.007497\n",
      "Step 19: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009338\n",
      "Step 19: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.005548\n",
      "Step 19: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.008644\n",
      "Step 19: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.007100\n",
      "Step 19: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.014586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.004977\n",
      "Step 19: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.011615\n",
      "Step 19: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.007763\n",
      "Step 19: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.011519\n",
      "Step 19: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006228\n",
      "Step 19: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.012295\n",
      "Step 19: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.012124\n",
      "Step 19: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008380\n",
      "Step 19: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.010966\n",
      "Step 19: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.017484\n",
      "Step 19: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.015041\n",
      "Step 19: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.005819\n",
      "Step 19: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.010125\n",
      "Step 19: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.013150\n",
      "Step 19: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007409\n",
      "Step 19: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011997\n",
      "Step 19: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008705\n",
      "Step 19: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.019052\n",
      "Step 19: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.020178\n",
      "Step 19: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.006692\n",
      "Step 19: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.012878\n",
      "Step 19: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.013029\n",
      "Step 19: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009713\n",
      "Step 19: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011438\n",
      "Step 19: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011124\n",
      "Step 19: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.011784\n",
      "Step 19: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.011440\n",
      "Step 19: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009238\n",
      "Step 19: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.013058\n",
      "Step 19: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009645\n",
      "Step 19: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008666\n",
      "Step 19: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008281\n",
      "Step 19: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.007421\n",
      "Step 19: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.010303\n",
      "Step 19: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007048\n",
      "Step 19: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010336\n",
      "Step 19: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011387\n",
      "Step 19: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012188\n",
      "Step 19: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.011021\n",
      "Step 19: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.009111\n",
      "Step 19: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.004722\n",
      "Step 19: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.012504\n",
      "Step 19: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.009259\n",
      "Step 19: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.008921\n",
      "Step 19: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.008558\n",
      "Step 19: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010716\n",
      "Step 19: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.015114\n",
      "Step 19: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.011553\n",
      "Step 19: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.012423\n",
      "Step 19: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008680\n",
      "Step 19: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.008567\n",
      "Step 19: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.009562\n",
      "Step 19: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.005433\n",
      "Step 19: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.008705\n",
      "Step 19: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.007475\n",
      "Step 19: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.014402\n",
      "Step 19: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015125\n",
      "Step 19: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.014713\n",
      "Step 19: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.004861\n",
      "Step 19: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.009915\n",
      "Step 19: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.010784\n",
      "Step 19: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.019045\n",
      "Step 19: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.006514\n",
      "Step 19: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010050\n",
      "Step 19: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007901\n",
      "Step 19: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.005245\n",
      "Step 19: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.007257\n",
      "Step 19: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011829\n",
      "Step 19: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.011893\n",
      "Step 19: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.012213\n",
      "Step 19: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011341\n",
      "Step 19: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.008534\n",
      "Step 19: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.011827\n",
      "Step 19: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.013323\n",
      "Step 19: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.013625\n",
      "Step 19: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.000190\n",
      "Step 19: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009519\n",
      "Step 19: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007578\n",
      "Step 19: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009630\n",
      "Step 19: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.014998\n",
      "Step 19: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.012148\n",
      "Step 19: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009566\n",
      "Step 19: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.013193\n",
      "Step 19: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.008563\n",
      "Step 19: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.012694\n",
      "Step 19: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011386\n",
      "Step 19: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.005636\n",
      "Step 19: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.006057\n",
      "Step 19: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.008563\n",
      "Step 19: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.009677\n",
      "Step 19: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007562\n",
      "Step 19: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.005995\n",
      "Step 19: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.011860\n",
      "Step 19: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010146\n",
      "Step 19: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.014241\n",
      "Step 19: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.012820\n",
      "time =  0:06:55.603007\n",
      "Step 20: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.002699\n",
      "Step 20: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.013176\n",
      "Step 20: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.012264\n",
      "Step 20: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.012916\n",
      "Step 20: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007815\n",
      "Step 20: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009824\n",
      "Step 20: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.010672\n",
      "Step 20: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009465\n",
      "Step 20: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009089\n",
      "Step 20: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.008507\n",
      "Step 20: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.006050\n",
      "Step 20: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.009813\n",
      "Step 20: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.012582\n",
      "Step 20: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.014226\n",
      "Step 20: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006286\n",
      "Step 20: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012260\n",
      "Step 20: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.007998\n",
      "Step 20: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.010168\n",
      "Step 20: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.004096\n",
      "Step 20: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011968\n",
      "Step 20: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.007297\n",
      "Step 20: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.013946\n",
      "Step 20: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.011293\n",
      "Step 20: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.004102\n",
      "Step 20: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.012030\n",
      "Step 20: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.007175\n",
      "Step 20: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.012284\n",
      "Step 20: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.013986\n",
      "Step 20: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.013680\n",
      "Step 20: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011369\n",
      "Step 20: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.015528\n",
      "Step 20: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.008582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.008917\n",
      "Step 20: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008303\n",
      "Step 20: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.012890\n",
      "Step 20: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.013767\n",
      "Step 20: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.013351\n",
      "Step 20: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010733\n",
      "Step 20: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.002878\n",
      "Step 20: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.015233\n",
      "Step 20: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.012622\n",
      "Step 20: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.007073\n",
      "Step 20: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.009118\n",
      "Step 20: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.007231\n",
      "Step 20: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.014279\n",
      "Step 20: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.005509\n",
      "Step 20: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.013106\n",
      "Step 20: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.007575\n",
      "Step 20: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.015171\n",
      "Step 20: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.006736\n",
      "Step 20: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.014479\n",
      "Step 20: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.011416\n",
      "Step 20: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.005059\n",
      "Step 20: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.007278\n",
      "Step 20: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.011464\n",
      "Step 20: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013037\n",
      "Step 20: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.002032\n",
      "Step 20: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.019282\n",
      "Step 20: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.007760\n",
      "Step 20: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009393\n",
      "Step 20: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.006322\n",
      "Step 20: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006891\n",
      "Step 20: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.005486\n",
      "Step 20: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.010669\n",
      "Step 20: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.013480\n",
      "Step 20: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.009853\n",
      "Step 20: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010333\n",
      "Step 20: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008253\n",
      "Step 20: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.013654\n",
      "Step 20: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.007732\n",
      "Step 20: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.006540\n",
      "Step 20: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.016343\n",
      "Step 20: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.012017\n",
      "Step 20: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.013659\n",
      "Step 20: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011348\n",
      "Step 20: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011724\n",
      "Step 20: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.011412\n",
      "Step 20: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.011757\n",
      "Step 20: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.013370\n",
      "Step 20: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.010729\n",
      "Step 20: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011087\n",
      "Step 20: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.006504\n",
      "Step 20: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.006326\n",
      "Step 20: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.007941\n",
      "Step 20: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005983\n",
      "Step 20: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.010823\n",
      "Step 20: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010701\n",
      "Step 20: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.008584\n",
      "Step 20: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007465\n",
      "Step 20: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.005360\n",
      "Step 20: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.011396\n",
      "Step 20: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.005373\n",
      "Step 20: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.016337\n",
      "Step 20: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.011755\n",
      "Step 20: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.006413\n",
      "Step 20: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.011341\n",
      "Step 20: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010678\n",
      "Step 20: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007219\n",
      "Step 20: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.006246\n",
      "Step 20: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.010067\n",
      "Step 20: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008031\n",
      "Step 20: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.010912\n",
      "Step 20: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.011333\n",
      "Step 20: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.008383\n",
      "Step 20: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.005758\n",
      "Step 20: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009845\n",
      "Step 20: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.010783\n",
      "Step 20: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.011110\n",
      "Step 20: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.004097\n",
      "Step 20: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.007943\n",
      "Step 20: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010522\n",
      "Step 20: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011551\n",
      "Step 20: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011914\n",
      "Step 20: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.017443\n",
      "Step 20: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009465\n",
      "Step 20: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.014963\n",
      "Step 20: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.011991\n",
      "Step 20: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.012115\n",
      "Step 20: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.009151\n",
      "Step 20: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.004411\n",
      "Step 20: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.016513\n",
      "Step 20: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.007364\n",
      "Step 20: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.008194\n",
      "Step 20: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.006178\n",
      "Step 20: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.010121\n",
      "Step 20: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.009039\n",
      "Step 20: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.004599\n",
      "Step 20: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011101\n",
      "Step 20: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009066\n",
      "Step 20: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009300\n",
      "Step 20: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007342\n",
      "Step 20: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.007384\n",
      "Step 20: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.012385\n",
      "Step 20: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.005334\n",
      "Step 20: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.006775\n",
      "Step 20: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009498\n",
      "Step 20: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.007539\n",
      "Step 20: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.012334\n",
      "Step 20: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.006944\n",
      "Step 20: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.010772\n",
      "Step 20: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.012653\n",
      "Step 20: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.006635\n",
      "Step 20: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.010408\n",
      "Step 20: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008496\n",
      "Step 20: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.005902\n",
      "Step 20: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011962\n",
      "Step 20: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.013357\n",
      "time =  0:06:56.876242\n",
      "Step 21: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.010736\n",
      "Step 21: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009664\n",
      "Step 21: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.009579\n",
      "Step 21: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009424\n",
      "Step 21: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.015967\n",
      "Step 21: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008504\n",
      "Step 21: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.004675\n",
      "Step 21: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.016559\n",
      "Step 21: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010505\n",
      "Step 21: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.004258\n",
      "Step 21: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.013778\n",
      "Step 21: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005497\n",
      "Step 21: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.005633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.010418\n",
      "Step 21: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.004842\n",
      "Step 21: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009195\n",
      "Step 21: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.003210\n",
      "Step 21: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.006549\n",
      "Step 21: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010197\n",
      "Step 21: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.012130\n",
      "Step 21: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011392\n",
      "Step 21: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008296\n",
      "Step 21: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.016673\n",
      "Step 21: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008284\n",
      "Step 21: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008481\n",
      "Step 21: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.016242\n",
      "Step 21: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008767\n",
      "Step 21: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.015610\n",
      "Step 21: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007209\n",
      "Step 21: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.014318\n",
      "Step 21: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.010182\n",
      "Step 21: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.014708\n",
      "Step 21: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.007085\n",
      "Step 21: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.006783\n",
      "Step 21: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.005272\n",
      "Step 21: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.012132\n",
      "Step 21: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.007970\n",
      "Step 21: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.012407\n",
      "Step 21: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.011832\n",
      "Step 21: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.014867\n",
      "Step 21: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.008194\n",
      "Step 21: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.008951\n",
      "Step 21: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.004792\n",
      "Step 21: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.004337\n",
      "Step 21: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.013848\n",
      "Step 21: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010646\n",
      "Step 21: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010155\n",
      "Step 21: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.016595\n",
      "Step 21: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.006095\n",
      "Step 21: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.015568\n",
      "Step 21: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.002771\n",
      "Step 21: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.014011\n",
      "Step 21: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.006902\n",
      "Step 21: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.008046\n",
      "Step 21: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.008542\n",
      "Step 21: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.001311\n",
      "Step 21: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.015402\n",
      "Step 21: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011186\n",
      "Step 21: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010405\n",
      "Step 21: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.010260\n",
      "Step 21: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.012377\n",
      "Step 21: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.010199\n",
      "Step 21: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.009186\n",
      "Step 21: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.007101\n",
      "Step 21: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.008262\n",
      "Step 21: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010048\n",
      "Step 21: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.003052\n",
      "Step 21: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.018787\n",
      "Step 21: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.007366\n",
      "Step 21: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008925\n",
      "Step 21: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.004715\n",
      "Step 21: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.013801\n",
      "Step 21: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.005088\n",
      "Step 21: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009900\n",
      "Step 21: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.008593\n",
      "Step 21: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010339\n",
      "Step 21: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.004330\n",
      "Step 21: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009286\n",
      "Step 21: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.006137\n",
      "Step 21: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.005489\n",
      "Step 21: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.015582\n",
      "Step 21: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008845\n",
      "Step 21: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.014742\n",
      "Step 21: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013951\n",
      "Step 21: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014201\n",
      "Step 21: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.015422\n",
      "Step 21: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008967\n",
      "Step 21: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.004219\n",
      "Step 21: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.006642\n",
      "Step 21: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.012719\n",
      "Step 21: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.011102\n",
      "Step 21: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.010393\n",
      "Step 21: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.012218\n",
      "Step 21: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.001982\n",
      "Step 21: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.010614\n",
      "Step 21: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.011071\n",
      "Step 21: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.008816\n",
      "Step 21: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.006378\n",
      "Step 21: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.012646\n",
      "Step 21: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.003975\n",
      "Step 21: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009001\n",
      "Step 21: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.014423\n",
      "Step 21: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.013647\n",
      "Step 21: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.014306\n",
      "Step 21: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.008670\n",
      "Step 21: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.014302\n",
      "Step 21: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.012227\n",
      "Step 21: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.011293\n",
      "Step 21: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.013216\n",
      "Step 21: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.013882\n",
      "Step 21: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010126\n",
      "Step 21: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008138\n",
      "Step 21: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010014\n",
      "Step 21: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.004573\n",
      "Step 21: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010985\n",
      "Step 21: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.012671\n",
      "Step 21: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.007945\n",
      "Step 21: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.007254\n",
      "Step 21: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.006035\n",
      "Step 21: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009510\n",
      "Step 21: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.011943\n",
      "Step 21: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.014508\n",
      "Step 21: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.009055\n",
      "Step 21: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.010992\n",
      "Step 21: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.008852\n",
      "Step 21: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012123\n",
      "Step 21: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.006108\n",
      "Step 21: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.013121\n",
      "Step 21: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.006902\n",
      "Step 21: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.010783\n",
      "Step 21: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.005721\n",
      "Step 21: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.009796\n",
      "Step 21: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.010475\n",
      "Step 21: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.007047\n",
      "Step 21: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.007838\n",
      "Step 21: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011250\n",
      "Step 21: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010608\n",
      "Step 21: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.006534\n",
      "Step 21: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.007165\n",
      "Step 21: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006541\n",
      "Step 21: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.011016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.016244\n",
      "Step 21: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008442\n",
      "Step 21: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012170\n",
      "Step 21: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.013294\n",
      "Step 21: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009247\n",
      "Step 21: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.011432\n",
      "time =  0:06:57.068130\n",
      "Step 22: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.012317\n",
      "Step 22: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012966\n",
      "Step 22: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.009751\n",
      "Step 22: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.013071\n",
      "Step 22: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007857\n",
      "Step 22: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.013746\n",
      "Step 22: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.009476\n",
      "Step 22: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.002941\n",
      "Step 22: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.005003\n",
      "Step 22: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.009845\n",
      "Step 22: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.017279\n",
      "Step 22: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008975\n",
      "Step 22: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009718\n",
      "Step 22: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.015688\n",
      "Step 22: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.011615\n",
      "Step 22: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012089\n",
      "Step 22: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.003973\n",
      "Step 22: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.007930\n",
      "Step 22: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.008768\n",
      "Step 22: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.009252\n",
      "Step 22: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.007605\n",
      "Step 22: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009305\n",
      "Step 22: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.006673\n",
      "Step 22: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.004858\n",
      "Step 22: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.012073\n",
      "Step 22: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.017509\n",
      "Step 22: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.009973\n",
      "Step 22: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.011245\n",
      "Step 22: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007513\n",
      "Step 22: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008996\n",
      "Step 22: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009717\n",
      "Step 22: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.017912\n",
      "Step 22: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.013468\n",
      "Step 22: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.012592\n",
      "Step 22: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.007794\n",
      "Step 22: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.014233\n",
      "Step 22: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.007420\n",
      "Step 22: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.007248\n",
      "Step 22: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.014954\n",
      "Step 22: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.004811\n",
      "Step 22: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.006963\n",
      "Step 22: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.004571\n",
      "Step 22: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010790\n",
      "Step 22: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.013871\n",
      "Step 22: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.014463\n",
      "Step 22: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.006843\n",
      "Step 22: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.004769\n",
      "Step 22: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006766\n",
      "Step 22: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.011935\n",
      "Step 22: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008619\n",
      "Step 22: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.010039\n",
      "Step 22: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.010393\n",
      "Step 22: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.008971\n",
      "Step 22: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010565\n",
      "Step 22: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.018530\n",
      "Step 22: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006129\n",
      "Step 22: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.007915\n",
      "Step 22: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010932\n",
      "Step 22: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.009910\n",
      "Step 22: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.011764\n",
      "Step 22: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009010\n",
      "Step 22: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.012754\n",
      "Step 22: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.014157\n",
      "Step 22: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008382\n",
      "Step 22: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.016243\n",
      "Step 22: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.009279\n",
      "Step 22: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.008678\n",
      "Step 22: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.014774\n",
      "Step 22: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.007839\n",
      "Step 22: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008219\n",
      "Step 22: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.007850\n",
      "Step 22: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.004994\n",
      "Step 22: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.008532\n",
      "Step 22: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009384\n",
      "Step 22: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.010067\n",
      "Step 22: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.007097\n",
      "Step 22: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.016751\n",
      "Step 22: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.007506\n",
      "Step 22: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.007175\n",
      "Step 22: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.006186\n",
      "Step 22: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009057\n",
      "Step 22: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.002452\n",
      "Step 22: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011496\n",
      "Step 22: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.008650\n",
      "Step 22: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014510\n",
      "Step 22: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.006774\n",
      "Step 22: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.003059\n",
      "Step 22: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.007932\n",
      "Step 22: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012343\n",
      "Step 22: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.008559\n",
      "Step 22: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.010379\n",
      "Step 22: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.009434\n",
      "Step 22: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.010885\n",
      "Step 22: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.009485\n",
      "Step 22: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.015475\n",
      "Step 22: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.004241\n",
      "Step 22: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.006588\n",
      "Step 22: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.009492\n",
      "Step 22: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.013377\n",
      "Step 22: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.010728\n",
      "Step 22: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.017394\n",
      "Step 22: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.010535\n",
      "Step 22: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.004860\n",
      "Step 22: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.013052\n",
      "Step 22: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.018272\n",
      "Step 22: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.012468\n",
      "Step 22: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007539\n",
      "Step 22: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009382\n",
      "Step 22: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.013440\n",
      "Step 22: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.009989\n",
      "Step 22: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.009095\n",
      "Step 22: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011624\n",
      "Step 22: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010402\n",
      "Step 22: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.004387\n",
      "Step 22: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.012534\n",
      "Step 22: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009761\n",
      "Step 22: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.012446\n",
      "Step 22: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.010985\n",
      "Step 22: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.013756\n",
      "Step 22: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.018840\n",
      "Step 22: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.004416\n",
      "Step 22: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.015887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.010464\n",
      "Step 22: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.003759\n",
      "Step 22: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006300\n",
      "Step 22: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.007617\n",
      "Step 22: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.011209\n",
      "Step 22: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.012206\n",
      "Step 22: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.004894\n",
      "Step 22: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008107\n",
      "Step 22: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.013076\n",
      "Step 22: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.012014\n",
      "Step 22: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.005276\n",
      "Step 22: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.007027\n",
      "Step 22: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.008231\n",
      "Step 22: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.012031\n",
      "Step 22: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.005362\n",
      "Step 22: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.008280\n",
      "Step 22: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.007654\n",
      "Step 22: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.010594\n",
      "Step 22: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.010957\n",
      "Step 22: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.010267\n",
      "Step 22: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.017196\n",
      "Step 22: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009791\n",
      "Step 22: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.005461\n",
      "Step 22: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011778\n",
      "Step 22: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.005717\n",
      "time =  0:06:56.417473\n",
      "Step 23: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.010874\n",
      "Step 23: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.003231\n",
      "Step 23: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006467\n",
      "Step 23: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.013501\n",
      "Step 23: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.005462\n",
      "Step 23: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.011436\n",
      "Step 23: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.012588\n",
      "Step 23: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.010784\n",
      "Step 23: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.013320\n",
      "Step 23: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.008582\n",
      "Step 23: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.013624\n",
      "Step 23: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.010733\n",
      "Step 23: Minibatch_NU: 1200/14680 Minibatch Loss: nan  0.999649\n",
      "Step 23: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.010880\n",
      "Step 23: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.007924\n",
      "Step 23: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.008983\n",
      "Step 23: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.011858\n",
      "Step 23: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012608\n",
      "Step 23: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.009830\n",
      "Step 23: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.007192\n",
      "Step 23: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.014230\n",
      "Step 23: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010966\n",
      "Step 23: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.004968\n",
      "Step 23: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.012490\n",
      "Step 23: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.007143\n",
      "Step 23: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009409\n",
      "Step 23: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.007767\n",
      "Step 23: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.013896\n",
      "Step 23: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009637\n",
      "Step 23: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008115\n",
      "Step 23: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.012551\n",
      "Step 23: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.014116\n",
      "Step 23: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.014188\n",
      "Step 23: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008128\n",
      "Step 23: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.005068\n",
      "Step 23: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.010019\n",
      "Step 23: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.011466\n",
      "Step 23: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.014890\n",
      "Step 23: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.008903\n",
      "Step 23: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.008939\n",
      "Step 23: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009451\n",
      "Step 23: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.004148\n",
      "Step 23: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.009279\n",
      "Step 23: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.016145\n",
      "Step 23: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.003897\n",
      "Step 23: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.014523\n",
      "Step 23: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.004540\n",
      "Step 23: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.007419\n",
      "Step 23: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.006409\n",
      "Step 23: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.004953\n",
      "Step 23: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.009484\n",
      "Step 23: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.013480\n",
      "Step 23: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010906\n",
      "Step 23: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.009343\n",
      "Step 23: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.004814\n",
      "Step 23: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.010789\n",
      "Step 23: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008372\n",
      "Step 23: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.006071\n",
      "Step 23: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.009795\n",
      "Step 23: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.013791\n",
      "Step 23: Minibatch_NU: 6000/14680 Minibatch Loss: nan  0.997811\n",
      "Step 23: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.011023\n",
      "Step 23: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012398\n",
      "Step 23: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.006290\n",
      "Step 23: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.007275\n",
      "Step 23: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.017001\n",
      "Step 23: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011632\n",
      "Step 23: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.011125\n",
      "Step 23: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.012649\n",
      "Step 23: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.012828\n",
      "Step 23: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.008909\n",
      "Step 23: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009379\n",
      "Step 23: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.010904\n",
      "Step 23: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.008054\n",
      "Step 23: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009405\n",
      "Step 23: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010448\n",
      "Step 23: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.014104\n",
      "Step 23: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.007111\n",
      "Step 23: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.006317\n",
      "Step 23: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.013619\n",
      "Step 23: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.014341\n",
      "Step 23: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.006034\n",
      "Step 23: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009553\n",
      "Step 23: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.005824\n",
      "Step 23: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005053\n",
      "Step 23: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.011065\n",
      "Step 23: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.005988\n",
      "Step 23: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.008243\n",
      "Step 23: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.017032\n",
      "Step 23: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.004112\n",
      "Step 23: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.010687\n",
      "Step 23: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.009850\n",
      "Step 23: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.010556\n",
      "Step 23: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.013039\n",
      "Step 23: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007313\n",
      "Step 23: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.004791\n",
      "Step 23: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012064\n",
      "Step 23: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.013001\n",
      "Step 23: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.012191\n",
      "Step 23: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008878\n",
      "Step 23: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.003998\n",
      "Step 23: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.005568\n",
      "Step 23: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.007678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.003984\n",
      "Step 23: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.014630\n",
      "Step 23: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010996\n",
      "Step 23: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.005962\n",
      "Step 23: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009677\n",
      "Step 23: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.013650\n",
      "Step 23: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011585\n",
      "Step 23: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.011321\n",
      "Step 23: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011353\n",
      "Step 23: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.013109\n",
      "Step 23: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009828\n",
      "Step 23: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009858\n",
      "Step 23: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009835\n",
      "Step 23: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.016688\n",
      "Step 23: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009990\n",
      "Step 23: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010122\n",
      "Step 23: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009923\n",
      "Step 23: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.007897\n",
      "Step 23: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.010867\n",
      "Step 23: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.012682\n",
      "Step 23: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.013195\n",
      "Step 23: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.011534\n",
      "Step 23: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.002910\n",
      "Step 23: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.016207\n",
      "Step 23: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.012232\n",
      "Step 23: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.008429\n",
      "Step 23: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008176\n",
      "Step 23: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.013422\n",
      "Step 23: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.009189\n",
      "Step 23: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.003912\n",
      "Step 23: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.005928\n",
      "Step 23: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.018287\n",
      "Step 23: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009863\n",
      "Step 23: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008594\n",
      "Step 23: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.010276\n",
      "Step 23: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.005539\n",
      "Step 23: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006005\n",
      "Step 23: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.006138\n",
      "Step 23: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.001502\n",
      "Step 23: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.017930\n",
      "Step 23: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.010882\n",
      "Step 23: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.003654\n",
      "Step 23: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.015368\n",
      "Step 23: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.012942\n",
      "time =  0:06:56.166029\n",
      "Step 24: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.013656\n",
      "Step 24: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012836\n",
      "Step 24: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.014429\n",
      "Step 24: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.007063\n",
      "Step 24: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.018766\n",
      "Step 24: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.014084\n",
      "Step 24: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.001566\n",
      "Step 24: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009538\n",
      "Step 24: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.008972\n",
      "Step 24: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010278\n",
      "Step 24: Minibatch_NU: 1000/14680 Minibatch Loss: nan  0.997236\n",
      "Step 24: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.003513\n",
      "Step 24: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.012387\n",
      "Step 24: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.006973\n",
      "Step 24: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.008149\n",
      "Step 24: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009713\n",
      "Step 24: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.014053\n",
      "Step 24: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009256\n",
      "Step 24: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.012614\n",
      "Step 24: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.008337\n",
      "Step 24: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.007159\n",
      "Step 24: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.012441\n",
      "Step 24: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.007378\n",
      "Step 24: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.006278\n",
      "Step 24: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.017584\n",
      "Step 24: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009480\n",
      "Step 24: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.014290\n",
      "Step 24: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007365\n",
      "Step 24: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.012358\n",
      "Step 24: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011063\n",
      "Step 24: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009745\n",
      "Step 24: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.018995\n",
      "Step 24: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.008080\n",
      "Step 24: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.007356\n",
      "Step 24: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.017478\n",
      "Step 24: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.017078\n",
      "Step 24: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006956\n",
      "Step 24: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.016253\n",
      "Step 24: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.004167\n",
      "Step 24: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009079\n",
      "Step 24: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.012057\n",
      "Step 24: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.006604\n",
      "Step 24: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010026\n",
      "Step 24: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.010607\n",
      "Step 24: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.010717\n",
      "Step 24: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.016417\n",
      "Step 24: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007258\n",
      "Step 24: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.009143\n",
      "Step 24: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.008643\n",
      "Step 24: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.013169\n",
      "Step 24: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.006300\n",
      "Step 24: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.009440\n",
      "Step 24: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.005751\n",
      "Step 24: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.008496\n",
      "Step 24: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.015030\n",
      "Step 24: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.005126\n",
      "Step 24: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.012179\n",
      "Step 24: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.007213\n",
      "Step 24: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.006237\n",
      "Step 24: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008881\n",
      "Step 24: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.010944\n",
      "Step 24: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.010372\n",
      "Step 24: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.011766\n",
      "Step 24: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.014300\n",
      "Step 24: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.004914\n",
      "Step 24: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.013756\n",
      "Step 24: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.009352\n",
      "Step 24: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009953\n",
      "Step 24: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.009277\n",
      "Step 24: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.010617\n",
      "Step 24: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.012919\n",
      "Step 24: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.008839\n",
      "Step 24: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.003543\n",
      "Step 24: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.006746\n",
      "Step 24: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.014394\n",
      "Step 24: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.005508\n",
      "Step 24: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.010696\n",
      "Step 24: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.006978\n",
      "Step 24: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009050\n",
      "Step 24: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.007819\n",
      "Step 24: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.008288\n",
      "Step 24: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.006349\n",
      "Step 24: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.010131\n",
      "Step 24: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.009482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.006208\n",
      "Step 24: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.011519\n",
      "Step 24: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.005899\n",
      "Step 24: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.007651\n",
      "Step 24: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.011280\n",
      "Step 24: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009484\n",
      "Step 24: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.008965\n",
      "Step 24: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.013945\n",
      "Step 24: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.009069\n",
      "Step 24: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.007192\n",
      "Step 24: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007561\n",
      "Step 24: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.009948\n",
      "Step 24: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.007723\n",
      "Step 24: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010022\n",
      "Step 24: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.005151\n",
      "Step 24: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.009911\n",
      "Step 24: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.014766\n",
      "Step 24: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.009645\n",
      "Step 24: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.014900\n",
      "Step 24: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.010270\n",
      "Step 24: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.013776\n",
      "Step 24: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.004635\n",
      "Step 24: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.004223\n",
      "Step 24: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.012567\n",
      "Step 24: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.010445\n",
      "Step 24: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.012657\n",
      "Step 24: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008790\n",
      "Step 24: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.013315\n",
      "Step 24: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.007961\n",
      "Step 24: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.007581\n",
      "Step 24: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.005690\n",
      "Step 24: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.010529\n",
      "Step 24: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.005616\n",
      "Step 24: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.012685\n",
      "Step 24: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010413\n",
      "Step 24: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.010804\n",
      "Step 24: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.013525\n",
      "Step 24: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.012561\n",
      "Step 24: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.012155\n",
      "Step 24: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.010791\n",
      "Step 24: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006147\n",
      "Step 24: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.016071\n",
      "Step 24: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.008935\n",
      "Step 24: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011574\n",
      "Step 24: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.008108\n",
      "Step 24: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.010606\n",
      "Step 24: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008341\n",
      "Step 24: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.006103\n",
      "Step 24: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.014519\n",
      "Step 24: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.004644\n",
      "Step 24: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.011591\n",
      "Step 24: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011432\n",
      "Step 24: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.000099\n",
      "Step 24: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.013626\n",
      "Step 24: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.007856\n",
      "Step 24: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.011573\n",
      "Step 24: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.013626\n",
      "Step 24: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.013484\n",
      "Step 24: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.004147\n",
      "Step 24: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009827\n",
      "Step 24: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010336\n",
      "Step 24: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.005545\n",
      "Step 24: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009107\n",
      "time =  0:07:00.307025\n",
      "Step 25: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.003319\n",
      "Step 25: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.007029\n",
      "Step 25: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.016653\n",
      "Step 25: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009707\n",
      "Step 25: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.003541\n",
      "Step 25: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.012314\n",
      "Step 25: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.004577\n",
      "Step 25: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.012708\n",
      "Step 25: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.014715\n",
      "Step 25: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.016476\n",
      "Step 25: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.005440\n",
      "Step 25: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.007063\n",
      "Step 25: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009058\n",
      "Step 25: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.005447\n",
      "Step 25: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.005472\n",
      "Step 25: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.007030\n",
      "Step 25: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012081\n",
      "Step 25: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.006871\n",
      "Step 25: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.018507\n",
      "Step 25: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.010499\n",
      "Step 25: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.009072\n",
      "Step 25: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010818\n",
      "Step 25: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.015730\n",
      "Step 25: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009692\n",
      "Step 25: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.011946\n",
      "Step 25: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.010178\n",
      "Step 25: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.009271\n",
      "Step 25: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.011879\n",
      "Step 25: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.007571\n",
      "Step 25: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010482\n",
      "Step 25: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.005388\n",
      "Step 25: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.016151\n",
      "Step 25: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.006580\n",
      "Step 25: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.009318\n",
      "Step 25: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.016303\n",
      "Step 25: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.007438\n",
      "Step 25: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.013304\n",
      "Step 25: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.012130\n",
      "Step 25: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.004198\n",
      "Step 25: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.003524\n",
      "Step 25: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009073\n",
      "Step 25: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.013239\n",
      "Step 25: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010408\n",
      "Step 25: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.012182\n",
      "Step 25: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.013553\n",
      "Step 25: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010737\n",
      "Step 25: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.010440\n",
      "Step 25: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.012814\n",
      "Step 25: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.011267\n",
      "Step 25: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.015378\n",
      "Step 25: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.010762\n",
      "Step 25: Minibatch_NU: 5100/14680 Minibatch Loss: nan  0.998979\n",
      "Step 25: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.013358\n",
      "Step 25: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.009232\n",
      "Step 25: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.014624\n",
      "Step 25: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006535\n",
      "Step 25: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.010838\n",
      "Step 25: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.017376\n",
      "Step 25: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.013365\n",
      "Step 25: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.010790\n",
      "Step 25: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007765\n",
      "Step 25: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008000\n",
      "Step 25: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.003135\n",
      "Step 25: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.007455\n",
      "Step 25: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.008917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010946\n",
      "Step 25: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.005643\n",
      "Step 25: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009602\n",
      "Step 25: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.009811\n",
      "Step 25: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.015893\n",
      "Step 25: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.008174\n",
      "Step 25: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.006281\n",
      "Step 25: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009286\n",
      "Step 25: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.007402\n",
      "Step 25: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011132\n",
      "Step 25: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.013885\n",
      "Step 25: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009631\n",
      "Step 25: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008914\n",
      "Step 25: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009499\n",
      "Step 25: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.008781\n",
      "Step 25: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.006638\n",
      "Step 25: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.009054\n",
      "Step 25: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008860\n",
      "Step 25: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.009704\n",
      "Step 25: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.008332\n",
      "Step 25: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.006545\n",
      "Step 25: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010504\n",
      "Step 25: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.012573\n",
      "Step 25: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.009061\n",
      "Step 25: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.016654\n",
      "Step 25: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.010289\n",
      "Step 25: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.005187\n",
      "Step 25: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.011581\n",
      "Step 25: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.010254\n",
      "Step 25: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.008287\n",
      "Step 25: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.013429\n",
      "Step 25: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.004754\n",
      "Step 25: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010726\n",
      "Step 25: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.010770\n",
      "Step 25: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.009030\n",
      "Step 25: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008043\n",
      "Step 25: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006986\n",
      "Step 25: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.011629\n",
      "Step 25: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.007052\n",
      "Step 25: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.013495\n",
      "Step 25: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010356\n",
      "Step 25: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.008881\n",
      "Step 25: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.010720\n",
      "Step 25: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009809\n",
      "Step 25: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.007895\n",
      "Step 25: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.013592\n",
      "Step 25: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011463\n",
      "Step 25: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010848\n",
      "Step 25: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.010636\n",
      "Step 25: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009058\n",
      "Step 25: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009799\n",
      "Step 25: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.006027\n",
      "Step 25: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.007359\n",
      "Step 25: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.007332\n",
      "Step 25: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.007957\n",
      "Step 25: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.012500\n",
      "Step 25: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011992\n",
      "Step 25: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.009609\n",
      "Step 25: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.011502\n",
      "Step 25: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.011997\n",
      "Step 25: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012454\n",
      "Step 25: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.005673\n",
      "Step 25: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.008700\n",
      "Step 25: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.008012\n",
      "Step 25: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.006444\n",
      "Step 25: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.010396\n",
      "Step 25: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008744\n",
      "Step 25: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.002405\n",
      "Step 25: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012985\n",
      "Step 25: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.015064\n",
      "Step 25: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011284\n",
      "Step 25: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.004865\n",
      "Step 25: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.009545\n",
      "Step 25: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.005626\n",
      "Step 25: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.014682\n",
      "Step 25: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.007968\n",
      "Step 25: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012384\n",
      "Step 25: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.012433\n",
      "Step 25: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008230\n",
      "Step 25: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.011164\n",
      "Step 25: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009701\n",
      "Step 25: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010770\n",
      "time =  0:06:56.807913\n",
      "Step 26: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.009662\n",
      "Step 26: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009700\n",
      "Step 26: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.002809\n",
      "Step 26: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.016280\n",
      "Step 26: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.009771\n",
      "Step 26: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.011439\n",
      "Step 26: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.011554\n",
      "Step 26: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.016804\n",
      "Step 26: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.006930\n",
      "Step 26: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010377\n",
      "Step 26: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.004325\n",
      "Step 26: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.012645\n",
      "Step 26: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.008754\n",
      "Step 26: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.006980\n",
      "Step 26: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.009936\n",
      "Step 26: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.004807\n",
      "Step 26: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.006026\n",
      "Step 26: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.008600\n",
      "Step 26: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010753\n",
      "Step 26: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011174\n",
      "Step 26: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011080\n",
      "Step 26: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008516\n",
      "Step 26: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009318\n",
      "Step 26: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.005599\n",
      "Step 26: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.009558\n",
      "Step 26: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.011297\n",
      "Step 26: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.013123\n",
      "Step 26: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.005386\n",
      "Step 26: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.014299\n",
      "Step 26: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.015731\n",
      "Step 26: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.012272\n",
      "Step 26: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.011637\n",
      "Step 26: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.015386\n",
      "Step 26: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.007205\n",
      "Step 26: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.007424\n",
      "Step 26: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.011714\n",
      "Step 26: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.008218\n",
      "Step 26: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.013877\n",
      "Step 26: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.012393\n",
      "Step 26: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.006758\n",
      "Step 26: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009617\n",
      "Step 26: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.013564\n",
      "Step 26: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.012012\n",
      "Step 26: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.009716\n",
      "Step 26: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009152\n",
      "Step 26: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.008929\n",
      "Step 26: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.016949\n",
      "Step 26: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.016536\n",
      "Step 26: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.011085\n",
      "Step 26: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.012086\n",
      "Step 26: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007823\n",
      "Step 26: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.006186\n",
      "Step 26: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.009244\n",
      "Step 26: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.008349\n",
      "Step 26: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.008664\n",
      "Step 26: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008420\n",
      "Step 26: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011852\n",
      "Step 26: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.014447\n",
      "Step 26: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.007053\n",
      "Step 26: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.012269\n",
      "Step 26: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008698\n",
      "Step 26: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010491\n",
      "Step 26: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009278\n",
      "Step 26: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.008842\n",
      "Step 26: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007263\n",
      "Step 26: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.007053\n",
      "Step 26: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.016681\n",
      "Step 26: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.015272\n",
      "Step 26: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.014847\n",
      "Step 26: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.009338\n",
      "Step 26: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.003531\n",
      "Step 26: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.006389\n",
      "Step 26: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.008887\n",
      "Step 26: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.008784\n",
      "Step 26: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.003622\n",
      "Step 26: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.008895\n",
      "Step 26: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.010702\n",
      "Step 26: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009351\n",
      "Step 26: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.004187\n",
      "Step 26: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.010345\n",
      "Step 26: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.003053\n",
      "Step 26: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.012715\n",
      "Step 26: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.007846\n",
      "Step 26: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.013533\n",
      "Step 26: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.006772\n",
      "Step 26: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.014315\n",
      "Step 26: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011715\n",
      "Step 26: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010352\n",
      "Step 26: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.006177\n",
      "Step 26: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.003033\n",
      "Step 26: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.005054\n",
      "Step 26: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.010034\n",
      "Step 26: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.010837\n",
      "Step 26: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007127\n",
      "Step 26: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.006763\n",
      "Step 26: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012308\n",
      "Step 26: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008594\n",
      "Step 26: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.004399\n",
      "Step 26: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.009692\n",
      "Step 26: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.007607\n",
      "Step 26: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.010238\n",
      "Step 26: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.004404\n",
      "Step 26: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011714\n",
      "Step 26: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.006783\n",
      "Step 26: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.011237\n",
      "Step 26: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.011180\n",
      "Step 26: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.004310\n",
      "Step 26: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.005489\n",
      "Step 26: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.012696\n",
      "Step 26: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.019707\n",
      "Step 26: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.004056\n",
      "Step 26: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.006697\n",
      "Step 26: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.006084\n",
      "Step 26: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.008306\n",
      "Step 26: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.001455\n",
      "Step 26: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010860\n",
      "Step 26: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009354\n",
      "Step 26: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.004389\n",
      "Step 26: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.012223\n",
      "Step 26: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.005009\n",
      "Step 26: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.003611\n",
      "Step 26: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.009089\n",
      "Step 26: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.014740\n",
      "Step 26: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.005672\n",
      "Step 26: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.011953\n",
      "Step 26: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.008390\n",
      "Step 26: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.007138\n",
      "Step 26: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.010290\n",
      "Step 26: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.011826\n",
      "Step 26: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.010847\n",
      "Step 26: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010126\n",
      "Step 26: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.008257\n",
      "Step 26: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012851\n",
      "Step 26: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.009915\n",
      "Step 26: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010078\n",
      "Step 26: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.013604\n",
      "Step 26: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.008738\n",
      "Step 26: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.016655\n",
      "Step 26: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.018374\n",
      "Step 26: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.013834\n",
      "Step 26: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.009228\n",
      "Step 26: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.012815\n",
      "Step 26: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013441\n",
      "Step 26: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010749\n",
      "Step 26: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009528\n",
      "Step 26: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008326\n",
      "time =  0:07:01.513698\n",
      "Step 27: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.013203\n",
      "Step 27: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.007168\n",
      "Step 27: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.007300\n",
      "Step 27: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.013082\n",
      "Step 27: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011307\n",
      "Step 27: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009067\n",
      "Step 27: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.012980\n",
      "Step 27: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009764\n",
      "Step 27: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.012024\n",
      "Step 27: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011949\n",
      "Step 27: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.005064\n",
      "Step 27: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.006603\n",
      "Step 27: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.012875\n",
      "Step 27: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.011726\n",
      "Step 27: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013563\n",
      "Step 27: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.005391\n",
      "Step 27: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.011359\n",
      "Step 27: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.007234\n",
      "Step 27: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.011164\n",
      "Step 27: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.007365\n",
      "Step 27: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010174\n",
      "Step 27: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.013325\n",
      "Step 27: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.007111\n",
      "Step 27: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.011151\n",
      "Step 27: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.010813\n",
      "Step 27: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.006027\n",
      "Step 27: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.010475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.008536\n",
      "Step 27: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.011322\n",
      "Step 27: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.015073\n",
      "Step 27: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011656\n",
      "Step 27: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.008472\n",
      "Step 27: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.010319\n",
      "Step 27: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008464\n",
      "Step 27: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.012455\n",
      "Step 27: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.006485\n",
      "Step 27: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.009175\n",
      "Step 27: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010522\n",
      "Step 27: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009157\n",
      "Step 27: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.017521\n",
      "Step 27: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.016966\n",
      "Step 27: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.005948\n",
      "Step 27: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.008597\n",
      "Step 27: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.006438\n",
      "Step 27: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012369\n",
      "Step 27: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.012208\n",
      "Step 27: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.011766\n",
      "Step 27: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.010286\n",
      "Step 27: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.007077\n",
      "Step 27: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.014003\n",
      "Step 27: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.003002\n",
      "Step 27: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.008230\n",
      "Step 27: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.004312\n",
      "Step 27: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.020030\n",
      "Step 27: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.006937\n",
      "Step 27: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.003644\n",
      "Step 27: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.008227\n",
      "Step 27: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.015189\n",
      "Step 27: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.013162\n",
      "Step 27: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.011605\n",
      "Step 27: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.012448\n",
      "Step 27: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.000544\n",
      "Step 27: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.016160\n",
      "Step 27: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011238\n",
      "Step 27: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.012868\n",
      "Step 27: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.003634\n",
      "Step 27: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.006124\n",
      "Step 27: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.007479\n",
      "Step 27: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.011478\n",
      "Step 27: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.007025\n",
      "Step 27: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.012904\n",
      "Step 27: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.012150\n",
      "Step 27: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.015327\n",
      "Step 27: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.012296\n",
      "Step 27: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.015954\n",
      "Step 27: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.009870\n",
      "Step 27: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.016324\n",
      "Step 27: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009750\n",
      "Step 27: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008035\n",
      "Step 27: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012478\n",
      "Step 27: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007680\n",
      "Step 27: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.003166\n",
      "Step 27: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011157\n",
      "Step 27: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013317\n",
      "Step 27: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.008282\n",
      "Step 27: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.010651\n",
      "Step 27: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.011965\n",
      "Step 27: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.008854\n",
      "Step 27: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010824\n",
      "Step 27: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.012091\n",
      "Step 27: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.015606\n",
      "Step 27: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.015411\n",
      "Step 27: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.011276\n",
      "Step 27: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.010915\n",
      "Step 27: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007593\n",
      "Step 27: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.012414\n",
      "Step 27: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.005638\n",
      "Step 27: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.011246\n",
      "Step 27: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.008413\n",
      "Step 27: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.007815\n",
      "Step 27: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.013791\n",
      "Step 27: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.008220\n",
      "Step 27: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.007862\n",
      "Step 27: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011992\n",
      "Step 27: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.011623\n",
      "Step 27: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009203\n",
      "Step 27: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.008813\n",
      "Step 27: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.006478\n",
      "Step 27: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.002850\n",
      "Step 27: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.010797\n",
      "Step 27: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010964\n",
      "Step 27: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.002623\n",
      "Step 27: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010924\n",
      "Step 27: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.013766\n",
      "Step 27: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010661\n",
      "Step 27: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011237\n",
      "Step 27: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.013129\n",
      "Step 27: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.011053\n",
      "Step 27: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.009989\n",
      "Step 27: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.006855\n",
      "Step 27: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.010259\n",
      "Step 27: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.005519\n",
      "Step 27: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011795\n",
      "Step 27: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.013953\n",
      "Step 27: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.010055\n",
      "Step 27: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012102\n",
      "Step 27: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.013530\n",
      "Step 27: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.010214\n",
      "Step 27: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.010139\n",
      "Step 27: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.012442\n",
      "Step 27: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.005388\n",
      "Step 27: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008623\n",
      "Step 27: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.006813\n",
      "Step 27: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.013140\n",
      "Step 27: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.016503\n",
      "Step 27: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.006955\n",
      "Step 27: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011112\n",
      "Step 27: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.005318\n",
      "Step 27: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.007673\n",
      "Step 27: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.013676\n",
      "Step 27: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.015177\n",
      "Step 27: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.010372\n",
      "Step 27: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.016331\n",
      "Step 27: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.010724\n",
      "Step 27: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.009026\n",
      "Step 27: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.007939\n",
      "Step 27: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008172\n",
      "time =  0:06:56.744201\n",
      "Step 28: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.004083\n",
      "Step 28: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.016707\n",
      "Step 28: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.009536\n",
      "Step 28: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008138\n",
      "Step 28: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011962\n",
      "Step 28: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.010693\n",
      "Step 28: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.000989\n",
      "Step 28: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.005435\n",
      "Step 28: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010383\n",
      "Step 28: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009356\n",
      "Step 28: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.011923\n",
      "Step 28: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009402\n",
      "Step 28: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.011274\n",
      "Step 28: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013268\n",
      "Step 28: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012264\n",
      "Step 28: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.005477\n",
      "Step 28: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009248\n",
      "Step 28: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010308\n",
      "Step 28: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.009523\n",
      "Step 28: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.007791\n",
      "Step 28: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.007847\n",
      "Step 28: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.012351\n",
      "Step 28: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008342\n",
      "Step 28: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.010944\n",
      "Step 28: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.013069\n",
      "Step 28: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.001145\n",
      "Step 28: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.005762\n",
      "Step 28: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.011326\n",
      "Step 28: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.005480\n",
      "Step 28: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.006009\n",
      "Step 28: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.011395\n",
      "Step 28: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.017521\n",
      "Step 28: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.010579\n",
      "Step 28: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.007500\n",
      "Step 28: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.011457\n",
      "Step 28: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006264\n",
      "Step 28: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.013574\n",
      "Step 28: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009341\n",
      "Step 28: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009131\n",
      "Step 28: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.008858\n",
      "Step 28: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.012473\n",
      "Step 28: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.022459\n",
      "Step 28: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.013120\n",
      "Step 28: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008726\n",
      "Step 28: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.002949\n",
      "Step 28: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.013781\n",
      "Step 28: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006215\n",
      "Step 28: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.004589\n",
      "Step 28: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008722\n",
      "Step 28: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.011526\n",
      "Step 28: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.013538\n",
      "Step 28: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.004175\n",
      "Step 28: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.006741\n",
      "Step 28: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.011052\n",
      "Step 28: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006254\n",
      "Step 28: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.003049\n",
      "Step 28: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010294\n",
      "Step 28: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008141\n",
      "Step 28: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.015430\n",
      "Step 28: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007002\n",
      "Step 28: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006526\n",
      "Step 28: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010716\n",
      "Step 28: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.014650\n",
      "Step 28: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.002177\n",
      "Step 28: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010743\n",
      "Step 28: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.008176\n",
      "Step 28: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.005850\n",
      "Step 28: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.017246\n",
      "Step 28: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008341\n",
      "Step 28: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.013047\n",
      "Step 28: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.006450\n",
      "Step 28: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.012916\n",
      "Step 28: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.008322\n",
      "Step 28: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.013738\n",
      "Step 28: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.013167\n",
      "Step 28: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.004417\n",
      "Step 28: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.001147\n",
      "Step 28: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.010628\n",
      "Step 28: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.007493\n",
      "Step 28: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007561\n",
      "Step 28: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.005953\n",
      "Step 28: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.007727\n",
      "Step 28: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.007428\n",
      "Step 28: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.011204\n",
      "Step 28: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.012675\n",
      "Step 28: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.007545\n",
      "Step 28: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011906\n",
      "Step 28: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.016610\n",
      "Step 28: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.014623\n",
      "Step 28: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.000094\n",
      "Step 28: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.014487\n",
      "Step 28: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.010049\n",
      "Step 28: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.012474\n",
      "Step 28: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.015003\n",
      "Step 28: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.008010\n",
      "Step 28: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.005887\n",
      "Step 28: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.009752\n",
      "Step 28: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.005402\n",
      "Step 28: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.011334\n",
      "Step 28: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010210\n",
      "Step 28: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.007249\n",
      "Step 28: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.007350\n",
      "Step 28: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.003108\n",
      "Step 28: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.012579\n",
      "Step 28: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.017352\n",
      "Step 28: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.012290\n",
      "Step 28: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015649\n",
      "Step 28: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009554\n",
      "Step 28: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.008061\n",
      "Step 28: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.013592\n",
      "Step 28: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.014307\n",
      "Step 28: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.005523\n",
      "Step 28: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.003610\n",
      "Step 28: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.013855\n",
      "Step 28: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.008695\n",
      "Step 28: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.006814\n",
      "Step 28: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.011982\n",
      "Step 28: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010928\n",
      "Step 28: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.010837\n",
      "Step 28: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.006929\n",
      "Step 28: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011306\n",
      "Step 28: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011229\n",
      "Step 28: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.006937\n",
      "Step 28: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.001627\n",
      "Step 28: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.015536\n",
      "Step 28: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.013908\n",
      "Step 28: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.014069\n",
      "Step 28: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009081\n",
      "Step 28: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.004792\n",
      "Step 28: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.013176\n",
      "Step 28: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.009764\n",
      "Step 28: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.005222\n",
      "Step 28: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.008011\n",
      "Step 28: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.008177\n",
      "Step 28: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.003925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.003773\n",
      "Step 28: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.012510\n",
      "Step 28: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010578\n",
      "Step 28: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.012146\n",
      "Step 28: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008507\n",
      "Step 28: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.006709\n",
      "Step 28: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008869\n",
      "Step 28: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.007241\n",
      "Step 28: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008577\n",
      "Step 28: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.008575\n",
      "Step 28: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.011064\n",
      "time =  0:07:01.926466\n",
      "Step 29: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.011589\n",
      "Step 29: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.004712\n",
      "Step 29: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.005919\n",
      "Step 29: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.005862\n",
      "Step 29: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.006322\n",
      "Step 29: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.002909\n",
      "Step 29: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.005265\n",
      "Step 29: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.007934\n",
      "Step 29: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.007991\n",
      "Step 29: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.016081\n",
      "Step 29: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.013455\n",
      "Step 29: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.016618\n",
      "Step 29: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.007931\n",
      "Step 29: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.013454\n",
      "Step 29: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.010201\n",
      "Step 29: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.012248\n",
      "Step 29: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012592\n",
      "Step 29: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.011322\n",
      "Step 29: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.008283\n",
      "Step 29: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.006648\n",
      "Step 29: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.006989\n",
      "Step 29: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.013598\n",
      "Step 29: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009061\n",
      "Step 29: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.002684\n",
      "Step 29: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.006102\n",
      "Step 29: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.006395\n",
      "Step 29: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.006494\n",
      "Step 29: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.008112\n",
      "Step 29: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.014572\n",
      "Step 29: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011953\n",
      "Step 29: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009396\n",
      "Step 29: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.014166\n",
      "Step 29: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.008795\n",
      "Step 29: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.006874\n",
      "Step 29: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.020590\n",
      "Step 29: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.014414\n",
      "Step 29: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.010061\n",
      "Step 29: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.019881\n",
      "Step 29: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.014068\n",
      "Step 29: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.006054\n",
      "Step 29: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.007336\n",
      "Step 29: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009489\n",
      "Step 29: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.015372\n",
      "Step 29: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.002987\n",
      "Step 29: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012650\n",
      "Step 29: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010006\n",
      "Step 29: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.002356\n",
      "Step 29: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.011823\n",
      "Step 29: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.010261\n",
      "Step 29: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008868\n",
      "Step 29: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.009255\n",
      "Step 29: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007262\n",
      "Step 29: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.008435\n",
      "Step 29: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.011622\n",
      "Step 29: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.002784\n",
      "Step 29: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.012288\n",
      "Step 29: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.009046\n",
      "Step 29: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011098\n",
      "Step 29: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.005183\n",
      "Step 29: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008749\n",
      "Step 29: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.005250\n",
      "Step 29: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.011840\n",
      "Step 29: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010018\n",
      "Step 29: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008111\n",
      "Step 29: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.013564\n",
      "Step 29: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.011757\n",
      "Step 29: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.009338\n",
      "Step 29: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.004568\n",
      "Step 29: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.013898\n",
      "Step 29: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.002603\n",
      "Step 29: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.010429\n",
      "Step 29: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009042\n",
      "Step 29: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.006091\n",
      "Step 29: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009080\n",
      "Step 29: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.008600\n",
      "Step 29: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.008797\n",
      "Step 29: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009903\n",
      "Step 29: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.013061\n",
      "Step 29: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.011664\n",
      "Step 29: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.006243\n",
      "Step 29: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011513\n",
      "Step 29: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.013408\n",
      "Step 29: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011367\n",
      "Step 29: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.014808\n",
      "Step 29: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009390\n",
      "Step 29: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.009633\n",
      "Step 29: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.009895\n",
      "Step 29: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.005071\n",
      "Step 29: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.011203\n",
      "Step 29: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.015011\n",
      "Step 29: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.007844\n",
      "Step 29: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.010003\n",
      "Step 29: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.007806\n",
      "Step 29: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.018881\n",
      "Step 29: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.012136\n",
      "Step 29: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.008612\n",
      "Step 29: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.015004\n",
      "Step 29: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008231\n",
      "Step 29: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.010077\n",
      "Step 29: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.006346\n",
      "Step 29: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009490\n",
      "Step 29: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.014994\n",
      "Step 29: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.016007\n",
      "Step 29: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.008540\n",
      "Step 29: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.007248\n",
      "Step 29: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010076\n",
      "Step 29: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.010569\n",
      "Step 29: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.002043\n",
      "Step 29: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.008877\n",
      "Step 29: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.010814\n",
      "Step 29: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008111\n",
      "Step 29: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.006566\n",
      "Step 29: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011040\n",
      "Step 29: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.005478\n",
      "Step 29: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.006434\n",
      "Step 29: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.017801\n",
      "Step 29: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.003631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.011460\n",
      "Step 29: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.014271\n",
      "Step 29: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.014289\n",
      "Step 29: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.010790\n",
      "Step 29: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.010470\n",
      "Step 29: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.007000\n",
      "Step 29: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.010761\n",
      "Step 29: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009112\n",
      "Step 29: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.007490\n",
      "Step 29: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012557\n",
      "Step 29: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.006056\n",
      "Step 29: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007636\n",
      "Step 29: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.010938\n",
      "Step 29: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.012776\n",
      "Step 29: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008064\n",
      "Step 29: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009306\n",
      "Step 29: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.008566\n",
      "Step 29: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010445\n",
      "Step 29: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009968\n",
      "Step 29: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008535\n",
      "Step 29: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.008330\n",
      "Step 29: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.002561\n",
      "Step 29: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.011085\n",
      "Step 29: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.013591\n",
      "Step 29: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012113\n",
      "Step 29: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008332\n",
      "Step 29: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.012771\n",
      "Step 29: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.011644\n",
      "Step 29: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011532\n",
      "Step 29: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.016404\n",
      "time =  0:06:56.292991\n",
      "Step 30: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.004257\n",
      "Step 30: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012057\n",
      "Step 30: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.008852\n",
      "Step 30: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.011951\n",
      "Step 30: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.009345\n",
      "Step 30: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.014638\n",
      "Step 30: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.006235\n",
      "Step 30: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008353\n",
      "Step 30: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.011147\n",
      "Step 30: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013086\n",
      "Step 30: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009475\n",
      "Step 30: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.009565\n",
      "Step 30: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.007721\n",
      "Step 30: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.008990\n",
      "Step 30: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.011788\n",
      "Step 30: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.013972\n",
      "Step 30: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.009983\n",
      "Step 30: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.003257\n",
      "Step 30: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.008450\n",
      "Step 30: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.006429\n",
      "Step 30: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.004785\n",
      "Step 30: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009968\n",
      "Step 30: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.017252\n",
      "Step 30: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008643\n",
      "Step 30: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.010877\n",
      "Step 30: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.011020\n",
      "Step 30: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008420\n",
      "Step 30: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.012533\n",
      "Step 30: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.018721\n",
      "Step 30: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.009251\n",
      "Step 30: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011326\n",
      "Step 30: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.009702\n",
      "Step 30: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.007371\n",
      "Step 30: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.006722\n",
      "Step 30: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.011359\n",
      "Step 30: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.006130\n",
      "Step 30: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.013186\n",
      "Step 30: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.004176\n",
      "Step 30: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009207\n",
      "Step 30: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.013321\n",
      "Step 30: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.016613\n",
      "Step 30: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.007865\n",
      "Step 30: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.016405\n",
      "Step 30: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.012447\n",
      "Step 30: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012195\n",
      "Step 30: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.007130\n",
      "Step 30: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009974\n",
      "Step 30: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.005982\n",
      "Step 30: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009309\n",
      "Step 30: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.015515\n",
      "Step 30: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.013106\n",
      "Step 30: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.009240\n",
      "Step 30: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.017883\n",
      "Step 30: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.008049\n",
      "Step 30: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.012082\n",
      "Step 30: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013331\n",
      "Step 30: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.012254\n",
      "Step 30: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.009808\n",
      "Step 30: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.014960\n",
      "Step 30: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.014350\n",
      "Step 30: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.010851\n",
      "Step 30: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.003854\n",
      "Step 30: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012536\n",
      "Step 30: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.015270\n",
      "Step 30: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.011812\n",
      "Step 30: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.017970\n",
      "Step 30: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.008434\n",
      "Step 30: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008162\n",
      "Step 30: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010145\n",
      "Step 30: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008771\n",
      "Step 30: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.011515\n",
      "Step 30: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010799\n",
      "Step 30: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.010221\n",
      "Step 30: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009331\n",
      "Step 30: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.013305\n",
      "Step 30: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010023\n",
      "Step 30: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.014582\n",
      "Step 30: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.006983\n",
      "Step 30: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008363\n",
      "Step 30: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.018067\n",
      "Step 30: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009184\n",
      "Step 30: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.003904\n",
      "Step 30: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.010138\n",
      "Step 30: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.005777\n",
      "Step 30: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.006844\n",
      "Step 30: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.005063\n",
      "Step 30: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010330\n",
      "Step 30: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.008972\n",
      "Step 30: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.014233\n",
      "Step 30: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.010326\n",
      "Step 30: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.014912\n",
      "Step 30: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.009414\n",
      "Step 30: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.006423\n",
      "Step 30: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.011230\n",
      "Step 30: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.012665\n",
      "Step 30: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.003046\n",
      "Step 30: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012387\n",
      "Step 30: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.004300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.007121\n",
      "Step 30: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.017858\n",
      "Step 30: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.011438\n",
      "Step 30: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.012123\n",
      "Step 30: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.009996\n",
      "Step 30: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011297\n",
      "Step 30: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.014390\n",
      "Step 30: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.011107\n",
      "Step 30: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007668\n",
      "Step 30: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.005110\n",
      "Step 30: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.012671\n",
      "Step 30: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011935\n",
      "Step 30: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008298\n",
      "Step 30: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.007699\n",
      "Step 30: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.004751\n",
      "Step 30: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.012308\n",
      "Step 30: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010563\n",
      "Step 30: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009057\n",
      "Step 30: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.014773\n",
      "Step 30: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.004738\n",
      "Step 30: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.012138\n",
      "Step 30: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.007858\n",
      "Step 30: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.011462\n",
      "Step 30: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011454\n",
      "Step 30: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.010741\n",
      "Step 30: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.006130\n",
      "Step 30: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.003964\n",
      "Step 30: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.006699\n",
      "Step 30: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.011117\n",
      "Step 30: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009464\n",
      "Step 30: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.014693\n",
      "Step 30: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.011734\n",
      "Step 30: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.014279\n",
      "Step 30: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.006747\n",
      "Step 30: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.003866\n",
      "Step 30: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.010495\n",
      "Step 30: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010783\n",
      "Step 30: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.007099\n",
      "Step 30: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.013800\n",
      "Step 30: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.005316\n",
      "Step 30: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.012391\n",
      "Step 30: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.011114\n",
      "Step 30: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008121\n",
      "Step 30: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.009371\n",
      "Step 30: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.021621\n",
      "Step 30: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009735\n",
      "Step 30: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.005607\n",
      "Step 30: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.007716\n",
      "Step 30: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010164\n",
      "time =  0:07:01.701632\n",
      "Step 31: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.010942\n",
      "Step 31: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.007697\n",
      "Step 31: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.013188\n",
      "Step 31: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.013445\n",
      "Step 31: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.012337\n",
      "Step 31: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.012491\n",
      "Step 31: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.004926\n",
      "Step 31: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008660\n",
      "Step 31: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.008610\n",
      "Step 31: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.009289\n",
      "Step 31: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009610\n",
      "Step 31: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.010170\n",
      "Step 31: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.006807\n",
      "Step 31: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.017804\n",
      "Step 31: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013227\n",
      "Step 31: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009530\n",
      "Step 31: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.008416\n",
      "Step 31: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009400\n",
      "Step 31: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010635\n",
      "Step 31: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.013231\n",
      "Step 31: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.003192\n",
      "Step 31: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.014859\n",
      "Step 31: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.008698\n",
      "Step 31: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009483\n",
      "Step 31: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.005338\n",
      "Step 31: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009052\n",
      "Step 31: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.011465\n",
      "Step 31: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.017380\n",
      "Step 31: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.016276\n",
      "Step 31: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.017330\n",
      "Step 31: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.007558\n",
      "Step 31: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.010195\n",
      "Step 31: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011869\n",
      "Step 31: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008574\n",
      "Step 31: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.017471\n",
      "Step 31: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.015101\n",
      "Step 31: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.013153\n",
      "Step 31: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010078\n",
      "Step 31: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010454\n",
      "Step 31: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.008440\n",
      "Step 31: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.010283\n",
      "Step 31: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.014413\n",
      "Step 31: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.006780\n",
      "Step 31: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.009439\n",
      "Step 31: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.011558\n",
      "Step 31: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.012139\n",
      "Step 31: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.006483\n",
      "Step 31: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.011138\n",
      "Step 31: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.015574\n",
      "Step 31: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.007508\n",
      "Step 31: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.015401\n",
      "Step 31: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007354\n",
      "Step 31: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.004322\n",
      "Step 31: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.012133\n",
      "Step 31: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.013564\n",
      "Step 31: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.017671\n",
      "Step 31: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.011759\n",
      "Step 31: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.012762\n",
      "Step 31: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010083\n",
      "Step 31: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.008267\n",
      "Step 31: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.005865\n",
      "Step 31: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.014315\n",
      "Step 31: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.009485\n",
      "Step 31: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009277\n",
      "Step 31: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.008442\n",
      "Step 31: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.012484\n",
      "Step 31: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.005924\n",
      "Step 31: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.006977\n",
      "Step 31: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010465\n",
      "Step 31: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.005472\n",
      "Step 31: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.009723\n",
      "Step 31: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.011875\n",
      "Step 31: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.012837\n",
      "Step 31: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.008263\n",
      "Step 31: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.010313\n",
      "Step 31: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.007753\n",
      "Step 31: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.011754\n",
      "Step 31: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.011521\n",
      "Step 31: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.011487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.007593\n",
      "Step 31: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.012995\n",
      "Step 31: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008270\n",
      "Step 31: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.002589\n",
      "Step 31: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013420\n",
      "Step 31: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009451\n",
      "Step 31: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.018080\n",
      "Step 31: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.012949\n",
      "Step 31: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011272\n",
      "Step 31: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007238\n",
      "Step 31: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009192\n",
      "Step 31: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.012582\n",
      "Step 31: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.013160\n",
      "Step 31: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.010131\n",
      "Step 31: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.003818\n",
      "Step 31: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.014666\n",
      "Step 31: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010896\n",
      "Step 31: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.005365\n",
      "Step 31: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.017107\n",
      "Step 31: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.011060\n",
      "Step 31: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.011450\n",
      "Step 31: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.012308\n",
      "Step 31: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.010341\n",
      "Step 31: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.014592\n",
      "Step 31: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.004292\n",
      "Step 31: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.004253\n",
      "Step 31: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.012105\n",
      "Step 31: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.011054\n",
      "Step 31: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009620\n",
      "Step 31: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.011089\n",
      "Step 31: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011732\n",
      "Step 31: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010475\n",
      "Step 31: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008812\n",
      "Step 31: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.009228\n",
      "Step 31: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.013448\n",
      "Step 31: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011709\n",
      "Step 31: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.018852\n",
      "Step 31: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010379\n",
      "Step 31: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.006208\n",
      "Step 31: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.014014\n",
      "Step 31: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009316\n",
      "Step 31: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.009555\n",
      "Step 31: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.006111\n",
      "Step 31: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011643\n",
      "Step 31: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.007407\n",
      "Step 31: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.010867\n",
      "Step 31: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.008129\n",
      "Step 31: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.010567\n",
      "Step 31: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009291\n",
      "Step 31: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.008873\n",
      "Step 31: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008726\n",
      "Step 31: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.009262\n",
      "Step 31: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008532\n",
      "Step 31: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009894\n",
      "Step 31: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.008128\n",
      "Step 31: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010464\n",
      "Step 31: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009779\n",
      "Step 31: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011048\n",
      "Step 31: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.011145\n",
      "Step 31: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010697\n",
      "Step 31: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.008213\n",
      "Step 31: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.010731\n",
      "Step 31: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.008384\n",
      "Step 31: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.011039\n",
      "Step 31: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.015315\n",
      "Step 31: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008978\n",
      "Step 31: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.015761\n",
      "Step 31: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008235\n",
      "time =  0:06:55.671851\n",
      "Step 32: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.009635\n",
      "Step 32: Minibatch_NU: 100/14680 Minibatch Loss: nan  0.999972\n",
      "Step 32: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.011158\n",
      "Step 32: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.015518\n",
      "Step 32: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.012080\n",
      "Step 32: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009563\n",
      "Step 32: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.007749\n",
      "Step 32: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.012643\n",
      "Step 32: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.007458\n",
      "Step 32: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011188\n",
      "Step 32: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.010156\n",
      "Step 32: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.011286\n",
      "Step 32: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.006184\n",
      "Step 32: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.014373\n",
      "Step 32: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.009231\n",
      "Step 32: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.011785\n",
      "Step 32: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.009119\n",
      "Step 32: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.010513\n",
      "Step 32: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.002541\n",
      "Step 32: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.010791\n",
      "Step 32: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.014567\n",
      "Step 32: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.007936\n",
      "Step 32: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.011579\n",
      "Step 32: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008260\n",
      "Step 32: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.014472\n",
      "Step 32: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.010195\n",
      "Step 32: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.015826\n",
      "Step 32: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009210\n",
      "Step 32: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.008917\n",
      "Step 32: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010763\n",
      "Step 32: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.008729\n",
      "Step 32: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.007220\n",
      "Step 32: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.010667\n",
      "Step 32: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.018756\n",
      "Step 32: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.010948\n",
      "Step 32: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.006510\n",
      "Step 32: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.015744\n",
      "Step 32: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.005248\n",
      "Step 32: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.013054\n",
      "Step 32: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.005753\n",
      "Step 32: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.005536\n",
      "Step 32: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009128\n",
      "Step 32: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.013487\n",
      "Step 32: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.008795\n",
      "Step 32: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008830\n",
      "Step 32: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.016313\n",
      "Step 32: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.015665\n",
      "Step 32: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.015073\n",
      "Step 32: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.012353\n",
      "Step 32: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.011064\n",
      "Step 32: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.010671\n",
      "Step 32: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007624\n",
      "Step 32: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.016954\n",
      "Step 32: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.007447\n",
      "Step 32: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.017400\n",
      "Step 32: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.009117\n",
      "Step 32: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.007701\n",
      "Step 32: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.008732\n",
      "Step 32: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.017235\n",
      "Step 32: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.008520\n",
      "Step 32: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006954\n",
      "Step 32: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.004419\n",
      "Step 32: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.010152\n",
      "Step 32: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.001809\n",
      "Step 32: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010250\n",
      "Step 32: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011933\n",
      "Step 32: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.006733\n",
      "Step 32: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010630\n",
      "Step 32: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.006922\n",
      "Step 32: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.008580\n",
      "Step 32: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.006635\n",
      "Step 32: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.007109\n",
      "Step 32: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.005705\n",
      "Step 32: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.013804\n",
      "Step 32: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011454\n",
      "Step 32: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.007952\n",
      "Step 32: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.010985\n",
      "Step 32: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.011594\n",
      "Step 32: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.008923\n",
      "Step 32: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007335\n",
      "Step 32: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008854\n",
      "Step 32: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011572\n",
      "Step 32: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.007990\n",
      "Step 32: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.007015\n",
      "Step 32: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.012316\n",
      "Step 32: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.009492\n",
      "Step 32: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.003705\n",
      "Step 32: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.013394\n",
      "Step 32: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.011392\n",
      "Step 32: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.008773\n",
      "Step 32: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.007885\n",
      "Step 32: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.010275\n",
      "Step 32: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.015959\n",
      "Step 32: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.009912\n",
      "Step 32: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.007957\n",
      "Step 32: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.011968\n",
      "Step 32: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.005630\n",
      "Step 32: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.009108\n",
      "Step 32: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.009137\n",
      "Step 32: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008560\n",
      "Step 32: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.007038\n",
      "Step 32: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.003511\n",
      "Step 32: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.010029\n",
      "Step 32: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.016225\n",
      "Step 32: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008768\n",
      "Step 32: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.013584\n",
      "Step 32: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015441\n",
      "Step 32: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.011947\n",
      "Step 32: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.009730\n",
      "Step 32: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010866\n",
      "Step 32: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.009002\n",
      "Step 32: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.017192\n",
      "Step 32: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.001695\n",
      "Step 32: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009967\n",
      "Step 32: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.010287\n",
      "Step 32: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.013673\n",
      "Step 32: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.008587\n",
      "Step 32: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.009030\n",
      "Step 32: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.019458\n",
      "Step 32: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.007062\n",
      "Step 32: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011040\n",
      "Step 32: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.012028\n",
      "Step 32: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.009394\n",
      "Step 32: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009890\n",
      "Step 32: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.006631\n",
      "Step 32: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012526\n",
      "Step 32: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.008767\n",
      "Step 32: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011126\n",
      "Step 32: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.015296\n",
      "Step 32: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.012730\n",
      "Step 32: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010121\n",
      "Step 32: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.014488\n",
      "Step 32: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.010983\n",
      "Step 32: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010649\n",
      "Step 32: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.019047\n",
      "Step 32: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.013896\n",
      "Step 32: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.015737\n",
      "Step 32: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.013909\n",
      "Step 32: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.012303\n",
      "Step 32: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008561\n",
      "Step 32: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007341\n",
      "Step 32: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.010815\n",
      "Step 32: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.006242\n",
      "Step 32: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.003216\n",
      "Step 32: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011070\n",
      "Step 32: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010783\n",
      "time =  0:06:55.971163\n",
      "Step 33: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.004679\n",
      "Step 33: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.013962\n",
      "Step 33: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.013831\n",
      "Step 33: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008625\n",
      "Step 33: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011021\n",
      "Step 33: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.011922\n",
      "Step 33: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.006260\n",
      "Step 33: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009476\n",
      "Step 33: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.012521\n",
      "Step 33: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.005738\n",
      "Step 33: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.011183\n",
      "Step 33: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.015180\n",
      "Step 33: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009624\n",
      "Step 33: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.004181\n",
      "Step 33: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006184\n",
      "Step 33: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.013096\n",
      "Step 33: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.014608\n",
      "Step 33: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.006274\n",
      "Step 33: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010170\n",
      "Step 33: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.013171\n",
      "Step 33: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010485\n",
      "Step 33: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008701\n",
      "Step 33: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.014555\n",
      "Step 33: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008713\n",
      "Step 33: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.012508\n",
      "Step 33: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.011396\n",
      "Step 33: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.010585\n",
      "Step 33: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.012520\n",
      "Step 33: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009165\n",
      "Step 33: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.006737\n",
      "Step 33: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.008761\n",
      "Step 33: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.011382\n",
      "Step 33: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009639\n",
      "Step 33: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008891\n",
      "Step 33: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009864\n",
      "Step 33: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.012374\n",
      "Step 33: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.015857\n",
      "Step 33: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.009100\n",
      "Step 33: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.011653\n",
      "Step 33: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.012400\n",
      "Step 33: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.005404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009384\n",
      "Step 33: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.012381\n",
      "Step 33: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.011859\n",
      "Step 33: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.002706\n",
      "Step 33: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010363\n",
      "Step 33: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.018080\n",
      "Step 33: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.013011\n",
      "Step 33: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.001266\n",
      "Step 33: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.005114\n",
      "Step 33: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005758\n",
      "Step 33: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.011550\n",
      "Step 33: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.012165\n",
      "Step 33: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.004786\n",
      "Step 33: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.010509\n",
      "Step 33: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006349\n",
      "Step 33: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014963\n",
      "Step 33: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.015846\n",
      "Step 33: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.005321\n",
      "Step 33: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.013052\n",
      "Step 33: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.008639\n",
      "Step 33: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006333\n",
      "Step 33: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.009298\n",
      "Step 33: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008116\n",
      "Step 33: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.012278\n",
      "Step 33: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.014877\n",
      "Step 33: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010962\n",
      "Step 33: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.007984\n",
      "Step 33: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.009923\n",
      "Step 33: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.011297\n",
      "Step 33: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.007135\n",
      "Step 33: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.018287\n",
      "Step 33: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009898\n",
      "Step 33: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.010016\n",
      "Step 33: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.001251\n",
      "Step 33: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.003310\n",
      "Step 33: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.008074\n",
      "Step 33: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008131\n",
      "Step 33: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.010816\n",
      "Step 33: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.006736\n",
      "Step 33: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007902\n",
      "Step 33: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.015689\n",
      "Step 33: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.004998\n",
      "Step 33: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.015247\n",
      "Step 33: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.012430\n",
      "Step 33: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.003837\n",
      "Step 33: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.003987\n",
      "Step 33: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010046\n",
      "Step 33: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007840\n",
      "Step 33: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.007610\n",
      "Step 33: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.009671\n",
      "Step 33: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.012107\n",
      "Step 33: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.006786\n",
      "Step 33: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.012905\n",
      "Step 33: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007746\n",
      "Step 33: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.011093\n",
      "Step 33: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.008507\n",
      "Step 33: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.003581\n",
      "Step 33: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.014206\n",
      "Step 33: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.009483\n",
      "Step 33: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009268\n",
      "Step 33: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006408\n",
      "Step 33: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.005530\n",
      "Step 33: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.016228\n",
      "Step 33: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.006375\n",
      "Step 33: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009310\n",
      "Step 33: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.009723\n",
      "Step 33: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009805\n",
      "Step 33: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.014741\n",
      "Step 33: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.010192\n",
      "Step 33: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008592\n",
      "Step 33: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.006378\n",
      "Step 33: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010949\n",
      "Step 33: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.008170\n",
      "Step 33: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011522\n",
      "Step 33: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009082\n",
      "Step 33: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.004913\n",
      "Step 33: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.010352\n",
      "Step 33: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010743\n",
      "Step 33: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.011366\n",
      "Step 33: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.005890\n",
      "Step 33: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.005188\n",
      "Step 33: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.013075\n",
      "Step 33: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.009534\n",
      "Step 33: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009964\n",
      "Step 33: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.007166\n",
      "Step 33: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.008707\n",
      "Step 33: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.013684\n",
      "Step 33: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011220\n",
      "Step 33: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009678\n",
      "Step 33: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008994\n",
      "Step 33: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.011498\n",
      "Step 33: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.013090\n",
      "Step 33: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.010468\n",
      "Step 33: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010015\n",
      "Step 33: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.008335\n",
      "Step 33: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008559\n",
      "Step 33: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.009065\n",
      "Step 33: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.008755\n",
      "Step 33: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.009358\n",
      "Step 33: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.006255\n",
      "Step 33: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.016246\n",
      "Step 33: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.013931\n",
      "Step 33: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.014547\n",
      "Step 33: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008081\n",
      "Step 33: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.006092\n",
      "Step 33: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010938\n",
      "time =  0:06:54.183478\n",
      "Step 34: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.013597\n",
      "Step 34: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.005444\n",
      "Step 34: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006081\n",
      "Step 34: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008515\n",
      "Step 34: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011392\n",
      "Step 34: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.007194\n",
      "Step 34: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.007191\n",
      "Step 34: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008959\n",
      "Step 34: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009104\n",
      "Step 34: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.014739\n",
      "Step 34: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009572\n",
      "Step 34: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.007257\n",
      "Step 34: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.008854\n",
      "Step 34: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007203\n",
      "Step 34: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.010154\n",
      "Step 34: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.011662\n",
      "Step 34: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.007267\n",
      "Step 34: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.007494\n",
      "Step 34: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010761\n",
      "Step 34: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.004521\n",
      "Step 34: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.013196\n",
      "Step 34: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.009045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.014130\n",
      "Step 34: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009576\n",
      "Step 34: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008083\n",
      "Step 34: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.007790\n",
      "Step 34: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.015978\n",
      "Step 34: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007896\n",
      "Step 34: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.010460\n",
      "Step 34: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.007817\n",
      "Step 34: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011353\n",
      "Step 34: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.007490\n",
      "Step 34: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.003943\n",
      "Step 34: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.004165\n",
      "Step 34: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.011939\n",
      "Step 34: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.013195\n",
      "Step 34: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.007968\n",
      "Step 34: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.011434\n",
      "Step 34: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.011786\n",
      "Step 34: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.011384\n",
      "Step 34: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.006587\n",
      "Step 34: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.008724\n",
      "Step 34: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.012393\n",
      "Step 34: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.009512\n",
      "Step 34: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.013118\n",
      "Step 34: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.015873\n",
      "Step 34: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007654\n",
      "Step 34: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.007732\n",
      "Step 34: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.010240\n",
      "Step 34: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.006075\n",
      "Step 34: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.010586\n",
      "Step 34: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.008170\n",
      "Step 34: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.014735\n",
      "Step 34: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.007250\n",
      "Step 34: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.012790\n",
      "Step 34: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.008543\n",
      "Step 34: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.010163\n",
      "Step 34: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.005178\n",
      "Step 34: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.003815\n",
      "Step 34: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.002941\n",
      "Step 34: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.014417\n",
      "Step 34: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.005233\n",
      "Step 34: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.006911\n",
      "Step 34: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.007143\n",
      "Step 34: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.008586\n",
      "Step 34: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010059\n",
      "Step 34: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.007969\n",
      "Step 34: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.010418\n",
      "Step 34: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.007947\n",
      "Step 34: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008992\n",
      "Step 34: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.006394\n",
      "Step 34: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.002753\n",
      "Step 34: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.010434\n",
      "Step 34: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.010299\n",
      "Step 34: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.014405\n",
      "Step 34: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010618\n",
      "Step 34: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.011491\n",
      "Step 34: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.012526\n",
      "Step 34: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.007019\n",
      "Step 34: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009683\n",
      "Step 34: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.008284\n",
      "Step 34: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.007115\n",
      "Step 34: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.014368\n",
      "Step 34: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.005405\n",
      "Step 34: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005364\n",
      "Step 34: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.005325\n",
      "Step 34: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.009951\n",
      "Step 34: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.016357\n",
      "Step 34: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012655\n",
      "Step 34: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.015821\n",
      "Step 34: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.013171\n",
      "Step 34: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.014971\n",
      "Step 34: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.011266\n",
      "Step 34: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.010549\n",
      "Step 34: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.013825\n",
      "Step 34: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010765\n",
      "Step 34: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.019753\n",
      "Step 34: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008221\n",
      "Step 34: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.008249\n",
      "Step 34: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.015421\n",
      "Step 34: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.013179\n",
      "Step 34: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.014696\n",
      "Step 34: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.008064\n",
      "Step 34: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.012220\n",
      "Step 34: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.002074\n",
      "Step 34: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.013247\n",
      "Step 34: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.006364\n",
      "Step 34: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009013\n",
      "Step 34: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.011270\n",
      "Step 34: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.005509\n",
      "Step 34: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.007758\n",
      "Step 34: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011424\n",
      "Step 34: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010467\n",
      "Step 34: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.010989\n",
      "Step 34: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011782\n",
      "Step 34: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.012286\n",
      "Step 34: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.013489\n",
      "Step 34: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009319\n",
      "Step 34: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.009504\n",
      "Step 34: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.012908\n",
      "Step 34: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.013781\n",
      "Step 34: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.010357\n",
      "Step 34: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.005582\n",
      "Step 34: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.005426\n",
      "Step 34: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.011167\n",
      "Step 34: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.009250\n",
      "Step 34: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.013815\n",
      "Step 34: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011240\n",
      "Step 34: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.014181\n",
      "Step 34: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.015704\n",
      "Step 34: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.006520\n",
      "Step 34: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.005458\n",
      "Step 34: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009338\n",
      "Step 34: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.004689\n",
      "Step 34: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.006680\n",
      "Step 34: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.004884\n",
      "Step 34: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.007306\n",
      "Step 34: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.004764\n",
      "Step 34: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.009238\n",
      "Step 34: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.008041\n",
      "Step 34: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.009077\n",
      "Step 34: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007155\n",
      "Step 34: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.014296\n",
      "Step 34: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.015149\n",
      "Step 34: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.006887\n",
      "Step 34: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.008175\n",
      "Step 34: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.007896\n",
      "time =  0:06:55.307046\n",
      "Step 35: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.011530\n",
      "Step 35: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.008678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006777\n",
      "Step 35: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.006834\n",
      "Step 35: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011622\n",
      "Step 35: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009364\n",
      "Step 35: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.011622\n",
      "Step 35: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.012080\n",
      "Step 35: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.011460\n",
      "Step 35: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010490\n",
      "Step 35: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.006425\n",
      "Step 35: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.010001\n",
      "Step 35: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.010978\n",
      "Step 35: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.014200\n",
      "Step 35: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006398\n",
      "Step 35: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.003061\n",
      "Step 35: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.011721\n",
      "Step 35: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.006685\n",
      "Step 35: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.011902\n",
      "Step 35: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.008227\n",
      "Step 35: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011787\n",
      "Step 35: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008918\n",
      "Step 35: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.008660\n",
      "Step 35: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.013497\n",
      "Step 35: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.006086\n",
      "Step 35: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.007827\n",
      "Step 35: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.015550\n",
      "Step 35: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.012475\n",
      "Step 35: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.004536\n",
      "Step 35: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011626\n",
      "Step 35: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009942\n",
      "Step 35: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.006004\n",
      "Step 35: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.007615\n",
      "Step 35: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008606\n",
      "Step 35: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.013801\n",
      "Step 35: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.008806\n",
      "Step 35: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.010960\n",
      "Step 35: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.013005\n",
      "Step 35: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.006426\n",
      "Step 35: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.006546\n",
      "Step 35: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011760\n",
      "Step 35: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.005658\n",
      "Step 35: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010695\n",
      "Step 35: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.010472\n",
      "Step 35: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.014897\n",
      "Step 35: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010939\n",
      "Step 35: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009414\n",
      "Step 35: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006758\n",
      "Step 35: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.008094\n",
      "Step 35: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.012702\n",
      "Step 35: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.012512\n",
      "Step 35: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.011058\n",
      "Step 35: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.009393\n",
      "Step 35: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010976\n",
      "Step 35: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.013588\n",
      "Step 35: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.008437\n",
      "Step 35: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.016418\n",
      "Step 35: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.005963\n",
      "Step 35: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.009883\n",
      "Step 35: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.013890\n",
      "Step 35: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.011798\n",
      "Step 35: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013852\n",
      "Step 35: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.011842\n",
      "Step 35: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.008511\n",
      "Step 35: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.006496\n",
      "Step 35: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.016360\n",
      "Step 35: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.005437\n",
      "Step 35: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.011100\n",
      "Step 35: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.003302\n",
      "Step 35: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.011685\n",
      "Step 35: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.008963\n",
      "Step 35: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.014998\n",
      "Step 35: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.012413\n",
      "Step 35: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.007580\n",
      "Step 35: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.007645\n",
      "Step 35: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.004587\n",
      "Step 35: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.013739\n",
      "Step 35: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.007066\n",
      "Step 35: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.013042\n",
      "Step 35: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.002453\n",
      "Step 35: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.010405\n",
      "Step 35: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.010946\n",
      "Step 35: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.016264\n",
      "Step 35: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.017156\n",
      "Step 35: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014575\n",
      "Step 35: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.016928\n",
      "Step 35: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.012967\n",
      "Step 35: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.016858\n",
      "Step 35: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012944\n",
      "Step 35: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.011101\n",
      "Step 35: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.015082\n",
      "Step 35: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.010159\n",
      "Step 35: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.006099\n",
      "Step 35: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.005428\n",
      "Step 35: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.013528\n",
      "Step 35: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.014017\n",
      "Step 35: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012666\n",
      "Step 35: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.014182\n",
      "Step 35: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.011441\n",
      "Step 35: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.007869\n",
      "Step 35: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.008485\n",
      "Step 35: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.013160\n",
      "Step 35: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.006917\n",
      "Step 35: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.001340\n",
      "Step 35: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009743\n",
      "Step 35: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008476\n",
      "Step 35: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.011735\n",
      "Step 35: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015692\n",
      "Step 35: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.006349\n",
      "Step 35: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.012583\n",
      "Step 35: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.004300\n",
      "Step 35: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.007194\n",
      "Step 35: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.009808\n",
      "Step 35: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009199\n",
      "Step 35: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.014044\n",
      "Step 35: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.003390\n",
      "Step 35: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.016434\n",
      "Step 35: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.006768\n",
      "Step 35: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.009125\n",
      "Step 35: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.012401\n",
      "Step 35: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008446\n",
      "Step 35: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.006102\n",
      "Step 35: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.003852\n",
      "Step 35: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.015190\n",
      "Step 35: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006735\n",
      "Step 35: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.006702\n",
      "Step 35: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.013834\n",
      "Step 35: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.017037\n",
      "Step 35: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.003823\n",
      "Step 35: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.013695\n",
      "Step 35: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.014590\n",
      "Step 35: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.007701\n",
      "Step 35: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009205\n",
      "Step 35: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.006976\n",
      "Step 35: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.007572\n",
      "Step 35: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.004874\n",
      "Step 35: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.011887\n",
      "Step 35: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.005817\n",
      "Step 35: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.003740\n",
      "Step 35: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.012113\n",
      "Step 35: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.011165\n",
      "Step 35: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.005627\n",
      "Step 35: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.014168\n",
      "Step 35: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.003440\n",
      "Step 35: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.004156\n",
      "Step 35: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008573\n",
      "time =  0:06:56.236933\n",
      "Step 36: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.007983\n",
      "Step 36: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009970\n",
      "Step 36: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.008945\n",
      "Step 36: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.007956\n",
      "Step 36: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.014576\n",
      "Step 36: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.007483\n",
      "Step 36: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.011827\n",
      "Step 36: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008395\n",
      "Step 36: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.003194\n",
      "Step 36: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.009104\n",
      "Step 36: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.013773\n",
      "Step 36: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.014809\n",
      "Step 36: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.010098\n",
      "Step 36: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.011378\n",
      "Step 36: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.005013\n",
      "Step 36: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.015834\n",
      "Step 36: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012563\n",
      "Step 36: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.011508\n",
      "Step 36: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.013667\n",
      "Step 36: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.016596\n",
      "Step 36: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.007542\n",
      "Step 36: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010951\n",
      "Step 36: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.005104\n",
      "Step 36: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.013266\n",
      "Step 36: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.011335\n",
      "Step 36: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.015049\n",
      "Step 36: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008910\n",
      "Step 36: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007601\n",
      "Step 36: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.022758\n",
      "Step 36: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008946\n",
      "Step 36: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009665\n",
      "Step 36: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.010318\n",
      "Step 36: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.015024\n",
      "Step 36: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008848\n",
      "Step 36: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.010165\n",
      "Step 36: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.011600\n",
      "Step 36: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.012950\n",
      "Step 36: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.005224\n",
      "Step 36: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009950\n",
      "Step 36: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.005880\n",
      "Step 36: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.008344\n",
      "Step 36: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.006792\n",
      "Step 36: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.008215\n",
      "Step 36: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.006299\n",
      "Step 36: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009120\n",
      "Step 36: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.013899\n",
      "Step 36: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.001107\n",
      "Step 36: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.010057\n",
      "Step 36: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009981\n",
      "Step 36: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.011160\n",
      "Step 36: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.012558\n",
      "Step 36: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.002500\n",
      "Step 36: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.012355\n",
      "Step 36: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.009922\n",
      "Step 36: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.013641\n",
      "Step 36: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013251\n",
      "Step 36: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.007369\n",
      "Step 36: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.007008\n",
      "Step 36: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.018855\n",
      "Step 36: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009503\n",
      "Step 36: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.019723\n",
      "Step 36: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008120\n",
      "Step 36: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.014205\n",
      "Step 36: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.010296\n",
      "Step 36: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.010606\n",
      "Step 36: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007619\n",
      "Step 36: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.012713\n",
      "Step 36: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.011369\n",
      "Step 36: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.006172\n",
      "Step 36: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.011367\n",
      "Step 36: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.009951\n",
      "Step 36: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009093\n",
      "Step 36: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009158\n",
      "Step 36: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.006914\n",
      "Step 36: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.005870\n",
      "Step 36: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.009360\n",
      "Step 36: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.004268\n",
      "Step 36: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.006454\n",
      "Step 36: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008592\n",
      "Step 36: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.010220\n",
      "Step 36: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.015829\n",
      "Step 36: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.016942\n",
      "Step 36: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008829\n",
      "Step 36: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.005515\n",
      "Step 36: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.008665\n",
      "Step 36: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.010034\n",
      "Step 36: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.009065\n",
      "Step 36: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.009539\n",
      "Step 36: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.005543\n",
      "Step 36: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.010320\n",
      "Step 36: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.008242\n",
      "Step 36: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.006481\n",
      "Step 36: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.013712\n",
      "Step 36: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.004288\n",
      "Step 36: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007340\n",
      "Step 36: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.004580\n",
      "Step 36: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012931\n",
      "Step 36: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010531\n",
      "Step 36: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.012679\n",
      "Step 36: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.010450\n",
      "Step 36: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010859\n",
      "Step 36: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.011163\n",
      "Step 36: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.009075\n",
      "Step 36: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.021561\n",
      "Step 36: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.005577\n",
      "Step 36: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.012295\n",
      "Step 36: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007940\n",
      "Step 36: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.010910\n",
      "Step 36: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.010504\n",
      "Step 36: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.007958\n",
      "Step 36: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.004095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008640\n",
      "Step 36: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010583\n",
      "Step 36: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.011539\n",
      "Step 36: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.004011\n",
      "Step 36: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.009747\n",
      "Step 36: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.005294\n",
      "Step 36: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009325\n",
      "Step 36: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011362\n",
      "Step 36: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.005833\n",
      "Step 36: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008447\n",
      "Step 36: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.006896\n",
      "Step 36: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.010188\n",
      "Step 36: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012133\n",
      "Step 36: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006224\n",
      "Step 36: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012127\n",
      "Step 36: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012786\n",
      "Step 36: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009063\n",
      "Step 36: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009822\n",
      "Step 36: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.013303\n",
      "Step 36: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008315\n",
      "Step 36: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.006742\n",
      "Step 36: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.006361\n",
      "Step 36: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.003099\n",
      "Step 36: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.007902\n",
      "Step 36: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.014030\n",
      "Step 36: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010906\n",
      "Step 36: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.015652\n",
      "Step 36: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010872\n",
      "Step 36: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.011153\n",
      "Step 36: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.000689\n",
      "Step 36: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.009749\n",
      "Step 36: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008184\n",
      "Step 36: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.014102\n",
      "Step 36: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.004377\n",
      "Step 36: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009888\n",
      "Step 36: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010601\n",
      "time =  0:07:01.093943\n",
      "Step 37: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.009620\n",
      "Step 37: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009552\n",
      "Step 37: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.016803\n",
      "Step 37: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.013553\n",
      "Step 37: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010286\n",
      "Step 37: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.011455\n",
      "Step 37: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.009614\n",
      "Step 37: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.007870\n",
      "Step 37: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010501\n",
      "Step 37: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013739\n",
      "Step 37: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.011083\n",
      "Step 37: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.010383\n",
      "Step 37: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.006883\n",
      "Step 37: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.009304\n",
      "Step 37: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.008103\n",
      "Step 37: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.002017\n",
      "Step 37: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.004320\n",
      "Step 37: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.016593\n",
      "Step 37: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.013234\n",
      "Step 37: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.015461\n",
      "Step 37: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.013809\n",
      "Step 37: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.019561\n",
      "Step 37: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.012071\n",
      "Step 37: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.006483\n",
      "Step 37: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.009457\n",
      "Step 37: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.006148\n",
      "Step 37: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.013099\n",
      "Step 37: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009423\n",
      "Step 37: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.013229\n",
      "Step 37: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.017024\n",
      "Step 37: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.007198\n",
      "Step 37: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.008699\n",
      "Step 37: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.007630\n",
      "Step 37: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008683\n",
      "Step 37: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.013229\n",
      "Step 37: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.010316\n",
      "Step 37: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.009716\n",
      "Step 37: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010124\n",
      "Step 37: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.005558\n",
      "Step 37: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.007811\n",
      "Step 37: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011336\n",
      "Step 37: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.007589\n",
      "Step 37: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.007633\n",
      "Step 37: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.011218\n",
      "Step 37: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.014937\n",
      "Step 37: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.006324\n",
      "Step 37: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.008370\n",
      "Step 37: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.013710\n",
      "Step 37: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.004019\n",
      "Step 37: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.011246\n",
      "Step 37: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.014357\n",
      "Step 37: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.014459\n",
      "Step 37: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.019096\n",
      "Step 37: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010558\n",
      "Step 37: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.010171\n",
      "Step 37: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.014292\n",
      "Step 37: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.011078\n",
      "Step 37: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010350\n",
      "Step 37: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010411\n",
      "Step 37: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.001223\n",
      "Step 37: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.013937\n",
      "Step 37: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.007088\n",
      "Step 37: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.011658\n",
      "Step 37: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.012700\n",
      "Step 37: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.013236\n",
      "Step 37: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007779\n",
      "Step 37: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011783\n",
      "Step 37: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009274\n",
      "Step 37: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010236\n",
      "Step 37: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009149\n",
      "Step 37: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.019095\n",
      "Step 37: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.013405\n",
      "Step 37: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.016189\n",
      "Step 37: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.007363\n",
      "Step 37: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009747\n",
      "Step 37: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011580\n",
      "Step 37: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.002761\n",
      "Step 37: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.020923\n",
      "Step 37: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.005277\n",
      "Step 37: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.007152\n",
      "Step 37: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009073\n",
      "Step 37: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.012865\n",
      "Step 37: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011092\n",
      "Step 37: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.001827\n",
      "Step 37: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.011073\n",
      "Step 37: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.006518\n",
      "Step 37: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008064\n",
      "Step 37: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.009992\n",
      "Step 37: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.014978\n",
      "Step 37: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.002474\n",
      "Step 37: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.007971\n",
      "Step 37: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.010338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.011639\n",
      "Step 37: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.012778\n",
      "Step 37: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.010532\n",
      "Step 37: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.012376\n",
      "Step 37: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.009039\n",
      "Step 37: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010038\n",
      "Step 37: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.004123\n",
      "Step 37: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.010533\n",
      "Step 37: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.006812\n",
      "Step 37: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006791\n",
      "Step 37: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.008477\n",
      "Step 37: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011911\n",
      "Step 37: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.012961\n",
      "Step 37: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.009760\n",
      "Step 37: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.009530\n",
      "Step 37: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.010825\n",
      "Step 37: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.015826\n",
      "Step 37: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.014299\n",
      "Step 37: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.013632\n",
      "Step 37: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008800\n",
      "Step 37: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.005276\n",
      "Step 37: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.013957\n",
      "Step 37: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011160\n",
      "Step 37: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.008665\n",
      "Step 37: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.008953\n",
      "Step 37: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.013495\n",
      "Step 37: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.005880\n",
      "Step 37: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.013034\n",
      "Step 37: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.015809\n",
      "Step 37: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.010276\n",
      "Step 37: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.009546\n",
      "Step 37: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.013192\n",
      "Step 37: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.014084\n",
      "Step 37: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.011142\n",
      "Step 37: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.012962\n",
      "Step 37: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009773\n",
      "Step 37: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009627\n",
      "Step 37: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.011523\n",
      "Step 37: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008335\n",
      "Step 37: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.007010\n",
      "Step 37: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009397\n",
      "Step 37: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.008924\n",
      "Step 37: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.014773\n",
      "Step 37: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010614\n",
      "Step 37: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011006\n",
      "Step 37: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.018661\n",
      "Step 37: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.016061\n",
      "Step 37: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.009074\n",
      "Step 37: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.010831\n",
      "Step 37: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.015174\n",
      "Step 37: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.011582\n",
      "Step 37: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008070\n",
      "Step 37: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008113\n",
      "Step 37: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.012651\n",
      "Step 37: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010079\n",
      "time =  0:06:56.059272\n",
      "Step 38: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.010620\n",
      "Step 38: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.014012\n",
      "Step 38: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.013602\n",
      "Step 38: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.007788\n",
      "Step 38: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.008234\n",
      "Step 38: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009788\n",
      "Step 38: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.006023\n",
      "Step 38: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008429\n",
      "Step 38: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.007596\n",
      "Step 38: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011245\n",
      "Step 38: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008317\n",
      "Step 38: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.013679\n",
      "Step 38: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.003053\n",
      "Step 38: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.009409\n",
      "Step 38: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.004093\n",
      "Step 38: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.011500\n",
      "Step 38: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012799\n",
      "Step 38: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012505\n",
      "Step 38: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.016122\n",
      "Step 38: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.019476\n",
      "Step 38: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.006319\n",
      "Step 38: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.007272\n",
      "Step 38: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.015267\n",
      "Step 38: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.007008\n",
      "Step 38: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.013270\n",
      "Step 38: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.012361\n",
      "Step 38: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.009887\n",
      "Step 38: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.011884\n",
      "Step 38: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.004885\n",
      "Step 38: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011359\n",
      "Step 38: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011234\n",
      "Step 38: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.012784\n",
      "Step 38: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009440\n",
      "Step 38: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008377\n",
      "Step 38: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.007176\n",
      "Step 38: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.005845\n",
      "Step 38: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.013637\n",
      "Step 38: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.009700\n",
      "Step 38: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.014296\n",
      "Step 38: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.006452\n",
      "Step 38: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.009472\n",
      "Step 38: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.012028\n",
      "Step 38: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.012669\n",
      "Step 38: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.009162\n",
      "Step 38: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.016079\n",
      "Step 38: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.014862\n",
      "Step 38: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.011873\n",
      "Step 38: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.012796\n",
      "Step 38: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.011161\n",
      "Step 38: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.009952\n",
      "Step 38: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005413\n",
      "Step 38: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.013889\n",
      "Step 38: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.009166\n",
      "Step 38: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.012029\n",
      "Step 38: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.014265\n",
      "Step 38: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.009812\n",
      "Step 38: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.010107\n",
      "Step 38: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.011318\n",
      "Step 38: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.009983\n",
      "Step 38: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.011097\n",
      "Step 38: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009049\n",
      "Step 38: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006533\n",
      "Step 38: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.011566\n",
      "Step 38: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011505\n",
      "Step 38: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.016480\n",
      "Step 38: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.003827\n",
      "Step 38: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.013950\n",
      "Step 38: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.010789\n",
      "Step 38: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.005172\n",
      "Step 38: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009106\n",
      "Step 38: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.018522\n",
      "Step 38: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.012526\n",
      "Step 38: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.013985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.011678\n",
      "Step 38: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.004384\n",
      "Step 38: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.012309\n",
      "Step 38: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.008872\n",
      "Step 38: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008319\n",
      "Step 38: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008793\n",
      "Step 38: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.015185\n",
      "Step 38: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011198\n",
      "Step 38: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008152\n",
      "Step 38: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.015495\n",
      "Step 38: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.009512\n",
      "Step 38: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014965\n",
      "Step 38: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.012924\n",
      "Step 38: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008923\n",
      "Step 38: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.006307\n",
      "Step 38: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.014479\n",
      "Step 38: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.004589\n",
      "Step 38: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.010887\n",
      "Step 38: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.014748\n",
      "Step 38: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.011114\n",
      "Step 38: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.013686\n",
      "Step 38: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007295\n",
      "Step 38: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.012794\n",
      "Step 38: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.002888\n",
      "Step 38: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.013011\n",
      "Step 38: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.007674\n",
      "Step 38: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.006242\n",
      "Step 38: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.006238\n",
      "Step 38: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.011354\n",
      "Step 38: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.011823\n",
      "Step 38: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.017289\n",
      "Step 38: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009164\n",
      "Step 38: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.007363\n",
      "Step 38: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.014473\n",
      "Step 38: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.011294\n",
      "Step 38: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.005582\n",
      "Step 38: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.012841\n",
      "Step 38: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.010578\n",
      "Step 38: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.011377\n",
      "Step 38: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.015872\n",
      "Step 38: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.006809\n",
      "Step 38: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011676\n",
      "Step 38: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.013809\n",
      "Step 38: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.008999\n",
      "Step 38: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.006894\n",
      "Step 38: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.008101\n",
      "Step 38: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.013977\n",
      "Step 38: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.007248\n",
      "Step 38: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.005898\n",
      "Step 38: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011510\n",
      "Step 38: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.012268\n",
      "Step 38: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006242\n",
      "Step 38: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.007057\n",
      "Step 38: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.015132\n",
      "Step 38: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.007352\n",
      "Step 38: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011783\n",
      "Step 38: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009992\n",
      "Step 38: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.018091\n",
      "Step 38: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.010604\n",
      "Step 38: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.007784\n",
      "Step 38: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012123\n",
      "Step 38: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.017486\n",
      "Step 38: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.006459\n",
      "Step 38: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.015869\n",
      "Step 38: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.007922\n",
      "Step 38: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010548\n",
      "Step 38: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006651\n",
      "Step 38: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.007838\n",
      "Step 38: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007462\n",
      "Step 38: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.012832\n",
      "Step 38: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009529\n",
      "Step 38: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010525\n",
      "Step 38: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.009754\n",
      "Step 38: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.008145\n",
      "time =  0:06:56.317968\n",
      "Step 39: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.012650\n",
      "Step 39: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009943\n",
      "Step 39: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.005475\n",
      "Step 39: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.010776\n",
      "Step 39: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.014976\n",
      "Step 39: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.006955\n",
      "Step 39: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.014550\n",
      "Step 39: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009902\n",
      "Step 39: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.001549\n",
      "Step 39: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.010637\n",
      "Step 39: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008532\n",
      "Step 39: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.007884\n",
      "Step 39: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009622\n",
      "Step 39: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.010735\n",
      "Step 39: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.005315\n",
      "Step 39: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009889\n",
      "Step 39: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.013700\n",
      "Step 39: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.006761\n",
      "Step 39: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.005614\n",
      "Step 39: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011055\n",
      "Step 39: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010378\n",
      "Step 39: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.006664\n",
      "Step 39: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.010710\n",
      "Step 39: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.002656\n",
      "Step 39: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.010539\n",
      "Step 39: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.005370\n",
      "Step 39: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008183\n",
      "Step 39: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.008247\n",
      "Step 39: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009539\n",
      "Step 39: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.018906\n",
      "Step 39: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.010905\n",
      "Step 39: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.006769\n",
      "Step 39: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.006002\n",
      "Step 39: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.010662\n",
      "Step 39: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.015771\n",
      "Step 39: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.007580\n",
      "Step 39: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.006282\n",
      "Step 39: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010792\n",
      "Step 39: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.003667\n",
      "Step 39: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009404\n",
      "Step 39: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.007955\n",
      "Step 39: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009437\n",
      "Step 39: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.011299\n",
      "Step 39: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.005644\n",
      "Step 39: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009266\n",
      "Step 39: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.011498\n",
      "Step 39: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.012435\n",
      "Step 39: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.016105\n",
      "Step 39: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.003613\n",
      "Step 39: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.007206\n",
      "Step 39: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.013557\n",
      "Step 39: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.007015\n",
      "Step 39: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.014600\n",
      "Step 39: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.002294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.008755\n",
      "Step 39: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.016256\n",
      "Step 39: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.007104\n",
      "Step 39: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.005149\n",
      "Step 39: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008332\n",
      "Step 39: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.014085\n",
      "Step 39: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.008349\n",
      "Step 39: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.007385\n",
      "Step 39: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.004991\n",
      "Step 39: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.010112\n",
      "Step 39: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.011047\n",
      "Step 39: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.012339\n",
      "Step 39: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.006405\n",
      "Step 39: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008210\n",
      "Step 39: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.014053\n",
      "Step 39: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.014166\n",
      "Step 39: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.013667\n",
      "Step 39: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009762\n",
      "Step 39: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.015091\n",
      "Step 39: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.010263\n",
      "Step 39: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011533\n",
      "Step 39: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.009027\n",
      "Step 39: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.018990\n",
      "Step 39: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.006822\n",
      "Step 39: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.007849\n",
      "Step 39: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.012423\n",
      "Step 39: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.007164\n",
      "Step 39: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.011219\n",
      "Step 39: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.003257\n",
      "Step 39: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.009932\n",
      "Step 39: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005277\n",
      "Step 39: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007344\n",
      "Step 39: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010875\n",
      "Step 39: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.003220\n",
      "Step 39: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012768\n",
      "Step 39: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.004576\n",
      "Step 39: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.002868\n",
      "Step 39: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.006465\n",
      "Step 39: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.021453\n",
      "Step 39: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.014511\n",
      "Step 39: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.008149\n",
      "Step 39: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010618\n",
      "Step 39: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010985\n",
      "Step 39: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.012485\n",
      "Step 39: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.014906\n",
      "Step 39: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.013945\n",
      "Step 39: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009660\n",
      "Step 39: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.007139\n",
      "Step 39: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.003758\n",
      "Step 39: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.014921\n",
      "Step 39: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.008750\n",
      "Step 39: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.011289\n",
      "Step 39: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.012834\n",
      "Step 39: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.012093\n",
      "Step 39: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009487\n",
      "Step 39: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.005110\n",
      "Step 39: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.009156\n",
      "Step 39: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008168\n",
      "Step 39: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.013978\n",
      "Step 39: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.011285\n",
      "Step 39: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009359\n",
      "Step 39: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007827\n",
      "Step 39: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.010812\n",
      "Step 39: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.011757\n",
      "Step 39: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.014658\n",
      "Step 39: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009964\n",
      "Step 39: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.007196\n",
      "Step 39: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.012796\n",
      "Step 39: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.008189\n",
      "Step 39: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.010862\n",
      "Step 39: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.014594\n",
      "Step 39: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.007303\n",
      "Step 39: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.006472\n",
      "Step 39: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009203\n",
      "Step 39: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.012393\n",
      "Step 39: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009418\n",
      "Step 39: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.007765\n",
      "Step 39: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.007780\n",
      "Step 39: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.005934\n",
      "Step 39: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.005150\n",
      "Step 39: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.011747\n",
      "Step 39: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.013058\n",
      "Step 39: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.015801\n",
      "Step 39: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.009383\n",
      "Step 39: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010635\n",
      "Step 39: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.007727\n",
      "Step 39: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.012550\n",
      "Step 39: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.015008\n",
      "Step 39: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.004733\n",
      "Step 39: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.001784\n",
      "Step 39: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008595\n",
      "Step 39: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.008233\n",
      "Step 39: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.013180\n",
      "time =  0:07:02.207672\n",
      "Step 40: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.004275\n",
      "Step 40: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.016042\n",
      "Step 40: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.005628\n",
      "Step 40: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008836\n",
      "Step 40: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010397\n",
      "Step 40: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.011801\n",
      "Step 40: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.014652\n",
      "Step 40: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.011632\n",
      "Step 40: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009459\n",
      "Step 40: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.011627\n",
      "Step 40: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008745\n",
      "Step 40: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.004889\n",
      "Step 40: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.017681\n",
      "Step 40: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.014533\n",
      "Step 40: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006086\n",
      "Step 40: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.020855\n",
      "Step 40: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.000813\n",
      "Step 40: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.015939\n",
      "Step 40: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.001723\n",
      "Step 40: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.012020\n",
      "Step 40: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010434\n",
      "Step 40: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.008288\n",
      "Step 40: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009706\n",
      "Step 40: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008482\n",
      "Step 40: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008546\n",
      "Step 40: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.014080\n",
      "Step 40: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.007595\n",
      "Step 40: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.013186\n",
      "Step 40: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.004466\n",
      "Step 40: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.007291\n",
      "Step 40: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.010647\n",
      "Step 40: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.009951\n",
      "Step 40: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.016465\n",
      "Step 40: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.009647\n",
      "Step 40: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.011288\n",
      "Step 40: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.008416\n",
      "Step 40: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.015990\n",
      "Step 40: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010559\n",
      "Step 40: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.006243\n",
      "Step 40: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011740\n",
      "Step 40: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.015792\n",
      "Step 40: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.014543\n",
      "Step 40: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.008891\n",
      "Step 40: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.007666\n",
      "Step 40: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.009762\n",
      "Step 40: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.003382\n",
      "Step 40: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006480\n",
      "Step 40: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.012072\n",
      "Step 40: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.013244\n",
      "Step 40: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.009643\n",
      "Step 40: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.003672\n",
      "Step 40: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010594\n",
      "Step 40: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.011023\n",
      "Step 40: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.002526\n",
      "Step 40: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006738\n",
      "Step 40: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.012082\n",
      "Step 40: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.004993\n",
      "Step 40: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010320\n",
      "Step 40: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.005238\n",
      "Step 40: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.012629\n",
      "Step 40: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.006401\n",
      "Step 40: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.004523\n",
      "Step 40: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011816\n",
      "Step 40: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.008297\n",
      "Step 40: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.008743\n",
      "Step 40: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.011040\n",
      "Step 40: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.009882\n",
      "Step 40: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.007614\n",
      "Step 40: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.014292\n",
      "Step 40: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.011474\n",
      "Step 40: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010197\n",
      "Step 40: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.007018\n",
      "Step 40: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.016981\n",
      "Step 40: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.013372\n",
      "Step 40: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.008790\n",
      "Step 40: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.008394\n",
      "Step 40: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008188\n",
      "Step 40: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009287\n",
      "Step 40: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009560\n",
      "Step 40: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.008547\n",
      "Step 40: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.016938\n",
      "Step 40: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.012716\n",
      "Step 40: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.011986\n",
      "Step 40: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.011680\n",
      "Step 40: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.014879\n",
      "Step 40: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008563\n",
      "Step 40: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010115\n",
      "Step 40: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007869\n",
      "Step 40: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.011086\n",
      "Step 40: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.014145\n",
      "Step 40: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.007299\n",
      "Step 40: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.009011\n",
      "Step 40: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.007195\n",
      "Step 40: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.009761\n",
      "Step 40: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.007751\n",
      "Step 40: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.003729\n",
      "Step 40: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007673\n",
      "Step 40: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.011218\n",
      "Step 40: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.010247\n",
      "Step 40: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.013408\n",
      "Step 40: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.010830\n",
      "Step 40: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.014215\n",
      "Step 40: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.009444\n",
      "Step 40: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.012957\n",
      "Step 40: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.006826\n",
      "Step 40: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.009551\n",
      "Step 40: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.016227\n",
      "Step 40: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.015462\n",
      "Step 40: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.013496\n",
      "Step 40: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.013121\n",
      "Step 40: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.010882\n",
      "Step 40: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.012652\n",
      "Step 40: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009022\n",
      "Step 40: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.005661\n",
      "Step 40: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007288\n",
      "Step 40: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.009307\n",
      "Step 40: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.012344\n",
      "Step 40: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.006536\n",
      "Step 40: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.016046\n",
      "Step 40: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.011811\n",
      "Step 40: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.007239\n",
      "Step 40: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011677\n",
      "Step 40: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.007193\n",
      "Step 40: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.013560\n",
      "Step 40: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.011999\n",
      "Step 40: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.007542\n",
      "Step 40: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.008759\n",
      "Step 40: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.007509\n",
      "Step 40: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008126\n",
      "Step 40: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.009052\n",
      "Step 40: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008443\n",
      "Step 40: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009541\n",
      "Step 40: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012743\n",
      "Step 40: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.005210\n",
      "Step 40: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.007419\n",
      "Step 40: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010461\n",
      "Step 40: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.005700\n",
      "Step 40: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.012472\n",
      "Step 40: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.007929\n",
      "Step 40: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.013547\n",
      "Step 40: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.014051\n",
      "Step 40: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.011581\n",
      "Step 40: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009303\n",
      "Step 40: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.006715\n",
      "Step 40: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.012097\n",
      "Step 40: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009784\n",
      "time =  0:07:02.294811\n",
      "Step 41: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.005800\n",
      "Step 41: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.005840\n",
      "Step 41: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.009998\n",
      "Step 41: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.012882\n",
      "Step 41: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011797\n",
      "Step 41: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008539\n",
      "Step 41: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.009556\n",
      "Step 41: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.014459\n",
      "Step 41: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009386\n",
      "Step 41: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013868\n",
      "Step 41: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.009418\n",
      "Step 41: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.015409\n",
      "Step 41: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.018791\n",
      "Step 41: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007423\n",
      "Step 41: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.007461\n",
      "Step 41: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.014375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.009752\n",
      "Step 41: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012505\n",
      "Step 41: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.006808\n",
      "Step 41: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.011269\n",
      "Step 41: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.008963\n",
      "Step 41: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.012938\n",
      "Step 41: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.016899\n",
      "Step 41: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.001838\n",
      "Step 41: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008780\n",
      "Step 41: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.005349\n",
      "Step 41: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.005141\n",
      "Step 41: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.023893\n",
      "Step 41: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009610\n",
      "Step 41: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.011389\n",
      "Step 41: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009947\n",
      "Step 41: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.010810\n",
      "Step 41: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011327\n",
      "Step 41: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.004616\n",
      "Step 41: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.016276\n",
      "Step 41: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.009730\n",
      "Step 41: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.010566\n",
      "Step 41: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.005050\n",
      "Step 41: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010629\n",
      "Step 41: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.013707\n",
      "Step 41: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011695\n",
      "Step 41: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.009699\n",
      "Step 41: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.008278\n",
      "Step 41: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.018766\n",
      "Step 41: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008142\n",
      "Step 41: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.013155\n",
      "Step 41: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007760\n",
      "Step 41: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.010850\n",
      "Step 41: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.013184\n",
      "Step 41: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.005596\n",
      "Step 41: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.012458\n",
      "Step 41: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.004421\n",
      "Step 41: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.008289\n",
      "Step 41: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010659\n",
      "Step 41: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.012222\n",
      "Step 41: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.011994\n",
      "Step 41: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014052\n",
      "Step 41: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.007612\n",
      "Step 41: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.006984\n",
      "Step 41: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.006814\n",
      "Step 41: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007440\n",
      "Step 41: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.012797\n",
      "Step 41: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.008813\n",
      "Step 41: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011411\n",
      "Step 41: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.017541\n",
      "Step 41: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.012367\n",
      "Step 41: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.008382\n",
      "Step 41: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008449\n",
      "Step 41: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.006477\n",
      "Step 41: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.004682\n",
      "Step 41: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.002263\n",
      "Step 41: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.009701\n",
      "Step 41: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.005460\n",
      "Step 41: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.012362\n",
      "Step 41: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009769\n",
      "Step 41: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.003869\n",
      "Step 41: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.004869\n",
      "Step 41: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008769\n",
      "Step 41: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.015770\n",
      "Step 41: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009943\n",
      "Step 41: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.008431\n",
      "Step 41: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.005461\n",
      "Step 41: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.005137\n",
      "Step 41: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.011980\n",
      "Step 41: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009922\n",
      "Step 41: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.010187\n",
      "Step 41: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.007597\n",
      "Step 41: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011223\n",
      "Step 41: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.010455\n",
      "Step 41: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.012782\n",
      "Step 41: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.011474\n",
      "Step 41: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.006595\n",
      "Step 41: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.009222\n",
      "Step 41: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.002070\n",
      "Step 41: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.011460\n",
      "Step 41: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.015064\n",
      "Step 41: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.008482\n",
      "Step 41: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.012262\n",
      "Step 41: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.015674\n",
      "Step 41: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008525\n",
      "Step 41: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.003748\n",
      "Step 41: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.009102\n",
      "Step 41: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.017420\n",
      "Step 41: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.010541\n",
      "Step 41: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.015226\n",
      "Step 41: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.006204\n",
      "Step 41: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.006810\n",
      "Step 41: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009659\n",
      "Step 41: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.010937\n",
      "Step 41: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.003966\n",
      "Step 41: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.007769\n",
      "Step 41: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.012522\n",
      "Step 41: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.007892\n",
      "Step 41: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009884\n",
      "Step 41: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011077\n",
      "Step 41: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.008104\n",
      "Step 41: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.014002\n",
      "Step 41: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009071\n",
      "Step 41: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.008527\n",
      "Step 41: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.012071\n",
      "Step 41: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008335\n",
      "Step 41: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.005737\n",
      "Step 41: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.007599\n",
      "Step 41: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.008720\n",
      "Step 41: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.008422\n",
      "Step 41: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.023550\n",
      "Step 41: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.010541\n",
      "Step 41: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.008037\n",
      "Step 41: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.013474\n",
      "Step 41: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.011440\n",
      "Step 41: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.015375\n",
      "Step 41: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.013554\n",
      "Step 41: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.010375\n",
      "Step 41: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.012902\n",
      "Step 41: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.009682\n",
      "Step 41: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011493\n",
      "Step 41: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010715\n",
      "Step 41: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.010808\n",
      "Step 41: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.011917\n",
      "Step 41: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.007611\n",
      "Step 41: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.008551\n",
      "Step 41: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.003790\n",
      "Step 41: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.010749\n",
      "Step 41: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.005916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008896\n",
      "Step 41: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011427\n",
      "Step 41: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.012854\n",
      "time =  0:06:54.840750\n",
      "Step 42: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.015680\n",
      "Step 42: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.006782\n",
      "Step 42: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.005971\n",
      "Step 42: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.009486\n",
      "Step 42: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010752\n",
      "Step 42: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008729\n",
      "Step 42: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.007842\n",
      "Step 42: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.011214\n",
      "Step 42: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.012327\n",
      "Step 42: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.013168\n",
      "Step 42: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.007067\n",
      "Step 42: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.014574\n",
      "Step 42: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.011152\n",
      "Step 42: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007708\n",
      "Step 42: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.012029\n",
      "Step 42: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.014408\n",
      "Step 42: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.014840\n",
      "Step 42: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.018360\n",
      "Step 42: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010475\n",
      "Step 42: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.006853\n",
      "Step 42: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010802\n",
      "Step 42: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.012316\n",
      "Step 42: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.008425\n",
      "Step 42: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.013988\n",
      "Step 42: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.001506\n",
      "Step 42: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.014584\n",
      "Step 42: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.015801\n",
      "Step 42: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.015768\n",
      "Step 42: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.010634\n",
      "Step 42: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.009914\n",
      "Step 42: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.006216\n",
      "Step 42: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.010343\n",
      "Step 42: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009798\n",
      "Step 42: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.017950\n",
      "Step 42: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.014012\n",
      "Step 42: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.004220\n",
      "Step 42: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.009067\n",
      "Step 42: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.011161\n",
      "Step 42: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009930\n",
      "Step 42: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.010241\n",
      "Step 42: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.006694\n",
      "Step 42: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.013321\n",
      "Step 42: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.009126\n",
      "Step 42: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.008912\n",
      "Step 42: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012963\n",
      "Step 42: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.007191\n",
      "Step 42: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.012616\n",
      "Step 42: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.009584\n",
      "Step 42: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.012151\n",
      "Step 42: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.010754\n",
      "Step 42: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.009910\n",
      "Step 42: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.004826\n",
      "Step 42: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.007298\n",
      "Step 42: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.006234\n",
      "Step 42: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.007403\n",
      "Step 42: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.007712\n",
      "Step 42: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.003572\n",
      "Step 42: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.016634\n",
      "Step 42: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.007511\n",
      "Step 42: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.012457\n",
      "Step 42: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.005640\n",
      "Step 42: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.009140\n",
      "Step 42: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.007923\n",
      "Step 42: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.011450\n",
      "Step 42: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.009816\n",
      "Step 42: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.014298\n",
      "Step 42: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010403\n",
      "Step 42: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008936\n",
      "Step 42: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.004990\n",
      "Step 42: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.015127\n",
      "Step 42: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.013838\n",
      "Step 42: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.010235\n",
      "Step 42: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.004642\n",
      "Step 42: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.005656\n",
      "Step 42: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.010542\n",
      "Step 42: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.006598\n",
      "Step 42: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009414\n",
      "Step 42: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.013546\n",
      "Step 42: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.005712\n",
      "Step 42: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.006606\n",
      "Step 42: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.014423\n",
      "Step 42: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.009167\n",
      "Step 42: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.005155\n",
      "Step 42: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013598\n",
      "Step 42: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.008782\n",
      "Step 42: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007732\n",
      "Step 42: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.006963\n",
      "Step 42: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011583\n",
      "Step 42: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.018889\n",
      "Step 42: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009325\n",
      "Step 42: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.015702\n",
      "Step 42: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.008017\n",
      "Step 42: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.007286\n",
      "Step 42: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.016074\n",
      "Step 42: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.018558\n",
      "Step 42: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.008508\n",
      "Step 42: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.007657\n",
      "Step 42: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.008772\n",
      "Step 42: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.008387\n",
      "Step 42: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.003438\n",
      "Step 42: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.009236\n",
      "Step 42: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.011479\n",
      "Step 42: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.011520\n",
      "Step 42: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.007339\n",
      "Step 42: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009550\n",
      "Step 42: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.013555\n",
      "Step 42: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.009440\n",
      "Step 42: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.011264\n",
      "Step 42: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.014859\n",
      "Step 42: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.010200\n",
      "Step 42: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.009525\n",
      "Step 42: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.007404\n",
      "Step 42: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.013148\n",
      "Step 42: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.004228\n",
      "Step 42: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010329\n",
      "Step 42: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011337\n",
      "Step 42: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.008161\n",
      "Step 42: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.012741\n",
      "Step 42: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.005075\n",
      "Step 42: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.011692\n",
      "Step 42: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.009623\n",
      "Step 42: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.008332\n",
      "Step 42: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.006982\n",
      "Step 42: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.014470\n",
      "Step 42: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.007502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.009102\n",
      "Step 42: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.005797\n",
      "Step 42: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009513\n",
      "Step 42: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.010336\n",
      "Step 42: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.011969\n",
      "Step 42: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.011118\n",
      "Step 42: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.005314\n",
      "Step 42: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.009187\n",
      "Step 42: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.006622\n",
      "Step 42: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.007605\n",
      "Step 42: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009842\n",
      "Step 42: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008431\n",
      "Step 42: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.009406\n",
      "Step 42: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.005091\n",
      "Step 42: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.012048\n",
      "Step 42: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.007060\n",
      "Step 42: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.008220\n",
      "Step 42: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.009112\n",
      "Step 42: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.014935\n",
      "Step 42: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.008728\n",
      "Step 42: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.010528\n",
      "Step 42: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.013793\n",
      "time =  0:06:59.737278\n",
      "Step 43: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.008135\n",
      "Step 43: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.012833\n",
      "Step 43: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.011478\n",
      "Step 43: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.014525\n",
      "Step 43: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007059\n",
      "Step 43: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.012384\n",
      "Step 43: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.011307\n",
      "Step 43: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009825\n",
      "Step 43: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.009540\n",
      "Step 43: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.012380\n",
      "Step 43: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008091\n",
      "Step 43: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.006240\n",
      "Step 43: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.015983\n",
      "Step 43: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.005687\n",
      "Step 43: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006987\n",
      "Step 43: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.015495\n",
      "Step 43: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.011763\n",
      "Step 43: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.012145\n",
      "Step 43: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.005574\n",
      "Step 43: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.012312\n",
      "Step 43: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.011719\n",
      "Step 43: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.006995\n",
      "Step 43: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.011951\n",
      "Step 43: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.017271\n",
      "Step 43: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.024487\n",
      "Step 43: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.014926\n",
      "Step 43: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.009769\n",
      "Step 43: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.016563\n",
      "Step 43: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009072\n",
      "Step 43: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008197\n",
      "Step 43: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.009775\n",
      "Step 43: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.013067\n",
      "Step 43: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.007962\n",
      "Step 43: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.008572\n",
      "Step 43: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.001702\n",
      "Step 43: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.009557\n",
      "Step 43: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.009052\n",
      "Step 43: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.012351\n",
      "Step 43: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.008474\n",
      "Step 43: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.008040\n",
      "Step 43: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011112\n",
      "Step 43: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.012876\n",
      "Step 43: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.017188\n",
      "Step 43: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.007653\n",
      "Step 43: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.018509\n",
      "Step 43: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.016644\n",
      "Step 43: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.014388\n",
      "Step 43: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.006907\n",
      "Step 43: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.006567\n",
      "Step 43: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.010893\n",
      "Step 43: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.005660\n",
      "Step 43: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.006347\n",
      "Step 43: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010520\n",
      "Step 43: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.008991\n",
      "Step 43: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.016972\n",
      "Step 43: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.006491\n",
      "Step 43: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.014911\n",
      "Step 43: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.007618\n",
      "Step 43: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.012653\n",
      "Step 43: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.010430\n",
      "Step 43: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.007322\n",
      "Step 43: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.013031\n",
      "Step 43: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.009819\n",
      "Step 43: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009707\n",
      "Step 43: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.015001\n",
      "Step 43: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010254\n",
      "Step 43: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.007512\n",
      "Step 43: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.006747\n",
      "Step 43: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.006462\n",
      "Step 43: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009807\n",
      "Step 43: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.009879\n",
      "Step 43: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.008805\n",
      "Step 43: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.010063\n",
      "Step 43: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009234\n",
      "Step 43: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.016177\n",
      "Step 43: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.008851\n",
      "Step 43: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009631\n",
      "Step 43: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009479\n",
      "Step 43: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.010027\n",
      "Step 43: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009525\n",
      "Step 43: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009006\n",
      "Step 43: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.005561\n",
      "Step 43: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.008560\n",
      "Step 43: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.017573\n",
      "Step 43: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.006748\n",
      "Step 43: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.002666\n",
      "Step 43: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.010671\n",
      "Step 43: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.009493\n",
      "Step 43: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.016247\n",
      "Step 43: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009379\n",
      "Step 43: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.013991\n",
      "Step 43: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.015012\n",
      "Step 43: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.016461\n",
      "Step 43: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.013331\n",
      "Step 43: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.013951\n",
      "Step 43: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010132\n",
      "Step 43: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.007523\n",
      "Step 43: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.011719\n",
      "Step 43: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.005002\n",
      "Step 43: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.013979\n",
      "Step 43: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.026364\n",
      "Step 43: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.010879\n",
      "Step 43: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.015136\n",
      "Step 43: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.005884\n",
      "Step 43: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.008147\n",
      "Step 43: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.011746\n",
      "Step 43: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.015094\n",
      "Step 43: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.014189\n",
      "Step 43: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.010732\n",
      "Step 43: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.014103\n",
      "Step 43: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.012392\n",
      "Step 43: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011746\n",
      "Step 43: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009911\n",
      "Step 43: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009369\n",
      "Step 43: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007663\n",
      "Step 43: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.002784\n",
      "Step 43: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.015341\n",
      "Step 43: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011979\n",
      "Step 43: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.009993\n",
      "Step 43: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008317\n",
      "Step 43: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.008129\n",
      "Step 43: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011599\n",
      "Step 43: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.008882\n",
      "Step 43: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.008461\n",
      "Step 43: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.005340\n",
      "Step 43: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.017696\n",
      "Step 43: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.008709\n",
      "Step 43: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.006542\n",
      "Step 43: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009801\n",
      "Step 43: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008157\n",
      "Step 43: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.007323\n",
      "Step 43: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.008159\n",
      "Step 43: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009874\n",
      "Step 43: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.013415\n",
      "Step 43: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.005343\n",
      "Step 43: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.016772\n",
      "Step 43: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.008003\n",
      "Step 43: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010075\n",
      "Step 43: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006678\n",
      "Step 43: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.006120\n",
      "Step 43: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007845\n",
      "Step 43: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008701\n",
      "Step 43: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.009493\n",
      "Step 43: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.013161\n",
      "Step 43: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.013388\n",
      "Step 43: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.006851\n",
      "time =  0:06:59.688539\n",
      "Step 44: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.008108\n",
      "Step 44: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.005999\n",
      "Step 44: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.012882\n",
      "Step 44: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.007705\n",
      "Step 44: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007520\n",
      "Step 44: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.012215\n",
      "Step 44: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.017053\n",
      "Step 44: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008764\n",
      "Step 44: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010168\n",
      "Step 44: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.015707\n",
      "Step 44: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.010516\n",
      "Step 44: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.011496\n",
      "Step 44: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.013094\n",
      "Step 44: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.012918\n",
      "Step 44: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.013543\n",
      "Step 44: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.011529\n",
      "Step 44: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.014018\n",
      "Step 44: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.016579\n",
      "Step 44: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.011117\n",
      "Step 44: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.007373\n",
      "Step 44: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010949\n",
      "Step 44: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.004283\n",
      "Step 44: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.007923\n",
      "Step 44: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.013898\n",
      "Step 44: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.011947\n",
      "Step 44: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.009117\n",
      "Step 44: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.004306\n",
      "Step 44: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.014654\n",
      "Step 44: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.014630\n",
      "Step 44: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.008871\n",
      "Step 44: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.003351\n",
      "Step 44: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.010797\n",
      "Step 44: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009351\n",
      "Step 44: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.010692\n",
      "Step 44: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.014659\n",
      "Step 44: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.014527\n",
      "Step 44: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.017394\n",
      "Step 44: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.005282\n",
      "Step 44: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.012311\n",
      "Step 44: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.013160\n",
      "Step 44: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011271\n",
      "Step 44: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.021106\n",
      "Step 44: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.010835\n",
      "Step 44: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.006731\n",
      "Step 44: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.010389\n",
      "Step 44: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.009545\n",
      "Step 44: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007720\n",
      "Step 44: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.008003\n",
      "Step 44: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.017552\n",
      "Step 44: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008981\n",
      "Step 44: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.011009\n",
      "Step 44: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.003957\n",
      "Step 44: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.011156\n",
      "Step 44: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010145\n",
      "Step 44: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.014423\n",
      "Step 44: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.008107\n",
      "Step 44: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.010821\n",
      "Step 44: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.013337\n",
      "Step 44: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.013213\n",
      "Step 44: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.006205\n",
      "Step 44: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009537\n",
      "Step 44: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.004076\n",
      "Step 44: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010493\n",
      "Step 44: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.007272\n",
      "Step 44: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.006382\n",
      "Step 44: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.011649\n",
      "Step 44: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.017952\n",
      "Step 44: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.010746\n",
      "Step 44: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010745\n",
      "Step 44: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.008528\n",
      "Step 44: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.011924\n",
      "Step 44: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.012455\n",
      "Step 44: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.006062\n",
      "Step 44: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.015309\n",
      "Step 44: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009926\n",
      "Step 44: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010395\n",
      "Step 44: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.006751\n",
      "Step 44: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.007466\n",
      "Step 44: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.011149\n",
      "Step 44: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.008115\n",
      "Step 44: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.014911\n",
      "Step 44: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.007702\n",
      "Step 44: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009202\n",
      "Step 44: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.006907\n",
      "Step 44: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.014433\n",
      "Step 44: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.009379\n",
      "Step 44: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.014803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.004310\n",
      "Step 44: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012724\n",
      "Step 44: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.009630\n",
      "Step 44: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.006757\n",
      "Step 44: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.014431\n",
      "Step 44: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.005125\n",
      "Step 44: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.013914\n",
      "Step 44: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.012012\n",
      "Step 44: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.013783\n",
      "Step 44: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.005563\n",
      "Step 44: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.010616\n",
      "Step 44: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.011309\n",
      "Step 44: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008217\n",
      "Step 44: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.006218\n",
      "Step 44: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.005427\n",
      "Step 44: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.009482\n",
      "Step 44: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.007960\n",
      "Step 44: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.009869\n",
      "Step 44: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.012779\n",
      "Step 44: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.005375\n",
      "Step 44: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.008463\n",
      "Step 44: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.004468\n",
      "Step 44: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.005546\n",
      "Step 44: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.012324\n",
      "Step 44: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.007541\n",
      "Step 44: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.006953\n",
      "Step 44: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.006981\n",
      "Step 44: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011993\n",
      "Step 44: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007840\n",
      "Step 44: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.012452\n",
      "Step 44: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.014502\n",
      "Step 44: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.010683\n",
      "Step 44: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.013610\n",
      "Step 44: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.015632\n",
      "Step 44: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.013780\n",
      "Step 44: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.006898\n",
      "Step 44: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.011122\n",
      "Step 44: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.009851\n",
      "Step 44: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.012530\n",
      "Step 44: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.013031\n",
      "Step 44: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.009251\n",
      "Step 44: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.012570\n",
      "Step 44: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.013085\n",
      "Step 44: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.010648\n",
      "Step 44: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.006482\n",
      "Step 44: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.011853\n",
      "Step 44: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009713\n",
      "Step 44: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.003007\n",
      "Step 44: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.011181\n",
      "Step 44: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.011632\n",
      "Step 44: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.010143\n",
      "Step 44: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.009597\n",
      "Step 44: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.011591\n",
      "Step 44: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.009716\n",
      "Step 44: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.015617\n",
      "Step 44: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.009599\n",
      "Step 44: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013765\n",
      "Step 44: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.015576\n",
      "Step 44: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.010265\n",
      "Step 44: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.010848\n",
      "time =  0:06:55.784882\n",
      "Step 45: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.002404\n",
      "Step 45: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.009995\n",
      "Step 45: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.008702\n",
      "Step 45: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.012135\n",
      "Step 45: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010872\n",
      "Step 45: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.009524\n",
      "Step 45: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.008672\n",
      "Step 45: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.005786\n",
      "Step 45: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.003479\n",
      "Step 45: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.014731\n",
      "Step 45: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.012904\n",
      "Step 45: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.004869\n",
      "Step 45: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.001764\n",
      "Step 45: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007641\n",
      "Step 45: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.011183\n",
      "Step 45: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.006616\n",
      "Step 45: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.005111\n",
      "Step 45: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.001392\n",
      "Step 45: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.012431\n",
      "Step 45: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.013142\n",
      "Step 45: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.022567\n",
      "Step 45: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010038\n",
      "Step 45: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.011909\n",
      "Step 45: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.005247\n",
      "Step 45: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008516\n",
      "Step 45: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.008684\n",
      "Step 45: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.012757\n",
      "Step 45: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.007521\n",
      "Step 45: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.013979\n",
      "Step 45: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.013756\n",
      "Step 45: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.007327\n",
      "Step 45: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.014749\n",
      "Step 45: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.013591\n",
      "Step 45: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.016288\n",
      "Step 45: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.012699\n",
      "Step 45: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.016822\n",
      "Step 45: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.008620\n",
      "Step 45: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.015542\n",
      "Step 45: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.014324\n",
      "Step 45: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.012520\n",
      "Step 45: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.010443\n",
      "Step 45: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.012796\n",
      "Step 45: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.003917\n",
      "Step 45: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.011199\n",
      "Step 45: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.012880\n",
      "Step 45: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010395\n",
      "Step 45: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.011306\n",
      "Step 45: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.008334\n",
      "Step 45: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.007305\n",
      "Step 45: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.008188\n",
      "Step 45: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.003218\n",
      "Step 45: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.008878\n",
      "Step 45: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.006900\n",
      "Step 45: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010710\n",
      "Step 45: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.007327\n",
      "Step 45: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.016237\n",
      "Step 45: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.010596\n",
      "Step 45: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.007005\n",
      "Step 45: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008185\n",
      "Step 45: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.007426\n",
      "Step 45: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009677\n",
      "Step 45: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.007553\n",
      "Step 45: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010040\n",
      "Step 45: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009369\n",
      "Step 45: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.020009\n",
      "Step 45: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.007595\n",
      "Step 45: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.009411\n",
      "Step 45: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.006949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.008310\n",
      "Step 45: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.007970\n",
      "Step 45: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.006098\n",
      "Step 45: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.005327\n",
      "Step 45: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.009472\n",
      "Step 45: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009619\n",
      "Step 45: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011015\n",
      "Step 45: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011120\n",
      "Step 45: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.015039\n",
      "Step 45: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009381\n",
      "Step 45: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.009173\n",
      "Step 45: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.009899\n",
      "Step 45: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011004\n",
      "Step 45: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.004803\n",
      "Step 45: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011842\n",
      "Step 45: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.008005\n",
      "Step 45: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.009511\n",
      "Step 45: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.007250\n",
      "Step 45: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.005478\n",
      "Step 45: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.004297\n",
      "Step 45: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007144\n",
      "Step 45: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.015496\n",
      "Step 45: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.006059\n",
      "Step 45: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.012833\n",
      "Step 45: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.005876\n",
      "Step 45: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.011595\n",
      "Step 45: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.008341\n",
      "Step 45: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.004680\n",
      "Step 45: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010598\n",
      "Step 45: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007486\n",
      "Step 45: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.006822\n",
      "Step 45: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.008964\n",
      "Step 45: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.000770\n",
      "Step 45: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.005034\n",
      "Step 45: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.005940\n",
      "Step 45: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.009667\n",
      "Step 45: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.013702\n",
      "Step 45: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.016138\n",
      "Step 45: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007065\n",
      "Step 45: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.006480\n",
      "Step 45: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009039\n",
      "Step 45: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.014380\n",
      "Step 45: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.003177\n",
      "Step 45: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.017378\n",
      "Step 45: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.009852\n",
      "Step 45: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009078\n",
      "Step 45: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.011571\n",
      "Step 45: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.014223\n",
      "Step 45: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.005760\n",
      "Step 45: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.009414\n",
      "Step 45: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.009886\n",
      "Step 45: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.012305\n",
      "Step 45: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.015733\n",
      "Step 45: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.005180\n",
      "Step 45: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.011279\n",
      "Step 45: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.013096\n",
      "Step 45: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.012895\n",
      "Step 45: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.010632\n",
      "Step 45: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.005692\n",
      "Step 45: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.004354\n",
      "Step 45: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.015580\n",
      "Step 45: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.009540\n",
      "Step 45: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.010563\n",
      "Step 45: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.001898\n",
      "Step 45: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.011339\n",
      "Step 45: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009324\n",
      "Step 45: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.010444\n",
      "Step 45: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.009480\n",
      "Step 45: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.002685\n",
      "Step 45: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.008682\n",
      "Step 45: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.010010\n",
      "Step 45: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.009494\n",
      "Step 45: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.006535\n",
      "Step 45: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.011990\n",
      "Step 45: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008141\n",
      "Step 45: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008839\n",
      "Step 45: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.011996\n",
      "Step 45: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011355\n",
      "Step 45: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009334\n",
      "time =  0:06:57.842234\n",
      "Step 46: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.013925\n",
      "Step 46: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.013966\n",
      "Step 46: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.007145\n",
      "Step 46: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.001634\n",
      "Step 46: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.012229\n",
      "Step 46: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008227\n",
      "Step 46: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.008948\n",
      "Step 46: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.002872\n",
      "Step 46: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010594\n",
      "Step 46: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.018442\n",
      "Step 46: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.005931\n",
      "Step 46: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.008669\n",
      "Step 46: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.014627\n",
      "Step 46: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007294\n",
      "Step 46: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.015732\n",
      "Step 46: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.006645\n",
      "Step 46: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.002082\n",
      "Step 46: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009214\n",
      "Step 46: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.012381\n",
      "Step 46: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.007439\n",
      "Step 46: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.005739\n",
      "Step 46: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010114\n",
      "Step 46: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.011393\n",
      "Step 46: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008905\n",
      "Step 46: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.006425\n",
      "Step 46: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.019138\n",
      "Step 46: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008648\n",
      "Step 46: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.005381\n",
      "Step 46: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.014484\n",
      "Step 46: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.014904\n",
      "Step 46: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.013187\n",
      "Step 46: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.004808\n",
      "Step 46: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.012266\n",
      "Step 46: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.009562\n",
      "Step 46: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009327\n",
      "Step 46: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.013243\n",
      "Step 46: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.016975\n",
      "Step 46: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.008754\n",
      "Step 46: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010461\n",
      "Step 46: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.007354\n",
      "Step 46: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.004213\n",
      "Step 46: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.004787\n",
      "Step 46: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.008457\n",
      "Step 46: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.008136\n",
      "Step 46: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009156\n",
      "Step 46: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010886\n",
      "Step 46: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009166\n",
      "Step 46: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.012520\n",
      "Step 46: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.004163\n",
      "Step 46: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.009117\n",
      "Step 46: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.002760\n",
      "Step 46: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.008193\n",
      "Step 46: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.011174\n",
      "Step 46: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.007437\n",
      "Step 46: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.007439\n",
      "Step 46: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.009680\n",
      "Step 46: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010251\n",
      "Step 46: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010129\n",
      "Step 46: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.005658\n",
      "Step 46: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.005550\n",
      "Step 46: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008827\n",
      "Step 46: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.009663\n",
      "Step 46: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.006832\n",
      "Step 46: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.011169\n",
      "Step 46: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.011268\n",
      "Step 46: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010418\n",
      "Step 46: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.002681\n",
      "Step 46: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.007715\n",
      "Step 46: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009745\n",
      "Step 46: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.013651\n",
      "Step 46: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.007329\n",
      "Step 46: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.014084\n",
      "Step 46: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.008249\n",
      "Step 46: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011392\n",
      "Step 46: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011935\n",
      "Step 46: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.017029\n",
      "Step 46: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009866\n",
      "Step 46: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008865\n",
      "Step 46: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.004631\n",
      "Step 46: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.011070\n",
      "Step 46: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.007711\n",
      "Step 46: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009023\n",
      "Step 46: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013211\n",
      "Step 46: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005054\n",
      "Step 46: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.013020\n",
      "Step 46: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.017522\n",
      "Step 46: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.005983\n",
      "Step 46: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.008092\n",
      "Step 46: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.006881\n",
      "Step 46: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.008559\n",
      "Step 46: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.004349\n",
      "Step 46: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.006410\n",
      "Step 46: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.014869\n",
      "Step 46: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.009427\n",
      "Step 46: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.017044\n",
      "Step 46: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.010108\n",
      "Step 46: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.004618\n",
      "Step 46: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.012572\n",
      "Step 46: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.004713\n",
      "Step 46: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.006991\n",
      "Step 46: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.016715\n",
      "Step 46: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.005596\n",
      "Step 46: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.009192\n",
      "Step 46: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.012180\n",
      "Step 46: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.002372\n",
      "Step 46: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007592\n",
      "Step 46: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.008745\n",
      "Step 46: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.008065\n",
      "Step 46: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011350\n",
      "Step 46: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.012899\n",
      "Step 46: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008995\n",
      "Step 46: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.006898\n",
      "Step 46: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.008661\n",
      "Step 46: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.010254\n",
      "Step 46: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007456\n",
      "Step 46: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.018351\n",
      "Step 46: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.004056\n",
      "Step 46: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.015692\n",
      "Step 46: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.011442\n",
      "Step 46: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.008443\n",
      "Step 46: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.006021\n",
      "Step 46: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.009122\n",
      "Step 46: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.014804\n",
      "Step 46: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.006209\n",
      "Step 46: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.013510\n",
      "Step 46: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.009813\n",
      "Step 46: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.006675\n",
      "Step 46: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.011767\n",
      "Step 46: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008622\n",
      "Step 46: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.009542\n",
      "Step 46: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.014399\n",
      "Step 46: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.007096\n",
      "Step 46: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.011871\n",
      "Step 46: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.012099\n",
      "Step 46: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010121\n",
      "Step 46: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.009665\n",
      "Step 46: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.006929\n",
      "Step 46: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.006083\n",
      "Step 46: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.006982\n",
      "Step 46: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.004704\n",
      "Step 46: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.004478\n",
      "Step 46: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.008443\n",
      "Step 46: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013789\n",
      "Step 46: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.003511\n",
      "Step 46: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.013314\n",
      "Step 46: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.005964\n",
      "time =  0:07:00.211788\n",
      "Step 47: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.009707\n",
      "Step 47: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.008247\n",
      "Step 47: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.011417\n",
      "Step 47: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.007308\n",
      "Step 47: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.010301\n",
      "Step 47: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.013676\n",
      "Step 47: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.014463\n",
      "Step 47: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.004290\n",
      "Step 47: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.008287\n",
      "Step 47: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.003759\n",
      "Step 47: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.008494\n",
      "Step 47: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.013657\n",
      "Step 47: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.005570\n",
      "Step 47: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.007786\n",
      "Step 47: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.005817\n",
      "Step 47: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.009982\n",
      "Step 47: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.005682\n",
      "Step 47: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.004314\n",
      "Step 47: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.010351\n",
      "Step 47: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.010371\n",
      "Step 47: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.013859\n",
      "Step 47: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.012721\n",
      "Step 47: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.008804\n",
      "Step 47: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.007914\n",
      "Step 47: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008231\n",
      "Step 47: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.010555\n",
      "Step 47: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008591\n",
      "Step 47: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009980\n",
      "Step 47: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009859\n",
      "Step 47: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.012620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.006710\n",
      "Step 47: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.013536\n",
      "Step 47: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009134\n",
      "Step 47: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.011650\n",
      "Step 47: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.005649\n",
      "Step 47: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.006532\n",
      "Step 47: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.011170\n",
      "Step 47: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.019109\n",
      "Step 47: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.005014\n",
      "Step 47: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.011715\n",
      "Step 47: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.011534\n",
      "Step 47: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.011127\n",
      "Step 47: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.016753\n",
      "Step 47: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.012394\n",
      "Step 47: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.007719\n",
      "Step 47: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.011750\n",
      "Step 47: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.009351\n",
      "Step 47: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.011182\n",
      "Step 47: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.004986\n",
      "Step 47: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.005711\n",
      "Step 47: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.016816\n",
      "Step 47: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.004133\n",
      "Step 47: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.011231\n",
      "Step 47: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.006167\n",
      "Step 47: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.006730\n",
      "Step 47: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.013648\n",
      "Step 47: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.011024\n",
      "Step 47: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.013583\n",
      "Step 47: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010124\n",
      "Step 47: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.009181\n",
      "Step 47: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009489\n",
      "Step 47: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.005368\n",
      "Step 47: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.010763\n",
      "Step 47: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.014874\n",
      "Step 47: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.011807\n",
      "Step 47: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.009538\n",
      "Step 47: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.007828\n",
      "Step 47: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.011468\n",
      "Step 47: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010126\n",
      "Step 47: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.004818\n",
      "Step 47: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.008147\n",
      "Step 47: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.008547\n",
      "Step 47: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.006858\n",
      "Step 47: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.015755\n",
      "Step 47: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.011442\n",
      "Step 47: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.013885\n",
      "Step 47: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.008460\n",
      "Step 47: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.015048\n",
      "Step 47: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.007778\n",
      "Step 47: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.014454\n",
      "Step 47: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009868\n",
      "Step 47: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.005476\n",
      "Step 47: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.011700\n",
      "Step 47: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.001765\n",
      "Step 47: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.010636\n",
      "Step 47: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.012938\n",
      "Step 47: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.006443\n",
      "Step 47: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.011202\n",
      "Step 47: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.008998\n",
      "Step 47: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.013201\n",
      "Step 47: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.010942\n",
      "Step 47: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.008215\n",
      "Step 47: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.007465\n",
      "Step 47: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.003920\n",
      "Step 47: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.007712\n",
      "Step 47: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.010633\n",
      "Step 47: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.012858\n",
      "Step 47: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007335\n",
      "Step 47: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.008245\n",
      "Step 47: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.011682\n",
      "Step 47: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010500\n",
      "Step 47: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.008854\n",
      "Step 47: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.014110\n",
      "Step 47: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.009048\n",
      "Step 47: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.006591\n",
      "Step 47: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008436\n",
      "Step 47: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.012639\n",
      "Step 47: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.010807\n",
      "Step 47: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.013509\n",
      "Step 47: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.008142\n",
      "Step 47: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.006337\n",
      "Step 47: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.013327\n",
      "Step 47: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.011009\n",
      "Step 47: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.009632\n",
      "Step 47: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.009806\n",
      "Step 47: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.006734\n",
      "Step 47: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.011894\n",
      "Step 47: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.008255\n",
      "Step 47: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.013222\n",
      "Step 47: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.006353\n",
      "Step 47: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.009188\n",
      "Step 47: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.009948\n",
      "Step 47: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.007582\n",
      "Step 47: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.017092\n",
      "Step 47: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.014321\n",
      "Step 47: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.013083\n",
      "Step 47: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.004717\n",
      "Step 47: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011352\n",
      "Step 47: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.009338\n",
      "Step 47: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.014906\n",
      "Step 47: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.011993\n",
      "Step 47: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.009031\n",
      "Step 47: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.006397\n",
      "Step 47: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.004201\n",
      "Step 47: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.011673\n",
      "Step 47: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.010819\n",
      "Step 47: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.013005\n",
      "Step 47: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.008460\n",
      "Step 47: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.011624\n",
      "Step 47: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.011581\n",
      "Step 47: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.009660\n",
      "Step 47: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.007005\n",
      "Step 47: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.007104\n",
      "Step 47: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.013201\n",
      "Step 47: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.010194\n",
      "Step 47: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.015627\n",
      "Step 47: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.003601\n",
      "time =  0:07:00.665763\n",
      "Step 48: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.011083\n",
      "Step 48: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.013959\n",
      "Step 48: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.006611\n",
      "Step 48: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.016989\n",
      "Step 48: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.011467\n",
      "Step 48: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.008673\n",
      "Step 48: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.008013\n",
      "Step 48: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.013914\n",
      "Step 48: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.013526\n",
      "Step 48: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.006197\n",
      "Step 48: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.013625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005766\n",
      "Step 48: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009377\n",
      "Step 48: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.006227\n",
      "Step 48: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.009920\n",
      "Step 48: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.007467\n",
      "Step 48: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.012103\n",
      "Step 48: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.007023\n",
      "Step 48: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.009639\n",
      "Step 48: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.009357\n",
      "Step 48: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.013657\n",
      "Step 48: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.007263\n",
      "Step 48: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.004979\n",
      "Step 48: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.012094\n",
      "Step 48: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.012989\n",
      "Step 48: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.010637\n",
      "Step 48: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.016814\n",
      "Step 48: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009140\n",
      "Step 48: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.003656\n",
      "Step 48: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010173\n",
      "Step 48: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.011642\n",
      "Step 48: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.013181\n",
      "Step 48: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.003437\n",
      "Step 48: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.007354\n",
      "Step 48: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009627\n",
      "Step 48: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.014991\n",
      "Step 48: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.015408\n",
      "Step 48: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.010622\n",
      "Step 48: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.007737\n",
      "Step 48: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.005343\n",
      "Step 48: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.010763\n",
      "Step 48: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.013992\n",
      "Step 48: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.008684\n",
      "Step 48: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.014970\n",
      "Step 48: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008853\n",
      "Step 48: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.010800\n",
      "Step 48: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007801\n",
      "Step 48: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.012322\n",
      "Step 48: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.009376\n",
      "Step 48: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.004992\n",
      "Step 48: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.008695\n",
      "Step 48: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.010725\n",
      "Step 48: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.009177\n",
      "Step 48: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.015471\n",
      "Step 48: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.008267\n",
      "Step 48: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.012510\n",
      "Step 48: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.011559\n",
      "Step 48: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.009342\n",
      "Step 48: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.014392\n",
      "Step 48: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.014626\n",
      "Step 48: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009230\n",
      "Step 48: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.008228\n",
      "Step 48: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012551\n",
      "Step 48: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.006904\n",
      "Step 48: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.012312\n",
      "Step 48: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.010377\n",
      "Step 48: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.010328\n",
      "Step 48: Minibatch_NU: 6700/14680 Minibatch Loss: nan  0.997570\n",
      "Step 48: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.012389\n",
      "Step 48: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.003674\n",
      "Step 48: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.012640\n",
      "Step 48: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.011974\n",
      "Step 48: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.007698\n",
      "Step 48: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.009895\n",
      "Step 48: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.009287\n",
      "Step 48: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.011505\n",
      "Step 48: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.012570\n",
      "Step 48: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.012986\n",
      "Step 48: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.008836\n",
      "Step 48: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.006785\n",
      "Step 48: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.005634\n",
      "Step 48: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.006859\n",
      "Step 48: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.009564\n",
      "Step 48: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.010392\n",
      "Step 48: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.005311\n",
      "Step 48: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.012824\n",
      "Step 48: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.011940\n",
      "Step 48: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.006518\n",
      "Step 48: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.012828\n",
      "Step 48: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.017583\n",
      "Step 48: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.014383\n",
      "Step 48: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.008045\n",
      "Step 48: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.007343\n",
      "Step 48: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.014445\n",
      "Step 48: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.010782\n",
      "Step 48: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.008775\n",
      "Step 48: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.014180\n",
      "Step 48: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.005856\n",
      "Step 48: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.014089\n",
      "Step 48: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.006104\n",
      "Step 48: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.010847\n",
      "Step 48: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.008346\n",
      "Step 48: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.010627\n",
      "Step 48: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.012109\n",
      "Step 48: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.007882\n",
      "Step 48: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.008569\n",
      "Step 48: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.014074\n",
      "Step 48: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.006087\n",
      "Step 48: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.017744\n",
      "Step 48: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011210\n",
      "Step 48: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.013117\n",
      "Step 48: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.008980\n",
      "Step 48: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.010347\n",
      "Step 48: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.008361\n",
      "Step 48: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.013318\n",
      "Step 48: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007263\n",
      "Step 48: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.009617\n",
      "Step 48: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.013321\n",
      "Step 48: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.006108\n",
      "Step 48: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.008986\n",
      "Step 48: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.011271\n",
      "Step 48: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.015187\n",
      "Step 48: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.010323\n",
      "Step 48: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.003351\n",
      "Step 48: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.012058\n",
      "Step 48: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.009676\n",
      "Step 48: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.004275\n",
      "Step 48: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.011993\n",
      "Step 48: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.012066\n",
      "Step 48: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.011130\n",
      "Step 48: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.014599\n",
      "Step 48: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.011910\n",
      "Step 48: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.006330\n",
      "Step 48: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.011100\n",
      "Step 48: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.016036\n",
      "Step 48: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.015406\n",
      "Step 48: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010365\n",
      "Step 48: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.010908\n",
      "Step 48: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.019765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.009772\n",
      "Step 48: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.007187\n",
      "Step 48: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.012048\n",
      "Step 48: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.007017\n",
      "Step 48: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.011849\n",
      "Step 48: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.007559\n",
      "Step 48: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011645\n",
      "Step 48: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.009698\n",
      "time =  0:07:01.337907\n",
      "Step 49: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.000388\n",
      "Step 49: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.007557\n",
      "Step 49: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.010310\n",
      "Step 49: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.010626\n",
      "Step 49: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.007799\n",
      "Step 49: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.010369\n",
      "Step 49: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.008197\n",
      "Step 49: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.009261\n",
      "Step 49: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.010689\n",
      "Step 49: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.009561\n",
      "Step 49: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.012285\n",
      "Step 49: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.015234\n",
      "Step 49: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.009671\n",
      "Step 49: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.006684\n",
      "Step 49: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.005543\n",
      "Step 49: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.008989\n",
      "Step 49: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.002530\n",
      "Step 49: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.010307\n",
      "Step 49: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.011721\n",
      "Step 49: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.009109\n",
      "Step 49: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.015534\n",
      "Step 49: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.003676\n",
      "Step 49: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009796\n",
      "Step 49: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.008703\n",
      "Step 49: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.009064\n",
      "Step 49: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.007082\n",
      "Step 49: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.008473\n",
      "Step 49: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.013057\n",
      "Step 49: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.005022\n",
      "Step 49: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.014032\n",
      "Step 49: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.004076\n",
      "Step 49: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.010359\n",
      "Step 49: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.009745\n",
      "Step 49: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.011382\n",
      "Step 49: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009608\n",
      "Step 49: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.011208\n",
      "Step 49: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.011006\n",
      "Step 49: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.004934\n",
      "Step 49: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.009152\n",
      "Step 49: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.008683\n",
      "Step 49: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.007026\n",
      "Step 49: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.014070\n",
      "Step 49: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.013791\n",
      "Step 49: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.007365\n",
      "Step 49: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.009081\n",
      "Step 49: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.011644\n",
      "Step 49: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.007840\n",
      "Step 49: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.014041\n",
      "Step 49: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.012667\n",
      "Step 49: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.007966\n",
      "Step 49: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.015345\n",
      "Step 49: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.009016\n",
      "Step 49: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.010330\n",
      "Step 49: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.010836\n",
      "Step 49: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.011396\n",
      "Step 49: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.010174\n",
      "Step 49: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.015473\n",
      "Step 49: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.010112\n",
      "Step 49: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.008466\n",
      "Step 49: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.015650\n",
      "Step 49: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.009020\n",
      "Step 49: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.005808\n",
      "Step 49: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.012172\n",
      "Step 49: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.006402\n",
      "Step 49: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.010381\n",
      "Step 49: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.005454\n",
      "Step 49: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.006243\n",
      "Step 49: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.008043\n",
      "Step 49: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.011169\n",
      "Step 49: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.006421\n",
      "Step 49: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.008299\n",
      "Step 49: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.007179\n",
      "Step 49: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.008012\n",
      "Step 49: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.013117\n",
      "Step 49: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.005121\n",
      "Step 49: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.010034\n",
      "Step 49: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009862\n",
      "Step 49: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.009755\n",
      "Step 49: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.004298\n",
      "Step 49: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.014835\n",
      "Step 49: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.005765\n",
      "Step 49: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.008735\n",
      "Step 49: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.014084\n",
      "Step 49: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.014063\n",
      "Step 49: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.013345\n",
      "Step 49: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.011169\n",
      "Step 49: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.009809\n",
      "Step 49: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.010823\n",
      "Step 49: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.003727\n",
      "Step 49: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.004818\n",
      "Step 49: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.009647\n",
      "Step 49: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.007728\n",
      "Step 49: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.005444\n",
      "Step 49: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.003832\n",
      "Step 49: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.006523\n",
      "Step 49: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.005579\n",
      "Step 49: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.011823\n",
      "Step 49: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.004955\n",
      "Step 49: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.007634\n",
      "Step 49: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.011817\n",
      "Step 49: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.012435\n",
      "Step 49: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.012459\n",
      "Step 49: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.009007\n",
      "Step 49: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.011216\n",
      "Step 49: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.007167\n",
      "Step 49: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.010852\n",
      "Step 49: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.008311\n",
      "Step 49: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009371\n",
      "Step 49: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.009357\n",
      "Step 49: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.012697\n",
      "Step 49: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.007649\n",
      "Step 49: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.007082\n",
      "Step 49: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.005556\n",
      "Step 49: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.005399\n",
      "Step 49: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.001362\n",
      "Step 49: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.011078\n",
      "Step 49: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.011018\n",
      "Step 49: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.012008\n",
      "Step 49: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.011195\n",
      "Step 49: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.006422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.006023\n",
      "Step 49: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.011691\n",
      "Step 49: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.004177\n",
      "Step 49: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.015658\n",
      "Step 49: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.008054\n",
      "Step 49: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.010921\n",
      "Step 49: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.014912\n",
      "Step 49: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.012226\n",
      "Step 49: Minibatch_NU: 12800/14680 Minibatch Loss: nan  0.997911\n",
      "Step 49: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.008444\n",
      "Step 49: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.010647\n",
      "Step 49: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.008773\n",
      "Step 49: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.005811\n",
      "Step 49: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.011047\n",
      "Step 49: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.008733\n",
      "Step 49: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.007390\n",
      "Step 49: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.008239\n",
      "Step 49: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.002268\n",
      "Step 49: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.004851\n",
      "Step 49: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.004077\n",
      "Step 49: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.010489\n",
      "Step 49: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.010030\n",
      "Step 49: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.007272\n",
      "Step 49: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.015412\n",
      "Step 49: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.007690\n",
      "Step 49: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.011774\n",
      "Step 49: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.012703\n",
      "time =  0:07:01.129369\n",
      "Step 50: Minibatch_NU: 1/14680 Minibatch Loss: nan  1.010996\n",
      "Step 50: Minibatch_NU: 100/14680 Minibatch Loss: nan  1.010378\n",
      "Step 50: Minibatch_NU: 200/14680 Minibatch Loss: nan  1.008361\n",
      "Step 50: Minibatch_NU: 300/14680 Minibatch Loss: nan  1.008605\n",
      "Step 50: Minibatch_NU: 400/14680 Minibatch Loss: nan  1.008388\n",
      "Step 50: Minibatch_NU: 500/14680 Minibatch Loss: nan  1.013268\n",
      "Step 50: Minibatch_NU: 600/14680 Minibatch Loss: nan  1.010424\n",
      "Step 50: Minibatch_NU: 700/14680 Minibatch Loss: nan  1.008380\n",
      "Step 50: Minibatch_NU: 800/14680 Minibatch Loss: nan  1.004080\n",
      "Step 50: Minibatch_NU: 900/14680 Minibatch Loss: nan  1.008281\n",
      "Step 50: Minibatch_NU: 1000/14680 Minibatch Loss: nan  1.004143\n",
      "Step 50: Minibatch_NU: 1100/14680 Minibatch Loss: nan  1.005659\n",
      "Step 50: Minibatch_NU: 1200/14680 Minibatch Loss: nan  1.012117\n",
      "Step 50: Minibatch_NU: 1300/14680 Minibatch Loss: nan  1.008477\n",
      "Step 50: Minibatch_NU: 1400/14680 Minibatch Loss: nan  1.006218\n",
      "Step 50: Minibatch_NU: 1500/14680 Minibatch Loss: nan  1.004923\n",
      "Step 50: Minibatch_NU: 1600/14680 Minibatch Loss: nan  1.008131\n",
      "Step 50: Minibatch_NU: 1700/14680 Minibatch Loss: nan  1.009520\n",
      "Step 50: Minibatch_NU: 1800/14680 Minibatch Loss: nan  1.011270\n",
      "Step 50: Minibatch_NU: 1900/14680 Minibatch Loss: nan  1.008727\n",
      "Step 50: Minibatch_NU: 2000/14680 Minibatch Loss: nan  1.010808\n",
      "Step 50: Minibatch_NU: 2100/14680 Minibatch Loss: nan  1.010211\n",
      "Step 50: Minibatch_NU: 2200/14680 Minibatch Loss: nan  1.009394\n",
      "Step 50: Minibatch_NU: 2300/14680 Minibatch Loss: nan  1.009809\n",
      "Step 50: Minibatch_NU: 2400/14680 Minibatch Loss: nan  1.008959\n",
      "Step 50: Minibatch_NU: 2500/14680 Minibatch Loss: nan  1.004209\n",
      "Step 50: Minibatch_NU: 2600/14680 Minibatch Loss: nan  1.015606\n",
      "Step 50: Minibatch_NU: 2700/14680 Minibatch Loss: nan  1.009799\n",
      "Step 50: Minibatch_NU: 2800/14680 Minibatch Loss: nan  1.009944\n",
      "Step 50: Minibatch_NU: 2900/14680 Minibatch Loss: nan  1.010975\n",
      "Step 50: Minibatch_NU: 3000/14680 Minibatch Loss: nan  1.006755\n",
      "Step 50: Minibatch_NU: 3100/14680 Minibatch Loss: nan  1.009607\n",
      "Step 50: Minibatch_NU: 3200/14680 Minibatch Loss: nan  1.011743\n",
      "Step 50: Minibatch_NU: 3300/14680 Minibatch Loss: nan  1.004249\n",
      "Step 50: Minibatch_NU: 3400/14680 Minibatch Loss: nan  1.009642\n",
      "Step 50: Minibatch_NU: 3500/14680 Minibatch Loss: nan  1.002305\n",
      "Step 50: Minibatch_NU: 3600/14680 Minibatch Loss: nan  1.009659\n",
      "Step 50: Minibatch_NU: 3700/14680 Minibatch Loss: nan  1.008904\n",
      "Step 50: Minibatch_NU: 3800/14680 Minibatch Loss: nan  1.010080\n",
      "Step 50: Minibatch_NU: 3900/14680 Minibatch Loss: nan  1.009102\n",
      "Step 50: Minibatch_NU: 4000/14680 Minibatch Loss: nan  1.014689\n",
      "Step 50: Minibatch_NU: 4100/14680 Minibatch Loss: nan  1.011481\n",
      "Step 50: Minibatch_NU: 4200/14680 Minibatch Loss: nan  1.005211\n",
      "Step 50: Minibatch_NU: 4300/14680 Minibatch Loss: nan  1.012350\n",
      "Step 50: Minibatch_NU: 4400/14680 Minibatch Loss: nan  1.008016\n",
      "Step 50: Minibatch_NU: 4500/14680 Minibatch Loss: nan  1.003846\n",
      "Step 50: Minibatch_NU: 4600/14680 Minibatch Loss: nan  1.015388\n",
      "Step 50: Minibatch_NU: 4700/14680 Minibatch Loss: nan  1.010660\n",
      "Step 50: Minibatch_NU: 4800/14680 Minibatch Loss: nan  1.008418\n",
      "Step 50: Minibatch_NU: 4900/14680 Minibatch Loss: nan  1.011901\n",
      "Step 50: Minibatch_NU: 5000/14680 Minibatch Loss: nan  1.008183\n",
      "Step 50: Minibatch_NU: 5100/14680 Minibatch Loss: nan  1.015640\n",
      "Step 50: Minibatch_NU: 5200/14680 Minibatch Loss: nan  1.012112\n",
      "Step 50: Minibatch_NU: 5300/14680 Minibatch Loss: nan  1.015131\n",
      "Step 50: Minibatch_NU: 5400/14680 Minibatch Loss: nan  1.008218\n",
      "Step 50: Minibatch_NU: 5500/14680 Minibatch Loss: nan  1.002759\n",
      "Step 50: Minibatch_NU: 5600/14680 Minibatch Loss: nan  1.004984\n",
      "Step 50: Minibatch_NU: 5700/14680 Minibatch Loss: nan  1.006241\n",
      "Step 50: Minibatch_NU: 5800/14680 Minibatch Loss: nan  1.010587\n",
      "Step 50: Minibatch_NU: 5900/14680 Minibatch Loss: nan  1.015064\n",
      "Step 50: Minibatch_NU: 6000/14680 Minibatch Loss: nan  1.006961\n",
      "Step 50: Minibatch_NU: 6100/14680 Minibatch Loss: nan  1.011524\n",
      "Step 50: Minibatch_NU: 6200/14680 Minibatch Loss: nan  1.014605\n",
      "Step 50: Minibatch_NU: 6300/14680 Minibatch Loss: nan  1.009011\n",
      "Step 50: Minibatch_NU: 6400/14680 Minibatch Loss: nan  1.010433\n",
      "Step 50: Minibatch_NU: 6500/14680 Minibatch Loss: nan  1.014084\n",
      "Step 50: Minibatch_NU: 6600/14680 Minibatch Loss: nan  1.012238\n",
      "Step 50: Minibatch_NU: 6700/14680 Minibatch Loss: nan  1.014261\n",
      "Step 50: Minibatch_NU: 6800/14680 Minibatch Loss: nan  1.010567\n",
      "Step 50: Minibatch_NU: 6900/14680 Minibatch Loss: nan  1.009221\n",
      "Step 50: Minibatch_NU: 7000/14680 Minibatch Loss: nan  1.012701\n",
      "Step 50: Minibatch_NU: 7100/14680 Minibatch Loss: nan  1.018946\n",
      "Step 50: Minibatch_NU: 7200/14680 Minibatch Loss: nan  1.010320\n",
      "Step 50: Minibatch_NU: 7300/14680 Minibatch Loss: nan  1.011072\n",
      "Step 50: Minibatch_NU: 7400/14680 Minibatch Loss: nan  1.006220\n",
      "Step 50: Minibatch_NU: 7500/14680 Minibatch Loss: nan  1.007212\n",
      "Step 50: Minibatch_NU: 7600/14680 Minibatch Loss: nan  1.009382\n",
      "Step 50: Minibatch_NU: 7700/14680 Minibatch Loss: nan  1.008693\n",
      "Step 50: Minibatch_NU: 7800/14680 Minibatch Loss: nan  1.007363\n",
      "Step 50: Minibatch_NU: 7900/14680 Minibatch Loss: nan  1.010614\n",
      "Step 50: Minibatch_NU: 8000/14680 Minibatch Loss: nan  1.009646\n",
      "Step 50: Minibatch_NU: 8100/14680 Minibatch Loss: nan  1.018388\n",
      "Step 50: Minibatch_NU: 8200/14680 Minibatch Loss: nan  1.006925\n",
      "Step 50: Minibatch_NU: 8300/14680 Minibatch Loss: nan  1.013866\n",
      "Step 50: Minibatch_NU: 8400/14680 Minibatch Loss: nan  1.010413\n",
      "Step 50: Minibatch_NU: 8500/14680 Minibatch Loss: nan  1.009666\n",
      "Step 50: Minibatch_NU: 8600/14680 Minibatch Loss: nan  1.008286\n",
      "Step 50: Minibatch_NU: 8700/14680 Minibatch Loss: nan  1.006262\n",
      "Step 50: Minibatch_NU: 8800/14680 Minibatch Loss: nan  1.007512\n",
      "Step 50: Minibatch_NU: 8900/14680 Minibatch Loss: nan  1.012825\n",
      "Step 50: Minibatch_NU: 9000/14680 Minibatch Loss: nan  1.008971\n",
      "Step 50: Minibatch_NU: 9100/14680 Minibatch Loss: nan  1.012361\n",
      "Step 50: Minibatch_NU: 9200/14680 Minibatch Loss: nan  1.008207\n",
      "Step 50: Minibatch_NU: 9300/14680 Minibatch Loss: nan  1.007618\n",
      "Step 50: Minibatch_NU: 9400/14680 Minibatch Loss: nan  1.009642\n",
      "Step 50: Minibatch_NU: 9500/14680 Minibatch Loss: nan  1.009939\n",
      "Step 50: Minibatch_NU: 9600/14680 Minibatch Loss: nan  1.008286\n",
      "Step 50: Minibatch_NU: 9700/14680 Minibatch Loss: nan  1.007082\n",
      "Step 50: Minibatch_NU: 9800/14680 Minibatch Loss: nan  1.006056\n",
      "Step 50: Minibatch_NU: 9900/14680 Minibatch Loss: nan  1.011094\n",
      "Step 50: Minibatch_NU: 10000/14680 Minibatch Loss: nan  1.012521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Minibatch_NU: 10100/14680 Minibatch Loss: nan  1.006897\n",
      "Step 50: Minibatch_NU: 10200/14680 Minibatch Loss: nan  1.010048\n",
      "Step 50: Minibatch_NU: 10300/14680 Minibatch Loss: nan  1.016030\n",
      "Step 50: Minibatch_NU: 10400/14680 Minibatch Loss: nan  1.006681\n",
      "Step 50: Minibatch_NU: 10500/14680 Minibatch Loss: nan  1.016446\n",
      "Step 50: Minibatch_NU: 10600/14680 Minibatch Loss: nan  1.007428\n",
      "Step 50: Minibatch_NU: 10700/14680 Minibatch Loss: nan  1.009955\n",
      "Step 50: Minibatch_NU: 10800/14680 Minibatch Loss: nan  1.013275\n",
      "Step 50: Minibatch_NU: 10900/14680 Minibatch Loss: nan  1.011231\n",
      "Step 50: Minibatch_NU: 11000/14680 Minibatch Loss: nan  1.008609\n",
      "Step 50: Minibatch_NU: 11100/14680 Minibatch Loss: nan  1.006246\n",
      "Step 50: Minibatch_NU: 11200/14680 Minibatch Loss: nan  1.009018\n",
      "Step 50: Minibatch_NU: 11300/14680 Minibatch Loss: nan  1.016761\n",
      "Step 50: Minibatch_NU: 11400/14680 Minibatch Loss: nan  1.007497\n",
      "Step 50: Minibatch_NU: 11500/14680 Minibatch Loss: nan  1.007728\n",
      "Step 50: Minibatch_NU: 11600/14680 Minibatch Loss: nan  1.017731\n",
      "Step 50: Minibatch_NU: 11700/14680 Minibatch Loss: nan  1.007847\n",
      "Step 50: Minibatch_NU: 11800/14680 Minibatch Loss: nan  1.012515\n",
      "Step 50: Minibatch_NU: 11900/14680 Minibatch Loss: nan  1.008689\n",
      "Step 50: Minibatch_NU: 12000/14680 Minibatch Loss: nan  1.015465\n",
      "Step 50: Minibatch_NU: 12100/14680 Minibatch Loss: nan  1.009131\n",
      "Step 50: Minibatch_NU: 12200/14680 Minibatch Loss: nan  1.005121\n",
      "Step 50: Minibatch_NU: 12300/14680 Minibatch Loss: nan  1.020341\n",
      "Step 50: Minibatch_NU: 12400/14680 Minibatch Loss: nan  1.015495\n",
      "Step 50: Minibatch_NU: 12500/14680 Minibatch Loss: nan  1.006003\n",
      "Step 50: Minibatch_NU: 12600/14680 Minibatch Loss: nan  1.009189\n",
      "Step 50: Minibatch_NU: 12700/14680 Minibatch Loss: nan  1.014479\n",
      "Step 50: Minibatch_NU: 12800/14680 Minibatch Loss: nan  1.006073\n",
      "Step 50: Minibatch_NU: 12900/14680 Minibatch Loss: nan  1.005328\n",
      "Step 50: Minibatch_NU: 13000/14680 Minibatch Loss: nan  1.008879\n",
      "Step 50: Minibatch_NU: 13100/14680 Minibatch Loss: nan  1.003757\n",
      "Step 50: Minibatch_NU: 13200/14680 Minibatch Loss: nan  1.002352\n",
      "Step 50: Minibatch_NU: 13300/14680 Minibatch Loss: nan  1.009453\n",
      "Step 50: Minibatch_NU: 13400/14680 Minibatch Loss: nan  1.008733\n",
      "Step 50: Minibatch_NU: 13500/14680 Minibatch Loss: nan  1.012379\n",
      "Step 50: Minibatch_NU: 13600/14680 Minibatch Loss: nan  1.010894\n",
      "Step 50: Minibatch_NU: 13700/14680 Minibatch Loss: nan  1.016147\n",
      "Step 50: Minibatch_NU: 13800/14680 Minibatch Loss: nan  1.012806\n",
      "Step 50: Minibatch_NU: 13900/14680 Minibatch Loss: nan  1.010860\n",
      "Step 50: Minibatch_NU: 14000/14680 Minibatch Loss: nan  1.017999\n",
      "Step 50: Minibatch_NU: 14100/14680 Minibatch Loss: nan  1.009871\n",
      "Step 50: Minibatch_NU: 14200/14680 Minibatch Loss: nan  1.011074\n",
      "Step 50: Minibatch_NU: 14300/14680 Minibatch Loss: nan  1.008663\n",
      "Step 50: Minibatch_NU: 14400/14680 Minibatch Loss: nan  1.009234\n",
      "Step 50: Minibatch_NU: 14500/14680 Minibatch Loss: nan  1.008675\n",
      "Step 50: Minibatch_NU: 14600/14680 Minibatch Loss: nan  1.014579\n",
      "time =  0:07:00.368115\n"
     ]
    }
   ],
   "source": [
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_label_addr = train_label_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 128\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 50\n",
    "# q = next(dg_train_ae())\n",
    "# print(q)\n",
    "# print(type(q))\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "num_steps = 30000\n",
    "display_step = 100\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 512 # 1st layer num features\n",
    "num_hidden_2 = 512 # 2nd layer num features \n",
    "num_hidden_3 = 64 # 3nd layer num features (the latent dim)\n",
    "num_input = 96*(window_size*2+1) # MNIST data input (img shape: 28*28)\n",
    "\n",
    "num_hidden_1_vad = 96*(window_size*2+1)\n",
    "num_hidden_2_vad = 96*(window_size*2+1)\n",
    "labels_dim = 2\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "true_clean_X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "# true_clean_X = clean_X\n",
    "# output = tf.placeholder(\"float\", [None, labels_dim], name='labels')\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'encoder_h_logvar': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_3])),\n",
    "    'encoder_h_mean': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_3])),\n",
    "    \n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_3, num_hidden_2])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h_logvar': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "    'decoder_h_mean': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "    'decoder_h_output': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "    \n",
    "    'vad_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1_vad])),\n",
    "    'vad_h2': tf.Variable(tf.random_normal([num_hidden_1_vad, num_hidden_2_vad])),\n",
    "    'vad_hout': tf.Variable(tf.random_normal([num_hidden_2_vad, 2]))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'encoder_b_logvar': tf.Variable(tf.random_normal([num_hidden_3])),\n",
    "    'encoder_b_mean': tf.Variable(tf.random_normal([num_hidden_3])),\n",
    "    \n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b_logvar': tf.Variable(tf.random_normal([num_input])),\n",
    "    'decoder_b_mean': tf.Variable(tf.random_normal([num_input])),\n",
    "    'decoder_b_output': tf.Variable(tf.random_normal([num_input])),\n",
    "    \n",
    "    'vad_b1': tf.Variable(tf.random_normal([num_hidden_1_vad])),\n",
    "    'vad_b2': tf.Variable(tf.random_normal([num_hidden_2_vad])),\n",
    "    'vad_bout': tf.Variable(tf.random_normal([labels_dim]))\n",
    "}\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(X):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    \n",
    "    layer_z_logvar = tf.maximum(tf.add(tf.matmul(layer_2, weights['encoder_h_logvar']),\n",
    "                                   biases['encoder_b_logvar']),10e-9)\n",
    "    layer_z_mean = tf.add(tf.matmul(layer_2, weights['encoder_h_mean']),\n",
    "                                   biases['encoder_b_mean'])\n",
    "    z = Lambda(sampling, output_shape=(num_hidden_3,), name='z')([layer_z_mean, layer_z_logvar])\n",
    "    return z, layer_z_logvar, layer_z_mean\n",
    "    \n",
    "\n",
    "# Building the decoder\n",
    "def decoder(z):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(z, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    \n",
    "    layer_x_logvar = tf.maximum(tf.add(tf.matmul(layer_2, weights['decoder_h_logvar']),\n",
    "                                   biases['decoder_b_logvar']),10e-9)\n",
    "    layer_x_mean = tf.add(tf.matmul(layer_2, weights['decoder_h_mean']),\n",
    "                                   biases['decoder_b_mean'])\n",
    "    denoise_X = Lambda(sampling, output_shape=(num_input,), name='result_x')([layer_x_mean, layer_x_logvar])\n",
    "#     denoise_X = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['decoder_h_output']), biases['decoder_b_output']))\n",
    "    return denoise_X, layer_x_logvar ,layer_x_mean\n",
    "\n",
    "def vad(MRCG,output):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(MRCG, weights['vad_h1']),\n",
    "                                   biases['vad_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['vad_h2']),\n",
    "                                   biases['vad_b2']))\n",
    "    output = tf.nn.softmax(tf.add(tf.matmul(layer_2, weights['vad_hout']),\n",
    "                                   biases['vad_bout']))\n",
    "    return output\n",
    "\n",
    "# Construct model\n",
    "z, z_logvar, z_mean = encoder(X)\n",
    "pred_clean_X, x_logvar ,x_mean = decoder(z)\n",
    "# pred_clean_X = decoder(z)\n",
    "# pred_label = vad(denoise_X, output)\n",
    "\n",
    "# Prediction\n",
    "# y_pred = result_x\n",
    "# Targets (Labels) are the input data.\n",
    "# y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "# calculate binary cross entropy\n",
    "def binary_cross_entropy(actual, predicted):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_score += actual[i] * log(1e-15 + predicted[i])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    return -mean_sum_score\n",
    "\n",
    "def categorical_cross_entropy(actual, predicted):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_score += actual[i][j] * log(1e-15 + predicted[i][j])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    return -mean_sum_score\n",
    "\n",
    "# loss_VAD = tf.keras.losses.categorical_crossentropy(output,pred_label)    \n",
    "loss_SE =  tf.reduce_mean(K.sum(1 + z_logvar - K.square(z_mean) - K.exp(z_logvar))- K.sum(0.5*x_logvar + K.square(pred_clean_X-x_mean)/(2*K.exp(x_logvar))))\n",
    "loss_SE2 = tf.reduce_mean(tf.pow(true_clean_X - pred_clean_X, 2))\n",
    "\n",
    "# final_loss = tf.reduce_mean(loss1+loss2)\n",
    "optimizer_SE = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_SE)\n",
    "optimizer_SE2 = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_SE2)\n",
    "# optimizer_VAD = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_VAD)\n",
    "# final_train_op = tf.group([optimizer_SE, optimizer_VAD])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([X,true_clean_X])\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "# print(dataset.output_types)\n",
    "# print(dataset.output_shapes)\n",
    "data_dg_joint=dg_train_jae()\n",
    "\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1,epochs_Nu+1):\n",
    "        tic = time.clock()\n",
    "        for j in range(1,steps_per_epoch+1):\n",
    "            in_data,clean_data,labels = next(data_dg_joint)\n",
    "    #         print(in_data,'------')\n",
    "    #         print(denoise_data,'------')\n",
    "            a,b,c = sess.run([optimizer_SE2,loss_SE,loss_SE2], feed_dict={ X: in_data, true_clean_X: clean_data })\n",
    "            \n",
    "    #         print(sess.run(el).shape)\n",
    "            if j % display_step == 0 or j == 1:\n",
    "                print('Step %i: Minibatch_NU: %i/%i Minibatch Loss: %f  %f' % (i, j, steps_per_epoch, b,c))\n",
    "        toc = time.clock()\n",
    "        print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "#         saver.save(sess, 'my_test_model')\n",
    "        saver.save(sess, enhance_model_save_addr+'model_ae_'+str(i)+'.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 8 5]\n"
     ]
    }
   ],
   "source": [
    "# files = tf.matching_files(\"gs://my-bucket/train-data-*.csv\")\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(tf.shape(files)[0])\n",
    "\n",
    "print(np.asarray([3,4,5])*np.asarray([1,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    \n",
    "    for i in range(epochs): \n",
    "        session.run(iterator.initializer)\n",
    "        \n",
    "        try:\n",
    "            # Go through the entire dataset\n",
    "            while True:\n",
    "                image_batch = session.run(batch_of_images)\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('End of Epoch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:           \n",
    "    # Testing\n",
    "    # Encode and decode images from test set and visualize their reconstruction.\n",
    "    n = 4\n",
    "    canvas_orig = np.empty((28 * n, 28 * n))\n",
    "    canvas_recon = np.empty((28 * n, 28 * n))\n",
    "    for i in range(n):\n",
    "        # MNIST test set\n",
    "        batch_x, _ = mnist.test.next_batch(n)\n",
    "        # Encode and decode the digit image\n",
    "        g = sess.run(decoder_op, feed_dict={X: batch_x})\n",
    "\n",
    "        # Display original images\n",
    "        for j in range(n):\n",
    "            # Draw the original digits\n",
    "            canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = \\\n",
    "                batch_x[j].reshape([28, 28])\n",
    "        # Display reconstructed images\n",
    "        for j in range(n):\n",
    "            # Draw the reconstructed digits\n",
    "            canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = \\\n",
    "                g[j].reshape([28, 28])\n",
    "\n",
    "    print(\"Original Images\")\n",
    "    plt.figure(figsize=(n, n))\n",
    "    plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Reconstructed Images\")\n",
    "    plt.figure(figsize=(n, n))\n",
    "    plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "def squares_all_of_them():\n",
    "    x = 0\n",
    "    while True:\n",
    "        yield x * x\n",
    "        x += 1\n",
    "\n",
    "squares = squares_all_of_them()\n",
    "for _ in range(4):\n",
    "    print(next(squares))\n",
    "    \n",
    "print(next(squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 30000\n",
    "batch_size = 256\n",
    "\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_hidden_2 = 128 # 2nd layer num features (the latent dim)\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training\n",
    "    for i in range(1, num_steps+1):\n",
    "        # Prepare Data\n",
    "        # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "        # Display logs per step\n",
    "        if i % display_step == 0 or i == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (i, l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Epoch 00000000  avg train loss over 1  batches  4.4255 DKL loss 0.9190  avg train acc  0.6000\n",
      "Epoch 00000000  test loss 288.9478 DKL loss 0.9190  test acc 0.4300\n",
      "Epoch 00000001  avg train loss over 1  batches  4.4179 DKL loss 0.9171  avg train acc  0.6000\n",
      "Epoch 00000001  test loss 286.5643 DKL loss 0.9171  test acc 0.4300\n",
      "Epoch 00000002  avg train loss over 1  batches  4.3187 DKL loss 0.9125  avg train acc  0.6000\n",
      "Epoch 00000002  test loss 283.8840 DKL loss 0.9125  test acc 0.4300\n",
      "Epoch 00000003  avg train loss over 1  batches  4.3089 DKL loss 0.9080  avg train acc  0.6000\n",
      "Epoch 00000003  test loss 280.9754 DKL loss 0.9080  test acc 0.4300\n",
      "Epoch 00000004  avg train loss over 1  batches  4.2531 DKL loss 0.9016  avg train acc  0.6000\n",
      "Epoch 00000004  test loss 277.7558 DKL loss 0.9016  test acc 0.4300\n",
      "Epoch 00000005  avg train loss over 1  batches  4.2085 DKL loss 0.8922  avg train acc  0.6000\n",
      "Epoch 00000005  test loss 274.3209 DKL loss 0.8922  test acc 0.4300\n",
      "Epoch 00000006  avg train loss over 1  batches  6.1689 DKL loss 0.8535  avg train acc  0.2000\n",
      "Epoch 00000006  test loss 261.3856 DKL loss 0.8535  test acc 0.4300\n",
      "Epoch 00000007  avg train loss over 1  batches  2.9670 DKL loss 0.8460  avg train acc  0.8000\n",
      "Epoch 00000007  test loss 262.2495 DKL loss 0.8460  test acc 0.4300\n",
      "Epoch 00000008  avg train loss over 1  batches  4.9306 DKL loss 0.8206  avg train acc  0.4000\n",
      "Epoch 00000008  test loss 252.3329 DKL loss 0.8206  test acc 0.4300\n",
      "Epoch 00000009  avg train loss over 1  batches  4.7737 DKL loss 0.7752  avg train acc  0.4000\n",
      "Epoch 00000009  test loss 241.3925 DKL loss 0.7752  test acc 0.4300\n",
      "Epoch 00000010  avg train loss over 1  batches  5.1399 DKL loss 0.6770  avg train acc  0.2000\n",
      "Epoch 00000010  test loss 221.7228 DKL loss 0.6770  test acc 0.4300\n",
      "Epoch 00000011  avg train loss over 1  batches  3.7553 DKL loss 0.6702  avg train acc  0.6000\n",
      "Epoch 00000011  test loss 216.5794 DKL loss 0.6702  test acc 0.4300\n",
      "Epoch 00000012  avg train loss over 1  batches  3.0711 DKL loss 0.6896  avg train acc  0.8000\n",
      "Epoch 00000012  test loss 219.3940 DKL loss 0.6896  test acc 0.4300\n",
      "Epoch 00000013  avg train loss over 1  batches  3.0235 DKL loss 0.6912  avg train acc  0.8000\n",
      "Epoch 00000013  test loss 222.0475 DKL loss 0.6912  test acc 0.4300\n",
      "Epoch 00000014  avg train loss over 1  batches  4.3392 DKL loss 0.6330  avg train acc  0.4000\n",
      "Epoch 00000014  test loss 208.6020 DKL loss 0.6330  test acc 0.4300\n",
      "Epoch 00000015  avg train loss over 1  batches  2.3357 DKL loss 0.6863  avg train acc  1.0000\n",
      "Epoch 00000015  test loss 219.8510 DKL loss 0.6863  test acc 0.4300\n",
      "Epoch 00000016  avg train loss over 1  batches  3.0737 DKL loss 0.6981  avg train acc  0.8000\n",
      "Epoch 00000016  test loss 222.6752 DKL loss 0.6981  test acc 0.4300\n",
      "Epoch 00000017  avg train loss over 1  batches  4.9163 DKL loss 0.6167  avg train acc  0.2000\n",
      "Epoch 00000017  test loss 200.6785 DKL loss 0.6167  test acc 0.4300\n",
      "Epoch 00000018  avg train loss over 1  batches  3.6734 DKL loss 0.5792  avg train acc  0.6000\n",
      "Epoch 00000018  test loss 195.5991 DKL loss 0.5792  test acc 0.4300\n",
      "Epoch 00000019  avg train loss over 1  batches  2.4397 DKL loss 0.6339  avg train acc  1.0000\n",
      "Epoch 00000019  test loss 208.2417 DKL loss 0.6339  test acc 0.4300\n",
      "Epoch 00000020  avg train loss over 1  batches  3.6928 DKL loss 0.6141  avg train acc  0.6000\n",
      "Epoch 00000020  test loss 203.0335 DKL loss 0.6141  test acc 0.4300\n",
      "Epoch 00000021  avg train loss over 1  batches  3.6803 DKL loss 0.5888  avg train acc  0.6000\n",
      "Epoch 00000021  test loss 197.9773 DKL loss 0.5888  test acc 0.4300\n",
      "Epoch 00000022  avg train loss over 1  batches  4.1657 DKL loss 0.5262  avg train acc  0.4000\n",
      "Epoch 00000022  test loss 183.7648 DKL loss 0.5262  test acc 0.4300\n",
      "Epoch 00000023  avg train loss over 1  batches  3.1052 DKL loss 0.5398  avg train acc  0.8000\n",
      "Epoch 00000023  test loss 188.6037 DKL loss 0.5398  test acc 0.4300\n",
      "Epoch 00000024  avg train loss over 1  batches  3.0871 DKL loss 0.5637  avg train acc  0.8000\n",
      "Epoch 00000024  test loss 193.0288 DKL loss 0.5637  test acc 0.4300\n",
      "Epoch 00000025  avg train loss over 1  batches  3.6915 DKL loss 0.5543  avg train acc  0.6000\n",
      "Epoch 00000025  test loss 188.2532 DKL loss 0.5543  test acc 0.4300\n",
      "Epoch 00000026  avg train loss over 1  batches  3.5845 DKL loss 0.5141  avg train acc  0.6000\n",
      "Epoch 00000026  test loss 183.3050 DKL loss 0.5141  test acc 0.4300\n",
      "Epoch 00000027  avg train loss over 1  batches  3.6126 DKL loss 0.5039  avg train acc  0.6000\n",
      "Epoch 00000027  test loss 178.4395 DKL loss 0.5039  test acc 0.4300\n",
      "Epoch 00000028  avg train loss over 1  batches  3.5507 DKL loss 0.4730  avg train acc  0.6000\n",
      "Epoch 00000028  test loss 173.4689 DKL loss 0.4730  test acc 0.4300\n",
      "Epoch 00000029  avg train loss over 1  batches  3.9676 DKL loss 0.4137  avg train acc  0.4000\n",
      "Epoch 00000029  test loss 158.6578 DKL loss 0.4137  test acc 0.4300\n",
      "Epoch 00000030  avg train loss over 1  batches  3.5634 DKL loss 0.3895  avg train acc  0.6000\n",
      "Epoch 00000030  test loss 154.5787 DKL loss 0.3895  test acc 0.4300\n",
      "Epoch 00000031  avg train loss over 1  batches  3.1665 DKL loss 0.4176  avg train acc  0.8000\n",
      "Epoch 00000031  test loss 161.1493 DKL loss 0.4176  test acc 0.4300\n",
      "Epoch 00000032  avg train loss over 1  batches  3.8716 DKL loss 0.3595  avg train acc  0.4000\n",
      "Epoch 00000032  test loss 145.9753 DKL loss 0.3595  test acc 0.4300\n",
      "Epoch 00000033  avg train loss over 1  batches  2.7820 DKL loss 0.4385  avg train acc  1.0000\n",
      "Epoch 00000033  test loss 163.5933 DKL loss 0.4385  test acc 0.4300\n",
      "Epoch 00000034  avg train loss over 1  batches  4.0605 DKL loss 0.3134  avg train acc  0.2000\n",
      "Epoch 00000034  test loss 137.1307 DKL loss 0.3134  test acc 0.4300\n",
      "Epoch 00000035  avg train loss over 1  batches  3.4675 DKL loss 0.2843  avg train acc  0.6000\n",
      "Epoch 00000035  test loss 133.2617 DKL loss 0.2843  test acc 0.4300\n",
      "Epoch 00000036  avg train loss over 1  batches  3.5504 DKL loss 0.2924  avg train acc  0.6000\n",
      "Epoch 00000036  test loss 129.7281 DKL loss 0.2924  test acc 0.4300\n",
      "Epoch 00000037  avg train loss over 1  batches  3.2649 DKL loss 0.3252  avg train acc  0.8000\n",
      "Epoch 00000037  test loss 137.6151 DKL loss 0.3252  test acc 0.4300\n",
      "Epoch 00000038  avg train loss over 1  batches  3.7916 DKL loss 0.1861  avg train acc  0.2000\n",
      "Epoch 00000038  test loss 110.3705 DKL loss 0.1861  test acc 0.4300\n",
      "Epoch 00000039  avg train loss over 1  batches  3.4744 DKL loss 0.1761  avg train acc  0.6000\n",
      "Epoch 00000039  test loss 107.4506 DKL loss 0.1761  test acc 0.4300\n",
      "Epoch 00000040  avg train loss over 1  batches  3.2824 DKL loss 0.2144  avg train acc  0.8000\n",
      "Epoch 00000040  test loss 117.0673 DKL loss 0.2144  test acc 0.4300\n",
      "Epoch 00000041  avg train loss over 1  batches  2.9605 DKL loss 0.3268  avg train acc  1.0000\n",
      "Epoch 00000041  test loss 137.6528 DKL loss 0.3268  test acc 0.4300\n",
      "Epoch 00000042  avg train loss over 1  batches  3.1919 DKL loss 0.3521  avg train acc  0.8000\n",
      "Epoch 00000042  test loss 145.2437 DKL loss 0.3521  test acc 0.4300\n",
      "Epoch 00000043  avg train loss over 1  batches  3.5095 DKL loss 0.3386  avg train acc  0.6000\n",
      "Epoch 00000043  test loss 141.4497 DKL loss 0.3386  test acc 0.4300\n",
      "Epoch 00000044  avg train loss over 1  batches  2.7973 DKL loss 0.4291  avg train acc  1.0000\n",
      "Epoch 00000044  test loss 159.3428 DKL loss 0.4291  test acc 0.4300\n",
      "Epoch 00000045  avg train loss over 1  batches  3.5743 DKL loss 0.4113  avg train acc  0.6000\n",
      "Epoch 00000045  test loss 154.8719 DKL loss 0.4113  test acc 0.4300\n",
      "Epoch 00000046  avg train loss over 1  batches  3.5200 DKL loss 0.3833  avg train acc  0.6000\n",
      "Epoch 00000046  test loss 150.6407 DKL loss 0.3833  test acc 0.4300\n",
      "Epoch 00000047  avg train loss over 1  batches  3.1569 DKL loss 0.4080  avg train acc  0.8000\n",
      "Epoch 00000047  test loss 157.1008 DKL loss 0.4080  test acc 0.4300\n",
      "Epoch 00000048  avg train loss over 1  batches  3.5185 DKL loss 0.3965  avg train acc  0.6000\n",
      "Epoch 00000048  test loss 152.9398 DKL loss 0.3965  test acc 0.4300\n",
      "Epoch 00000049  avg train loss over 1  batches  3.4785 DKL loss 0.3588  avg train acc  0.6000\n",
      "Epoch 00000049  test loss 148.5328 DKL loss 0.3588  test acc 0.4300\n",
      "Epoch 00000050  avg train loss over 1  batches  3.1905 DKL loss 0.4170  avg train acc  0.8000\n",
      "Epoch 00000050  test loss 155.2320 DKL loss 0.4170  test acc 0.4300\n",
      "Epoch 00000051  avg train loss over 1  batches  3.5426 DKL loss 0.4053  avg train acc  0.6000\n",
      "Epoch 00000051  test loss 151.1931 DKL loss 0.4053  test acc 0.4300\n",
      "Epoch 00000052  avg train loss over 1  batches  3.7792 DKL loss 0.3098  avg train acc  0.4000\n",
      "Epoch 00000052  test loss 135.7334 DKL loss 0.3098  test acc 0.4300\n",
      "Epoch 00000053  avg train loss over 1  batches  3.4877 DKL loss 0.2928  avg train acc  0.6000\n",
      "Epoch 00000053  test loss 131.8788 DKL loss 0.2928  test acc 0.4300\n",
      "Epoch 00000054  avg train loss over 1  batches  3.6250 DKL loss 0.2088  avg train acc  0.4000\n",
      "Epoch 00000054  test loss 116.4350 DKL loss 0.2088  test acc 0.4300\n",
      "Epoch 00000055  avg train loss over 1  batches  3.4154 DKL loss 0.1972  avg train acc  0.6000\n",
      "Epoch 00000055  test loss 113.4162 DKL loss 0.1972  test acc 0.4300\n",
      "Epoch 00000056  avg train loss over 1  batches  3.3968 DKL loss 0.1943  avg train acc  0.6000\n",
      "Epoch 00000056  test loss 110.8750 DKL loss 0.1943  test acc 0.4300\n",
      "Epoch 00000057  avg train loss over 1  batches  3.2721 DKL loss 0.2637  avg train acc  0.8000\n",
      "Epoch 00000057  test loss 120.1871 DKL loss 0.2637  test acc 0.4300\n",
      "Epoch 00000058  avg train loss over 1  batches  3.5967 DKL loss 0.1667  avg train acc  0.4000\n",
      "Epoch 00000058  test loss 104.8744 DKL loss 0.1667  test acc 0.4300\n",
      "Epoch 00000059  avg train loss over 1  batches  3.4914 DKL loss 0.0753  avg train acc  0.4000\n",
      "Epoch 00000059  test loss 89.7360 DKL loss 0.0753  test acc 0.4500\n",
      "Epoch 00000060  avg train loss over 1  batches  3.4243 DKL loss 0.0035  avg train acc  0.8000\n",
      "Epoch 00000060  test loss 74.9249 DKL loss 0.0035  test acc 0.4800\n",
      "Epoch 00000061  avg train loss over 1  batches  3.3956 DKL loss 0.0023  avg train acc  1.0000\n",
      "Epoch 00000061  test loss 76.8419 DKL loss 0.0023  test acc 0.4900\n",
      "Epoch 00000062  avg train loss over 1  batches  3.4113 DKL loss 0.0346  avg train acc  0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000062  test loss 75.6711 DKL loss 0.0346  test acc 0.5200\n",
      "Epoch 00000063  avg train loss over 1  batches  3.3962 DKL loss 0.0501  avg train acc  0.8000\n",
      "Epoch 00000063  test loss 76.0884 DKL loss 0.0501  test acc 0.6000\n",
      "Epoch 00000064  avg train loss over 1  batches  3.4256 DKL loss 0.0242  avg train acc  0.8000\n",
      "Epoch 00000064  test loss 73.0604 DKL loss 0.0242  test acc 0.5500\n",
      "Epoch 00000065  avg train loss over 1  batches  3.3867 DKL loss 0.0315  avg train acc  1.0000\n",
      "Epoch 00000065  test loss 80.1091 DKL loss 0.0315  test acc 0.5000\n",
      "Epoch 00000066  avg train loss over 1  batches  3.3586 DKL loss 0.0995  avg train acc  0.8000\n",
      "Epoch 00000066  test loss 84.5518 DKL loss 0.0995  test acc 0.6200\n",
      "Epoch 00000067  avg train loss over 1  batches  3.4128 DKL loss 0.0014  avg train acc  0.8000\n",
      "Epoch 00000067  test loss 69.3771 DKL loss 0.0014  test acc 0.5500\n",
      "Epoch 00000068  avg train loss over 1  batches  3.4388 DKL loss 0.0005  avg train acc  0.6000\n",
      "Epoch 00000068  test loss 71.4666 DKL loss 0.0005  test acc 0.5400\n",
      "Epoch 00000069  avg train loss over 1  batches  3.4459 DKL loss 0.0191  avg train acc  0.6000\n",
      "Epoch 00000069  test loss 70.3060 DKL loss 0.0191  test acc 0.5500\n",
      "Epoch 00000070  avg train loss over 1  batches  3.3581 DKL loss 0.1014  avg train acc  0.8000\n",
      "Epoch 00000070  test loss 82.3558 DKL loss 0.1014  test acc 0.4800\n",
      "Epoch 00000071  avg train loss over 1  batches  3.3780 DKL loss 0.0766  avg train acc  0.8000\n",
      "Epoch 00000071  test loss 80.9090 DKL loss 0.0766  test acc 0.5000\n",
      "Epoch 00000072  avg train loss over 1  batches  3.3509 DKL loss 0.0983  avg train acc  0.8000\n",
      "Epoch 00000072  test loss 83.3493 DKL loss 0.0983  test acc 0.6500\n",
      "Epoch 00000073  avg train loss over 1  batches  3.3244 DKL loss 0.1456  avg train acc  0.8000\n",
      "Epoch 00000073  test loss 92.6420 DKL loss 0.1456  test acc 0.5800\n",
      "Epoch 00000074  avg train loss over 1  batches  3.4533 DKL loss 0.1174  avg train acc  0.6000\n",
      "Epoch 00000074  test loss 88.5434 DKL loss 0.1174  test acc 0.6000\n",
      "Epoch 00000075  avg train loss over 1  batches  3.3724 DKL loss 0.0898  avg train acc  0.6000\n",
      "Epoch 00000075  test loss 84.1522 DKL loss 0.0898  test acc 0.6500\n",
      "Epoch 00000076  avg train loss over 1  batches  3.3687 DKL loss 0.0187  avg train acc  1.0000\n",
      "Epoch 00000076  test loss 69.2690 DKL loss 0.0187  test acc 0.6300\n",
      "Epoch 00000077  avg train loss over 1  batches  3.3649 DKL loss 0.0148  avg train acc  1.0000\n",
      "Epoch 00000077  test loss 68.5455 DKL loss 0.0148  test acc 0.6500\n",
      "Epoch 00000078  avg train loss over 1  batches  3.3611 DKL loss 0.0891  avg train acc  0.8000\n",
      "Epoch 00000078  test loss 80.2239 DKL loss 0.0891  test acc 0.5300\n",
      "Epoch 00000079  avg train loss over 1  batches  3.3148 DKL loss 0.0954  avg train acc  1.0000\n",
      "Epoch 00000079  test loss 83.3253 DKL loss 0.0954  test acc 0.6600\n",
      "Epoch 00000080  avg train loss over 1  batches  3.3453 DKL loss 0.1040  avg train acc  0.8000\n",
      "Epoch 00000080  test loss 82.5935 DKL loss 0.1040  test acc 0.5000\n",
      "Epoch 00000081  avg train loss over 1  batches  3.3060 DKL loss 0.0792  avg train acc  1.0000\n",
      "Epoch 00000081  test loss 81.0002 DKL loss 0.0792  test acc 0.5400\n",
      "Epoch 00000082  avg train loss over 1  batches  3.2393 DKL loss 0.1630  avg train acc  0.8000\n",
      "Epoch 00000082  test loss 92.2344 DKL loss 0.1630  test acc 0.4700\n",
      "Epoch 00000083  avg train loss over 1  batches  3.4028 DKL loss 0.1305  avg train acc  0.6000\n",
      "Epoch 00000083  test loss 90.1594 DKL loss 0.1305  test acc 0.4900\n",
      "Epoch 00000084  avg train loss over 1  batches  3.2584 DKL loss 0.1722  avg train acc  0.8000\n",
      "Epoch 00000084  test loss 100.5724 DKL loss 0.1722  test acc 0.4300\n",
      "Epoch 00000085  avg train loss over 1  batches  3.1686 DKL loss 0.2572  avg train acc  0.8000\n",
      "Epoch 00000085  test loss 110.4711 DKL loss 0.2572  test acc 0.4300\n",
      "Epoch 00000086  avg train loss over 1  batches  3.4711 DKL loss 0.1664  avg train acc  0.4000\n",
      "Epoch 00000086  test loss 95.4053 DKL loss 0.1664  test acc 0.4600\n",
      "Epoch 00000087  avg train loss over 1  batches  3.3100 DKL loss 0.1354  avg train acc  0.6000\n",
      "Epoch 00000087  test loss 93.0460 DKL loss 0.1354  test acc 0.4900\n",
      "Epoch 00000088  avg train loss over 1  batches  3.1348 DKL loss 0.2588  avg train acc  0.8000\n",
      "Epoch 00000088  test loss 103.4596 DKL loss 0.2588  test acc 0.4300\n",
      "Epoch 00000089  avg train loss over 1  batches  3.4044 DKL loss 0.0835  avg train acc  0.4000\n",
      "Epoch 00000089  test loss 88.2962 DKL loss 0.0835  test acc 0.4800\n",
      "Epoch 00000090  avg train loss over 1  batches  3.2930 DKL loss 0.0405  avg train acc  1.0000\n",
      "Epoch 00000090  test loss 73.5289 DKL loss 0.0405  test acc 0.7000\n",
      "Epoch 00000091  avg train loss over 1  batches  3.3344 DKL loss 0.0588  avg train acc  0.8000\n",
      "Epoch 00000091  test loss 75.7955 DKL loss 0.0588  test acc 0.7900\n",
      "Epoch 00000092  avg train loss over 1  batches  3.3903 DKL loss 0.0502  avg train acc  0.6000\n",
      "Epoch 00000092  test loss 75.2622 DKL loss 0.0502  test acc 0.6800\n",
      "Epoch 00000093  avg train loss over 1  batches  3.2179 DKL loss 0.1897  avg train acc  0.8000\n",
      "Epoch 00000093  test loss 86.7578 DKL loss 0.1897  test acc 0.4900\n",
      "Epoch 00000094  avg train loss over 1  batches  3.3279 DKL loss 0.0735  avg train acc  1.0000\n",
      "Epoch 00000094  test loss 84.7874 DKL loss 0.0735  test acc 0.5300\n",
      "Epoch 00000095  avg train loss over 1  batches  3.3378 DKL loss 0.0340  avg train acc  1.0000\n",
      "Epoch 00000095  test loss 69.9037 DKL loss 0.0340  test acc 0.7700\n",
      "Epoch 00000096  avg train loss over 1  batches  3.3435 DKL loss 0.0604  avg train acc  1.0000\n",
      "Epoch 00000096  test loss 81.6083 DKL loss 0.0604  test acc 0.5600\n",
      "Epoch 00000097  avg train loss over 1  batches  3.3256 DKL loss 0.1359  avg train acc  0.8000\n",
      "Epoch 00000097  test loss 80.7003 DKL loss 0.1359  test acc 0.7300\n",
      "Epoch 00000098  avg train loss over 1  batches  3.2576 DKL loss 0.1216  avg train acc  1.0000\n",
      "Epoch 00000098  test loss 83.5964 DKL loss 0.1216  test acc 0.5500\n",
      "Epoch 00000099  avg train loss over 1  batches  3.2215 DKL loss 0.1737  avg train acc  0.8000\n",
      "Epoch 00000099  test loss 94.3635 DKL loss 0.1737  test acc 0.4900\n",
      "Epoch 00000100  avg train loss over 1  batches  3.3846 DKL loss 0.1341  avg train acc  0.6000\n",
      "Epoch 00000100  test loss 92.0827 DKL loss 0.1341  test acc 0.5000\n",
      "Epoch 00000101  avg train loss over 1  batches  3.3235 DKL loss 0.1301  avg train acc  0.6000\n",
      "Epoch 00000101  test loss 89.8566 DKL loss 0.1301  test acc 0.4900\n",
      "Epoch 00000102  avg train loss over 1  batches  3.1668 DKL loss 0.2540  avg train acc  0.8000\n",
      "Epoch 00000102  test loss 100.3170 DKL loss 0.2540  test acc 0.4500\n",
      "Epoch 00000103  avg train loss over 1  batches  3.4079 DKL loss 0.1448  avg train acc  0.6000\n",
      "Epoch 00000103  test loss 97.8515 DKL loss 0.1448  test acc 0.4900\n",
      "Epoch 00000104  avg train loss over 1  batches  3.2876 DKL loss 0.0974  avg train acc  0.8000\n",
      "Epoch 00000104  test loss 82.6135 DKL loss 0.0974  test acc 0.5600\n",
      "Epoch 00000105  avg train loss over 1  batches  3.3377 DKL loss 0.0695  avg train acc  0.8000\n",
      "Epoch 00000105  test loss 80.7715 DKL loss 0.0695  test acc 0.6300\n",
      "Epoch 00000106  avg train loss over 1  batches  3.2414 DKL loss 0.1514  avg train acc  0.8000\n",
      "Epoch 00000106  test loss 91.7529 DKL loss 0.1514  test acc 0.5100\n",
      "Epoch 00000107  avg train loss over 1  batches  3.3128 DKL loss 0.0316  avg train acc  1.0000\n",
      "Epoch 00000107  test loss 76.6901 DKL loss 0.0316  test acc 0.6900\n",
      "Epoch 00000108  avg train loss over 1  batches  3.2982 DKL loss 0.0640  avg train acc  1.0000\n",
      "Epoch 00000108  test loss 75.1203 DKL loss 0.0640  test acc 0.7000\n",
      "Epoch 00000109  avg train loss over 1  batches  3.3439 DKL loss 0.0900  avg train acc  0.8000\n",
      "Epoch 00000109  test loss 69.4972 DKL loss 0.0900  test acc 0.7800\n",
      "Epoch 00000110  avg train loss over 1  batches  3.1537 DKL loss 0.2009  avg train acc  1.0000\n",
      "Epoch 00000110  test loss 89.0677 DKL loss 0.2009  test acc 0.5200\n",
      "Epoch 00000111  avg train loss over 1  batches  3.0496 DKL loss 0.2691  avg train acc  1.0000\n",
      "Epoch 00000111  test loss 111.7601 DKL loss 0.2691  test acc 0.4300\n",
      "Epoch 00000112  avg train loss over 1  batches  3.4481 DKL loss 0.1499  avg train acc  0.4000\n",
      "Epoch 00000112  test loss 96.4388 DKL loss 0.1499  test acc 0.4900\n",
      "Epoch 00000113  avg train loss over 1  batches  3.4100 DKL loss 0.0560  avg train acc  0.4000\n",
      "Epoch 00000113  test loss 81.4258 DKL loss 0.0560  test acc 0.6300\n",
      "Epoch 00000114  avg train loss over 1  batches  3.2951 DKL loss 0.0556  avg train acc  1.0000\n",
      "Epoch 00000114  test loss 79.5280 DKL loss 0.0556  test acc 0.6800\n",
      "Epoch 00000115  avg train loss over 1  batches  3.3983 DKL loss 0.0561  avg train acc  0.6000\n",
      "Epoch 00000115  test loss 68.4743 DKL loss 0.0561  test acc 0.8000\n",
      "Epoch 00000116  avg train loss over 1  batches  3.1392 DKL loss 0.1763  avg train acc  1.0000\n",
      "Epoch 00000116  test loss 76.7294 DKL loss 0.1763  test acc 0.6900\n",
      "Epoch 00000117  avg train loss over 1  batches  3.2783 DKL loss 0.0698  avg train acc  1.0000\n",
      "Epoch 00000117  test loss 75.0963 DKL loss 0.0698  test acc 0.7300\n",
      "Epoch 00000118  avg train loss over 1  batches  3.2727 DKL loss 0.0476  avg train acc  1.0000\n",
      "Epoch 00000118  test loss 72.5803 DKL loss 0.0476  test acc 0.8600\n",
      "Epoch 00000119  avg train loss over 1  batches  3.2814 DKL loss 0.0705  avg train acc  1.0000\n",
      "Epoch 00000119  test loss 72.6369 DKL loss 0.0705  test acc 0.7900\n",
      "Epoch 00000120  avg train loss over 1  batches  3.2456 DKL loss 0.0394  avg train acc  1.0000\n",
      "Epoch 00000120  test loss 71.0281 DKL loss 0.0394  test acc 0.8100\n",
      "Epoch 00000121  avg train loss over 1  batches  3.1160 DKL loss 0.2253  avg train acc  1.0000\n",
      "Epoch 00000121  test loss 95.2637 DKL loss 0.2253  test acc 0.5100\n",
      "Epoch 00000122  avg train loss over 1  batches  2.9875 DKL loss 0.3082  avg train acc  1.0000\n",
      "Epoch 00000122  test loss 117.4622 DKL loss 0.3082  test acc 0.4300\n",
      "Epoch 00000123  avg train loss over 1  batches  3.3929 DKL loss 0.2252  avg train acc  0.6000\n",
      "Epoch 00000123  test loss 114.3161 DKL loss 0.2252  test acc 0.4300\n",
      "Epoch 00000124  avg train loss over 1  batches  3.3001 DKL loss 0.2143  avg train acc  0.6000\n",
      "Epoch 00000124  test loss 110.9797 DKL loss 0.2143  test acc 0.4300\n",
      "Epoch 00000125  avg train loss over 1  batches  3.3631 DKL loss 0.1265  avg train acc  0.4000\n",
      "Epoch 00000125  test loss 95.3421 DKL loss 0.1265  test acc 0.5100\n",
      "Epoch 00000126  avg train loss over 1  batches  3.2352 DKL loss 0.1833  avg train acc  0.6000\n",
      "Epoch 00000126  test loss 92.6744 DKL loss 0.1833  test acc 0.5000\n",
      "Epoch 00000127  avg train loss over 1  batches  3.2389 DKL loss 0.0633  avg train acc  1.0000\n",
      "Epoch 00000127  test loss 77.2819 DKL loss 0.0633  test acc 0.6900\n",
      "Epoch 00000128  avg train loss over 1  batches  3.2283 DKL loss 0.0681  avg train acc  1.0000\n",
      "Epoch 00000128  test loss 75.4463 DKL loss 0.0681  test acc 0.7600\n",
      "Epoch 00000129  avg train loss over 1  batches  3.1385 DKL loss 0.1969  avg train acc  1.0000\n",
      "Epoch 00000129  test loss 86.4892 DKL loss 0.1969  test acc 0.5700\n",
      "Epoch 00000130  avg train loss over 1  batches  3.2383 DKL loss 0.1599  avg train acc  0.6000\n",
      "Epoch 00000130  test loss 84.3161 DKL loss 0.1599  test acc 0.6400\n",
      "Epoch 00000131  avg train loss over 1  batches  3.2245 DKL loss 0.1029  avg train acc  1.0000\n",
      "Epoch 00000131  test loss 82.1106 DKL loss 0.1029  test acc 0.6700\n",
      "Epoch 00000132  avg train loss over 1  batches  3.2054 DKL loss 0.0274  avg train acc  1.0000\n",
      "Epoch 00000132  test loss 67.0374 DKL loss 0.0274  test acc 0.8100\n",
      "Epoch 00000133  avg train loss over 1  batches  3.1507 DKL loss 0.1667  avg train acc  1.0000\n",
      "Epoch 00000133  test loss 78.5869 DKL loss 0.1667  test acc 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000134  avg train loss over 1  batches  3.2194 DKL loss 0.1283  avg train acc  1.0000\n",
      "Epoch 00000134  test loss 76.7244 DKL loss 0.1283  test acc 0.7400\n",
      "Epoch 00000135  avg train loss over 1  batches  3.3722 DKL loss 0.0042  avg train acc  1.0000\n",
      "Epoch 00000135  test loss 78.9615 DKL loss 0.0042  test acc 0.7000\n",
      "Epoch 00000136  avg train loss over 1  batches  3.1100 DKL loss 0.0319  avg train acc  1.0000\n",
      "Epoch 00000136  test loss 67.9569 DKL loss 0.0319  test acc 0.8600\n",
      "Epoch 00000137  avg train loss over 1  batches  3.0487 DKL loss 0.2695  avg train acc  1.0000\n",
      "Epoch 00000137  test loss 88.1981 DKL loss 0.2695  test acc 0.5700\n",
      "Epoch 00000138  avg train loss over 1  batches  2.8846 DKL loss 0.3730  avg train acc  1.0000\n",
      "Epoch 00000138  test loss 110.5662 DKL loss 0.3730  test acc 0.4700\n",
      "Epoch 00000139  avg train loss over 1  batches  3.3906 DKL loss 0.0625  avg train acc  0.4000\n",
      "Epoch 00000139  test loss 94.9509 DKL loss 0.0625  test acc 0.5200\n",
      "Epoch 00000140  avg train loss over 1  batches  3.2348 DKL loss 0.1682  avg train acc  0.8000\n",
      "Epoch 00000140  test loss 92.2858 DKL loss 0.1682  test acc 0.5500\n",
      "Epoch 00000141  avg train loss over 1  batches  3.1963 DKL loss 0.0682  avg train acc  1.0000\n",
      "Epoch 00000141  test loss 76.7038 DKL loss 0.0682  test acc 0.7500\n",
      "Epoch 00000142  avg train loss over 1  batches  3.2870 DKL loss 0.0640  avg train acc  0.8000\n",
      "Epoch 00000142  test loss 69.2625 DKL loss 0.0640  test acc 0.8600\n",
      "Epoch 00000143  avg train loss over 1  batches  3.3710 DKL loss 0.0654  avg train acc  1.0000\n",
      "Epoch 00000143  test loss 65.8371 DKL loss 0.0654  test acc 0.8500\n",
      "Epoch 00000144  avg train loss over 1  batches  3.1435 DKL loss 0.0847  avg train acc  1.0000\n",
      "Epoch 00000144  test loss 76.0177 DKL loss 0.0847  test acc 0.8600\n",
      "Epoch 00000145  avg train loss over 1  batches  3.1417 DKL loss 0.0104  avg train acc  1.0000\n",
      "Epoch 00000145  test loss 72.8082 DKL loss 0.0104  test acc 0.8700\n",
      "Epoch 00000146  avg train loss over 1  batches  3.0813 DKL loss 0.0544  avg train acc  1.0000\n",
      "Epoch 00000146  test loss 74.0531 DKL loss 0.0544  test acc 0.8700\n",
      "Epoch 00000147  avg train loss over 1  batches  3.2543 DKL loss 0.0503  avg train acc  1.0000\n",
      "Epoch 00000147  test loss 73.1516 DKL loss 0.0503  test acc 0.8100\n",
      "Epoch 00000148  avg train loss over 1  batches  3.0223 DKL loss 0.0022  avg train acc  1.0000\n",
      "Epoch 00000148  test loss 72.7356 DKL loss 0.0022  test acc 0.8700\n",
      "Epoch 00000149  avg train loss over 1  batches  3.2886 DKL loss 0.0568  avg train acc  0.8000\n",
      "Epoch 00000149  test loss 69.2151 DKL loss 0.0568  test acc 0.8800\n",
      "Epoch 00000150  avg train loss over 1  batches  3.0683 DKL loss 0.1984  avg train acc  1.0000\n",
      "Epoch 00000150  test loss 72.8523 DKL loss 0.1984  test acc 0.8100\n",
      "Epoch 00000151  avg train loss over 1  batches  3.3540 DKL loss 0.0189  avg train acc  1.0000\n",
      "Epoch 00000151  test loss 75.5613 DKL loss 0.0189  test acc 0.7900\n",
      "Epoch 00000152  avg train loss over 1  batches  3.2941 DKL loss 0.0250  avg train acc  1.0000\n",
      "Epoch 00000152  test loss 74.0806 DKL loss 0.0250  test acc 0.8100\n",
      "Epoch 00000153  avg train loss over 1  batches  3.1879 DKL loss 0.0452  avg train acc  1.0000\n",
      "Epoch 00000153  test loss 71.1500 DKL loss 0.0452  test acc 0.8700\n",
      "Epoch 00000154  avg train loss over 1  batches  3.3487 DKL loss 0.0916  avg train acc  0.6000\n",
      "Epoch 00000154  test loss 67.3699 DKL loss 0.0916  test acc 0.8600\n",
      "Epoch 00000155  avg train loss over 1  batches  3.2022 DKL loss 0.1153  avg train acc  1.0000\n",
      "Epoch 00000155  test loss 74.9187 DKL loss 0.1153  test acc 0.8000\n",
      "Epoch 00000156  avg train loss over 1  batches  3.1638 DKL loss 0.0766  avg train acc  1.0000\n",
      "Epoch 00000156  test loss 72.9126 DKL loss 0.0766  test acc 0.8100\n",
      "Epoch 00000157  avg train loss over 1  batches  3.0910 DKL loss 0.2343  avg train acc  1.0000\n",
      "Epoch 00000157  test loss 83.8592 DKL loss 0.2343  test acc 0.6900\n",
      "Epoch 00000158  avg train loss over 1  batches  3.0561 DKL loss 0.2817  avg train acc  0.8000\n",
      "Epoch 00000158  test loss 94.0584 DKL loss 0.2817  test acc 0.5600\n",
      "Epoch 00000159  avg train loss over 1  batches  2.6935 DKL loss 0.4881  avg train acc  1.0000\n",
      "Epoch 00000159  test loss 115.2141 DKL loss 0.4881  test acc 0.4800\n",
      "Epoch 00000160  avg train loss over 1  batches  3.3804 DKL loss 0.1297  avg train acc  0.4000\n",
      "Epoch 00000160  test loss 99.5365 DKL loss 0.1297  test acc 0.5200\n",
      "Epoch 00000161  avg train loss over 1  batches  3.0335 DKL loss 0.2968  avg train acc  0.8000\n",
      "Epoch 00000161  test loss 95.7925 DKL loss 0.2968  test acc 0.5500\n",
      "Epoch 00000162  avg train loss over 1  batches  3.3145 DKL loss 0.0948  avg train acc  1.0000\n",
      "Epoch 00000162  test loss 67.4024 DKL loss 0.0948  test acc 0.8500\n",
      "Epoch 00000163  avg train loss over 1  batches  3.0874 DKL loss 0.0824  avg train acc  1.0000\n",
      "Epoch 00000163  test loss 65.4440 DKL loss 0.0824  test acc 0.8500\n",
      "Epoch 00000164  avg train loss over 1  batches  3.0278 DKL loss 0.1099  avg train acc  1.0000\n",
      "Epoch 00000164  test loss 66.1657 DKL loss 0.1099  test acc 0.8700\n",
      "Epoch 00000165  avg train loss over 1  batches  3.1637 DKL loss 0.0007  avg train acc  1.0000\n",
      "Epoch 00000165  test loss 65.9320 DKL loss 0.0007  test acc 0.8600\n",
      "Epoch 00000166  avg train loss over 1  batches  3.0424 DKL loss 0.1249  avg train acc  1.0000\n",
      "Epoch 00000166  test loss 65.4389 DKL loss 0.1249  test acc 0.8500\n",
      "Epoch 00000167  avg train loss over 1  batches  3.2386 DKL loss 0.0398  avg train acc  1.0000\n",
      "Epoch 00000167  test loss 67.0054 DKL loss 0.0398  test acc 0.8500\n",
      "Epoch 00000168  avg train loss over 1  batches  3.1671 DKL loss 0.0656  avg train acc  0.8000\n",
      "Epoch 00000168  test loss 76.4762 DKL loss 0.0656  test acc 0.8700\n",
      "Epoch 00000169  avg train loss over 1  batches  3.1611 DKL loss 0.0481  avg train acc  0.8000\n",
      "Epoch 00000169  test loss 72.9506 DKL loss 0.0481  test acc 0.8700\n",
      "Epoch 00000170  avg train loss over 1  batches  3.2079 DKL loss 0.0577  avg train acc  1.0000\n",
      "Epoch 00000170  test loss 72.4473 DKL loss 0.0577  test acc 0.8200\n",
      "Epoch 00000171  avg train loss over 1  batches  3.0564 DKL loss 0.1305  avg train acc  1.0000\n",
      "Epoch 00000171  test loss 84.3009 DKL loss 0.1305  test acc 0.8700\n",
      "Epoch 00000172  avg train loss over 1  batches  3.1284 DKL loss 0.1320  avg train acc  0.8000\n",
      "Epoch 00000172  test loss 71.9659 DKL loss 0.1320  test acc 0.8800\n",
      "Epoch 00000173  avg train loss over 1  batches  2.9496 DKL loss 0.0466  avg train acc  1.0000\n",
      "Epoch 00000173  test loss 86.5606 DKL loss 0.0466  test acc 0.8500\n",
      "Epoch 00000174  avg train loss over 1  batches  2.9404 DKL loss 0.0292  avg train acc  1.0000\n",
      "Epoch 00000174  test loss 83.1286 DKL loss 0.0292  test acc 0.8500\n",
      "Epoch 00000175  avg train loss over 1  batches  3.2025 DKL loss 0.2801  avg train acc  0.8000\n",
      "Epoch 00000175  test loss 91.2338 DKL loss 0.2801  test acc 0.8200\n",
      "Epoch 00000176  avg train loss over 1  batches  2.9370 DKL loss 0.0495  avg train acc  1.0000\n",
      "Epoch 00000176  test loss 87.3895 DKL loss 0.0495  test acc 0.8500\n",
      "Epoch 00000177  avg train loss over 1  batches  3.1218 DKL loss 0.1243  avg train acc  0.8000\n",
      "Epoch 00000177  test loss 74.8669 DKL loss 0.1243  test acc 0.8700\n",
      "Epoch 00000178  avg train loss over 1  batches  2.9897 DKL loss 0.3052  avg train acc  1.0000\n",
      "Epoch 00000178  test loss 77.8031 DKL loss 0.3052  test acc 0.8000\n",
      "Epoch 00000179  avg train loss over 1  batches  3.1269 DKL loss 0.1154  avg train acc  1.0000\n",
      "Epoch 00000179  test loss 75.5284 DKL loss 0.1154  test acc 0.8100\n",
      "Epoch 00000180  avg train loss over 1  batches  3.2399 DKL loss 0.0556  avg train acc  0.8000\n",
      "Epoch 00000180  test loss 67.0416 DKL loss 0.0556  test acc 0.8900\n",
      "Epoch 00000181  avg train loss over 1  batches  2.9319 DKL loss 0.3416  avg train acc  1.0000\n",
      "Epoch 00000181  test loss 84.9521 DKL loss 0.3416  test acc 0.7300\n",
      "Epoch 00000182  avg train loss over 1  batches  3.0558 DKL loss 0.0745  avg train acc  1.0000\n",
      "Epoch 00000182  test loss 71.4272 DKL loss 0.0745  test acc 0.8800\n",
      "Epoch 00000183  avg train loss over 1  batches  3.1070 DKL loss 0.0197  avg train acc  0.8000\n",
      "Epoch 00000183  test loss 72.5294 DKL loss 0.0197  test acc 0.8800\n",
      "Epoch 00000184  avg train loss over 1  batches  2.9050 DKL loss 0.1391  avg train acc  1.0000\n",
      "Epoch 00000184  test loss 74.5651 DKL loss 0.1391  test acc 0.8800\n",
      "Epoch 00000185  avg train loss over 1  batches  3.1166 DKL loss 0.0468  avg train acc  0.8000\n",
      "Epoch 00000185  test loss 71.2054 DKL loss 0.0468  test acc 0.8900\n",
      "Epoch 00000186  avg train loss over 1  batches  3.2096 DKL loss 0.0772  avg train acc  0.8000\n",
      "Epoch 00000186  test loss 67.4625 DKL loss 0.0772  test acc 0.8900\n",
      "Epoch 00000187  avg train loss over 1  batches  3.0133 DKL loss 0.2165  avg train acc  1.0000\n",
      "Epoch 00000187  test loss 71.0233 DKL loss 0.2165  test acc 0.8100\n",
      "Epoch 00000188  avg train loss over 1  batches  2.9502 DKL loss 0.1648  avg train acc  1.0000\n",
      "Epoch 00000188  test loss 68.0694 DKL loss 0.1648  test acc 0.8600\n",
      "Epoch 00000189  avg train loss over 1  batches  3.2637 DKL loss 0.1400  avg train acc  0.8000\n",
      "Epoch 00000189  test loss 97.4299 DKL loss 0.1400  test acc 0.6000\n",
      "Epoch 00000190  avg train loss over 1  batches  3.1790 DKL loss 0.1733  avg train acc  0.8000\n",
      "Epoch 00000190  test loss 94.4907 DKL loss 0.1733  test acc 0.6500\n",
      "Epoch 00000191  avg train loss over 1  batches  2.9760 DKL loss 0.3195  avg train acc  0.8000\n",
      "Epoch 00000191  test loss 103.6269 DKL loss 0.3195  test acc 0.5500\n",
      "Epoch 00000192  avg train loss over 1  batches  3.1183 DKL loss 0.2376  avg train acc  0.8000\n",
      "Epoch 00000192  test loss 100.0563 DKL loss 0.2376  test acc 0.6000\n",
      "Epoch 00000193  avg train loss over 1  batches  3.2254 DKL loss 0.0451  avg train acc  1.0000\n",
      "Epoch 00000193  test loss 84.2939 DKL loss 0.0451  test acc 0.7500\n",
      "Epoch 00000194  avg train loss over 1  batches  3.0317 DKL loss 0.1914  avg train acc  1.0000\n",
      "Epoch 00000194  test loss 81.1878 DKL loss 0.1914  test acc 0.7700\n",
      "Epoch 00000195  avg train loss over 1  batches  2.7678 DKL loss 0.4419  avg train acc  1.0000\n",
      "Epoch 00000195  test loss 103.2836 DKL loss 0.4419  test acc 0.5700\n",
      "Epoch 00000196  avg train loss over 1  batches  2.8751 DKL loss 0.4049  avg train acc  0.8000\n",
      "Epoch 00000196  test loss 111.4304 DKL loss 0.4049  test acc 0.5200\n",
      "Epoch 00000197  avg train loss over 1  batches  3.3393 DKL loss 0.0588  avg train acc  0.8000\n",
      "Epoch 00000197  test loss 95.7919 DKL loss 0.0588  test acc 0.6500\n",
      "Epoch 00000198  avg train loss over 1  batches  2.8922 DKL loss 0.3716  avg train acc  0.8000\n",
      "Epoch 00000198  test loss 104.4043 DKL loss 0.3716  test acc 0.5600\n",
      "Epoch 00000199  avg train loss over 1  batches  3.1681 DKL loss 0.2092  avg train acc  0.8000\n",
      "Epoch 00000199  test loss 101.1072 DKL loss 0.2092  test acc 0.6000\n",
      "[[0.45064336]\n",
      " [0.48898906]\n",
      " [0.4678897 ]\n",
      " [0.46272048]\n",
      " [0.4175743 ]\n",
      " [0.42221057]\n",
      " [0.5326459 ]\n",
      " [0.49738944]\n",
      " [0.5300039 ]\n",
      " [0.47983968]\n",
      " [0.486746  ]\n",
      " [0.48306087]\n",
      " [0.49737963]\n",
      " [0.39819455]\n",
      " [0.39924356]\n",
      " [0.5313804 ]\n",
      " [0.52657604]\n",
      " [0.48901847]\n",
      " [0.5055626 ]\n",
      " [0.42242736]\n",
      " [0.4764786 ]\n",
      " [0.461538  ]\n",
      " [0.40059316]\n",
      " [0.4958412 ]\n",
      " [0.42061174]\n",
      " [0.48908153]\n",
      " [0.4698957 ]\n",
      " [0.43767497]\n",
      " [0.46523342]\n",
      " [0.40086713]\n",
      " [0.5326148 ]\n",
      " [0.4972728 ]\n",
      " [0.3661267 ]\n",
      " [0.51751935]\n",
      " [0.43206236]\n",
      " [0.5170081 ]\n",
      " [0.39787722]\n",
      " [0.50400084]\n",
      " [0.42979455]\n",
      " [0.42431435]\n",
      " [0.3661267 ]\n",
      " [0.46215868]\n",
      " [0.43462718]\n",
      " [0.40032986]\n",
      " [0.49838665]\n",
      " [0.46858966]\n",
      " [0.4903359 ]\n",
      " [0.4961745 ]\n",
      " [0.45699263]\n",
      " [0.49624875]\n",
      " [0.5178337 ]\n",
      " [0.39085692]\n",
      " [0.47863075]\n",
      " [0.47260398]\n",
      " [0.486746  ]\n",
      " [0.45489037]\n",
      " [0.505609  ]\n",
      " [0.49088788]\n",
      " [0.49737963]\n",
      " [0.48360354]\n",
      " [0.41494492]\n",
      " [0.41598228]\n",
      " [0.43931815]\n",
      " [0.45898905]\n",
      " [0.43767497]\n",
      " [0.48501623]\n",
      " [0.47723377]\n",
      " [0.43540797]\n",
      " [0.53076595]\n",
      " [0.47256985]\n",
      " [0.47933882]\n",
      " [0.43897405]\n",
      " [0.46589288]\n",
      " [0.41494492]\n",
      " [0.46057636]\n",
      " [0.46057636]\n",
      " [0.40662417]\n",
      " [0.41935182]\n",
      " [0.4600653 ]\n",
      " [0.43109423]\n",
      " [0.47863075]\n",
      " [0.5027409 ]\n",
      " [0.4687625 ]\n",
      " [0.37858826]\n",
      " [0.46757412]\n",
      " [0.50262403]\n",
      " [0.4322672 ]\n",
      " [0.3317441 ]\n",
      " [0.45829862]\n",
      " [0.37628904]\n",
      " [0.39085692]\n",
      " [0.50236547]\n",
      " [0.49148622]\n",
      " [0.42802724]\n",
      " [0.4467201 ]\n",
      " [0.41933408]\n",
      " [0.40326202]\n",
      " [0.5062938 ]\n",
      " [0.49804547]\n",
      " [0.5033571 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# load the sklearn breast cancer dataset\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data[:, :]\n",
    "Y = bc.target\n",
    "\n",
    "# min max scale and binarize the target labels\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X,Y)\n",
    "label = LabelBinarizer()\n",
    "Y = label.fit_transform(Y)\n",
    "\n",
    "# train fraction\n",
    "frac = 0.01\n",
    "np.random.seed(666)\n",
    "# shuffle dataset\n",
    "idx = np.random.randint(X.shape[0], size=len(X))\n",
    "X = X[idx]\n",
    "Y = Y[idx]\n",
    "\n",
    "train_stop = int(len(X) * frac)\n",
    "test_stop = 100\n",
    "X_ = X[:train_stop]\n",
    "Y_ = Y[:train_stop]\n",
    "\n",
    "# have the same 10% holdout as the previous example\n",
    "X_t = X[len(X) - test_stop:]\n",
    "Y_t = Y[len(X) - test_stop:]\n",
    "\n",
    "\n",
    "# plot the first 3 PCA dimensions of the sampled data\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=3).fit_transform(X_)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y_.ravel(),\n",
    "           cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "                                multi_class = 'one_vs_one',\n",
    "                                random_state=0).fit(X_, Y_)\n",
    "\n",
    "# lets see how good our fit on the train set is\n",
    "print(gpc.score(X_, Y_))\n",
    "\n",
    "# create the TF neural net\n",
    "# some hyperparams\n",
    "training_epochs = 200\n",
    "\n",
    "n_neurons_in_h1 = 10\n",
    "n_neurons_in_h2 = 10\n",
    "learning_rate = 0.01\n",
    "dkl_loss_rate = 0.1\n",
    "\n",
    "n_features = len(X[0])\n",
    "labels_dim = 1\n",
    "#############################################\n",
    "\n",
    "\n",
    "# these placeholders serve as our input tensors\n",
    "x = tf.placeholder(tf.float32, [None, n_features], name='input')\n",
    "y = tf.placeholder(tf.float32, [None, labels_dim], name='labels')\n",
    "# input tensor for our reference model predictions\n",
    "y_g = tf.placeholder(tf.float32, [None, labels_dim], name='labels')\n",
    "\n",
    "# TF Variables are our neural net parameter tensors, we initialize them to random (gaussian) values in\n",
    "# Layer1. Variables are allowed to be persistent across training epochs and updatable bt TF operations\n",
    "W1 = tf.Variable(tf.truncated_normal([n_features, n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)),\n",
    "                 name='weights1')\n",
    "b1 = tf.Variable(tf.truncated_normal([n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)), name='biases1')\n",
    "\n",
    "# note the output tensor of the 1st layer is the activation applied to a\n",
    "# linear transform of the layer 1 parameter tensors\n",
    "# the matmul operation calculates the dot product between the tensors\n",
    "y1 = tf.sigmoid((tf.matmul(x, W1) + b1), name='activationLayer1')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer2)\n",
    "W2 = tf.Variable(tf.random_normal([n_neurons_in_h1, n_neurons_in_h2], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b2 = tf.Variable(tf.random_normal([n_neurons_in_h2], mean=0, stddev=1), name='biases2')\n",
    "# activation function(sigmoid)\n",
    "y2 = tf.sigmoid((tf.matmul(y1, W2) + b2), name='activationLayer2')\n",
    "\n",
    "# output layer weights and biases\n",
    "Wo = tf.Variable(tf.random_normal([n_neurons_in_h2, labels_dim], mean=0, stddev=1 ),\n",
    "                 name='weightsOut')\n",
    "bo = tf.Variable(tf.random_normal([labels_dim], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "# the sigmoid (binary softmax) activation is absorbed into TF's sigmoid_cross_entropy_with_logits loss\n",
    "logits = (tf.matmul(y2, Wo) + bo)\n",
    "loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "\n",
    "# tap a separate output that applies softmax activation to the output layer\n",
    "# for training accuracy readout\n",
    "a = tf.nn.sigmoid(logits, name='activationOutputLayer')\n",
    "\n",
    "# here's the KL-Div loss, note the inputs are softmax distributions, not raw logits\n",
    "def kl_divergence(p, q):\n",
    "    return tf.reduce_sum(p * tf.log(p/q))\n",
    "\n",
    "loss_2 = kl_divergence(a, y_g)\n",
    "\n",
    "# combined loss, since the DKL loss can be negative, reverse its sign when negative\n",
    "# basically an abs() but the demonstration is on how to use tf.cond() to check tensor values\n",
    "loss_2 = tf.cond(loss_2 < 0, lambda: -1 * loss_2, lambda: loss_2)\n",
    "\n",
    "# can also normalize the losses for stability but not done in this case\n",
    "norm = 1 #tf.reduce_sum(loss_1 + loss_2)\n",
    "loss = loss_1 / norm + dkl_loss_rate*loss_2 / norm\n",
    "\n",
    "# optimizer used to compute gradient of loss and apply the parameter updates.\n",
    "# the train_step object returned is ran by a TF Session to train the net\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# prediction accuracy\n",
    "# compare predicted value from network with the expected value/target\n",
    "\n",
    "correct_prediction = tf.equal(tf.round(a), y)\n",
    "# accuracy determination\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
    "\n",
    "#############################################\n",
    "# ***NOTE global_variables_initializer() must be called before creating a tf.Session()!***\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# create a session for training and feedforward (prediction). Sessions are TF's way to run\n",
    "# feed data to placeholders and variables, obtain outputs and update neural net parameters\n",
    "with tf.Session() as sess:\n",
    "    # ***initialization of all variables... NOTE this must be done before running any further sessions!***\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # training loop over the number of epochs\n",
    "    batch_size = 5\n",
    "    batches = int(len(X_) / batch_size)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        losses = 0\n",
    "        dkl_losses = 0\n",
    "        accs = 0\n",
    "        for j in range(batches):\n",
    "            idx = np.random.randint(X_.shape[0], size=batch_size)\n",
    "            X_b = X_[idx]\n",
    "            Y_b = Y_[idx]\n",
    "\n",
    "            # get the GPC predictions... and slice only the positive class probabilities\n",
    "            Y_g = gpc.predict_proba(X_b)[:,1].reshape((-1,1))\n",
    "\n",
    "            # train the network, note the dictionary of inputs and labels\n",
    "            sess.run(train_step, feed_dict={x: X_b, y: Y_b, y_g: Y_g})\n",
    "            # feedforwad the same data and labels, but grab the accuracy and loss as outputs\n",
    "            acc, l, soft_max_a, l_2 = sess.run([accuracy, loss, a, loss_2], feed_dict={x: X_b, y: Y_b, y_g: Y_g})\n",
    "\n",
    "            losses = losses + np.sum(l)\n",
    "            accs = accs + np.sum(acc)\n",
    "            dkl_losses = dkl_losses + np.sum(l_2)\n",
    "        print(\"Epoch %.8d \" % epoch, \"avg train loss over\", batches, \" batches \", \"%.4f\" % (losses/batches),\n",
    "              \"DKL loss %.4f \" % (dkl_losses/batches), \"avg train acc \", \"%.4f\" % (accs/batches))\n",
    "\n",
    "        # test on the holdout set\n",
    "        Y_g = gpc.predict_proba(X_t)[:, 1].reshape((-1,1))\n",
    "\n",
    "        acc, l, soft_max_a = sess.run([accuracy, loss, a], feed_dict={x: X_t, y: Y_t, y_g: Y_g})\n",
    "        print(\"Epoch %.8d \" % epoch, \"test loss %.4f\" % np.sum(l),\n",
    "               \"DKL loss %.4f \" % dkl_losses, \"test acc %.4f\" % acc)\n",
    "\n",
    "\n",
    "print(soft_max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] data generator for MLP version 3 date 2/23\n",
    "def dg_train(all_train_data_addr, train_label_addr, mini_batch_size, batch_size, train_data_len, window_size):\n",
    "    global I\n",
    "    h5f = {}\n",
    "    for i in range(len(all_train_data_addr)):\n",
    "        h5f[i] = h5py.File(all_train_data_addr[i],'r')\n",
    "#         print(h5f[i]['train_data'].shape)\n",
    "    h5f_label = h5py.File(train_label_addr,'r')\n",
    "    \n",
    "    indx = np.arange(0, int(train_data_len/len(all_train_data_addr))-(mini_batch_size+1), mini_batch_size)\n",
    "    file_Nu = list(range(len(all_train_data_addr)))\n",
    "    all_data_mini_batch_flag = np.asarray([[n, i] for n in file_Nu for i in indx])\n",
    "    all_data_mini_batch_flag = shuffle(all_data_mini_batch_flag, random_state=0)\n",
    "#     print(all_data_mini_batch_flag.shape)\n",
    "    \n",
    "    while True:             #this line is just because keras needs infinite generators\n",
    "        first_mini_batch = 'True'\n",
    "        for I in all_data_mini_batch_flag:\n",
    "            if train_data_len-I[1]<=mini_batch_size or I[1]<=mini_batch_size:\n",
    "                I[1] = copy.deepcopy(mini_batch_size)\n",
    "            temp = h5f[I[0]]['train_data'][:,I[1]-window_size:I[1]+mini_batch_size+window_size]\n",
    "            if first_mini_batch == 'True':\n",
    "                y_int = h5f_label['train_label'][:,I[1]:I[1]+mini_batch_size]\n",
    "                a = np.reshape(temp[:,:window_size*2+1], (96*(window_size*2+1),1))\n",
    "                for i in range(1,mini_batch_size):\n",
    "                    b = np.reshape(temp[:,i:i+window_size*2+1],(96*(window_size*2+1),1))\n",
    "                    a = np.concatenate((a,b),axis = 1)\n",
    "                first_mini_batch = 'False'\n",
    "            else:                \n",
    "                y_int = np.concatenate((y_int,h5f_label['train_label'][:,I[1]:I[1]+mini_batch_size]), axis = 1)\n",
    "                for i in range(0,mini_batch_size):\n",
    "                    a = np.concatenate((a,np.reshape(temp[:,i:i+window_size*2+1],(96*(window_size*2+1),1))),axis = 1)\n",
    "#             print(a.shape[1])\n",
    "            if a.shape[1] == batch_size:\n",
    "                train_data_batch = a.T\n",
    "                train_data_batch_scale = preprocessing.scale(train_data_batch)\n",
    "                train_label_batch = y_int.T\n",
    "                train_label_batch = to_categorical(train_label_batch)\n",
    "#                 print(train_label_batch.shape)\n",
    "#                 print(train_data_batch[:,1])\n",
    "                first_mini_batch = 'True'\n",
    "                if train_label_batch.shape[1]==2:\n",
    "                    yield(train_data_batch_scale, train_label_batch)\n",
    "#                 yield(train_data_scale, train_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_addr = [train_data_addr, train_data_addr2]\n",
    "# train_data_len = 1417087*2\n",
    "train_data_len = 1879087 * len(all_train_data_addr)\n",
    "mini_batch_size = 64\n",
    "batch_size = 128\n",
    "window_size = 10\n",
    "q,w = next(dg_train(all_train_data_addr, train_label_addr, mini_batch_size, batch_size, train_data_len, window_size))\n",
    "print(q.shape)\n",
    "print(w.shape)\n",
    "plt.matshow(q.T[960-96:960+2*96,]);\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] VAD model\n",
    "all_train_data_addr = [train_data_addr, train_data_addr2]\n",
    "train_data_len = 1879087 * len(all_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 256\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 50\n",
    "h = [512, 512]\n",
    "\n",
    "\n",
    "X_dim = 96*(window_size*2+1)\n",
    "y_dim = 2\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim=X_dim, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(h[1], kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(y_dim, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tic = time.clock()\n",
    "model.fit_generator(generator=dg_train(all_train_data_addr, train_label_addr, mini_batch_size, batch_size, train_data_len, window_size),\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                          epochs=epochs_Nu,\n",
    "                          verbose=1)\n",
    "# model.fit(x=np.transpose(train_data), y=np.transpose(train_label), batch_size=128, epochs=50, verbose=1, shuffle=True)\n",
    "toc = time.clock()\n",
    "print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "\n",
    "model.save(vad_model_save_addr + '55_MRCG96_win10_batch512_mini8_e10_1024sigmoid_512sigmoid.h5')\n",
    "\n",
    "print ('done!')\n",
    "\n",
    "# \n",
    "# model = Sequential()\n",
    "# model.add(LSTM(n_units, input_shape=(None, vec_size), return_sequences=True))\n",
    "# model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] data generator for auto encoder version 1 date 3/27\n",
    "def dg_train_ae(all_noisy_train_data_addr, clean_train_data_addr, batch_size, train_data_len, window_size):\n",
    "    global I\n",
    "    h5f = {}\n",
    "    for i in range(len(all_noisy_train_data_addr)):\n",
    "        h5f[i] = h5py.File(all_noisy_train_data_addr[i],'r')\n",
    "#         print(h5f[i]['train_data'].shape)\n",
    "    h5f_clean_train_data = h5py.File(clean_train_data_addr,'r')\n",
    "    \n",
    "    indx = np.arange(0, int(train_data_len/len(all_noisy_train_data_addr))-(mini_batch_size+1), mini_batch_size)\n",
    "    file_Nu = list(range(len(all_noisy_train_data_addr)))\n",
    "    all_data_mini_batch_flag = np.asarray([[n, i] for n in file_Nu for i in indx])\n",
    "    all_data_mini_batch_flag = shuffle(all_data_mini_batch_flag, random_state=0)\n",
    "#     print(all_data_mini_batch_flag.shape)\n",
    "    \n",
    "    while True:             #this line is just because keras needs infinite generators\n",
    "        first_mini_batch = 'True'\n",
    "        for I in all_data_mini_batch_flag:\n",
    "            if train_data_len-I[1]<=mini_batch_size or I[1]<=mini_batch_size:\n",
    "                I[1] = copy.deepcopy(mini_batch_size)\n",
    "            temp = h5f[I[0]]['train_data'][:,I[1]-window_size:I[1]+mini_batch_size+window_size]\n",
    "            temp2 = h5f_clean_train_data['train_data'][:,I[1]-window_size:I[1]+mini_batch_size+window_size]\n",
    "            if first_mini_batch == 'True':\n",
    "                a = np.reshape(temp[:,:window_size*2+1], (96*(window_size*2+1),1))\n",
    "                in_data = (a - np.min(a)) / (np.max(a) - np.min(a)+0.0001)\n",
    "                b = np.reshape(temp2[:,:window_size*2+1], (96*(window_size*2+1),1))\n",
    "                out_data = (b - np.min(b)) / (np.max(b) - np.min(b)+0.0001)\n",
    "                for i in range(1,mini_batch_size):\n",
    "                    a = np.reshape(temp[:,i:i+window_size*2+1],(96*(window_size*2+1),1))\n",
    "                    in_data = np.concatenate((in_data,(a - np.min(a)) / (np.max(a) - np.min(a))+0.0001),axis = 1)\n",
    "                    b = np.reshape(temp2[:,i:i+window_size*2+1],(96*(window_size*2+1),1))\n",
    "                    out_data = np.concatenate((out_data,(b - np.min(b)) / (max(b) - np.min(b))+0.0001),axis = 1)\n",
    "                    \n",
    "                first_mini_batch = 'False'\n",
    "            else:\n",
    "                for i in range(0,mini_batch_size):\n",
    "                    a = np.reshape(temp[:,i:i+window_size*2+1],(96*(window_size*2+1),1))\n",
    "                    in_data = np.concatenate((in_data,(a - np.min(a)) / (np.max(a) - np.min(a))+0.0001),axis = 1)\n",
    "                    b = np.reshape(temp[:,i:i+window_size*2+1],(96*(window_size*2+1),1))\n",
    "                    out_data = np.concatenate((out_data,(b - np.min(b)) / (np.max(b) - np.min(b))+0.0001),axis = 1)\n",
    "#                     print(np.min(in_data))\n",
    "#                     print(np.min(out_data))\n",
    "            if in_data.shape[1] == batch_size and out_data.shape[1] == batch_size:            \n",
    "#                 preprocessing.normalize(train_data_batch,norm='l2',)\n",
    "#                 in_data_scale = preprocessing.scale(in_data.T)\n",
    "#                 out_data_scale = preprocessing.scale(out_data.T)\n",
    "                in_data_scale =in_data.T\n",
    "                out_data_scale = out_data.T\n",
    "#                 print(train_label_batch.shape)\n",
    "#                 print(train_data_batch[:,1])\n",
    "                first_mini_batch = 'True'\n",
    "#                 print(in_data_scale)\n",
    "#                 print(out_data_scale)\n",
    "#                 print(np.argwhere(np.isnan(in_data_scale)))\n",
    "#                 print(np.argwhere(np.isnan(out_data_scale)))\n",
    "                yield(in_data_scale, out_data_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noisy_train_data_addr = [train_data_addr, train_data_addr2]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 64\n",
    "batch_size = 128\n",
    "window_size = 10\n",
    "q,w = next(dg_train_ae(all_noisy_train_data_addr, clean_train_data_addr, batch_size, train_data_len, window_size))\n",
    "print(q)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae\n",
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 128\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 50\n",
    "ae_structure = [96*(window_size*2+1), 512, 512, 64, 512, 512, 96*(window_size*2+1)]\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(ae_structure[0],))\n",
    "encoder_1 = Dense(ae_structure[1], activation='relu')(input_layer)\n",
    "encoder_2 = Dense(ae_structure[2], activation='relu')(encoder_1)\n",
    "code = Dense(ae_structure[3], activation='relu')(encoder_2)\n",
    "decoder_1 = Dense(ae_structure[4], activation='relu')(code)\n",
    "decoder_2 = Dense(ae_structure[5], activation='relu')(decoder_1)\n",
    "output_layer = Dense(ae_structure[6], activation='sigmoid')(decoder_2)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mape')\n",
    "# autoencoder.fit(train_data.T, train_silence_add_clean.T, epochs=5)\n",
    "autoencoder.fit_generator(dg_train_ae(all_noisy_train_data_addr, clean_train_data_addr, batch_size, train_data_len, window_size),\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=epochs_Nu,\n",
    "                              verbose=1)\n",
    "autoencoder.save(enhance_model_save_addr + '55_MRCG96_win10_batch512_mini8_e10_1024sigmoid_512sigmoid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-40acf1bd8859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_Nu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                               verbose=1)\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menhance_model_save_addr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'55_MRCG96_win10_batch512_mini8_e10_1024sigmoid_512sigmoid.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[0;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m                         loss=self.total_loss)\n\u001b[0m\u001b[0;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[0;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             raise ValueError('An operation has `None` for gradient. '\n\u001b[0m\u001b[0;32m     92\u001b[0m                              \u001b[1;34m'Please make sure that all of your ops have a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                              \u001b[1;34m'gradient defined (i.e. are differentiable). '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval."
     ]
    }
   ],
   "source": [
    "#vae\n",
    "all_noisy_train_data_addr = [train_data_addr]\n",
    "clean_train_data_addr = train_silence_add_clean_addr\n",
    "train_data_len = 1879087 * len(all_noisy_train_data_addr)\n",
    "# train_data_len = 1417087*2\n",
    "mini_batch_size = 128\n",
    "batch_size = 128\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "epochs_Nu = 50\n",
    "vae_structure = [96*(window_size*2+1), 512, 512, 64, 512, 512, 96*(window_size*2+1)]\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(vae_structure[0],))\n",
    "encoder_1 = Dense(vae_structure[1], activation='relu')(input_layer)\n",
    "encoder_2 = Dense(vae_structure[2], activation='relu')(encoder_1)\n",
    "z_mean = Dense(vae_structure[3], name='z_mean')(encoder_2)\n",
    "z_log_var = Dense(vae_structure[3], name='z_log_var')(encoder_2)\n",
    "z = Lambda(sampling, output_shape=(vae_structure[3],), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(input_layer, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "latent_inputs = Input(shape=(vae_structure[3],), name='z_sampling')\n",
    "decoder_1 = Dense(vae_structure[4], activation='relu')(latent_inputs)\n",
    "decoder_2 = Dense(vae_structure[5], activation='relu')(decoder_1)\n",
    "x_mean = Dense(vae_structure[6], name='x_mean')(decoder_2)\n",
    "x_log_var = Dense(vae_structure[6], name='x_log_var')(decoder_2)\n",
    "x = Lambda(sampling, output_shape=(vae_structure[6],), name='x')([x_mean, x_log_var])\n",
    "# output_layer = Dense(vae_structure[6], activation='sigmoid')(x)\n",
    "decoder = Model(latent_inputs, [x_mean, x_log_var, x], name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(input_layer)[2])[2]\n",
    "vae = Model(input_layer, outputs, name='vae_mlp')\n",
    "\n",
    "\n",
    "# models = (encoder, decoder)\n",
    "# [z_mean, z_log_var, z] = decoder.predict(data)\n",
    "# [x_mean, x_log_var, x] = encoder.predict(z)\n",
    "def custom_loss(args):\n",
    "    z_mean = args\n",
    "    def loss(y_true, y_pred):\n",
    "#         [z_mean, z_log_var, z] = decoder.predict(data)\n",
    "#         [x_mean, x_log_var, x] = encoder.predict(z)\n",
    "# 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)       \n",
    "        kl_loss = K.sum(z_mean, axis=-1) #- K.sum(0.5*x_log_var + K.square(x-x_mean)/(2*K.exp(x_log_var)), axis=-1)\n",
    "        return kl_loss\n",
    "    return loss\n",
    "\n",
    "# vae.add_loss(vae_loss)\n",
    "# vae.compile(optimizer='adam')\n",
    "vae.compile(optimizer='adam', loss=custom_loss(z_mean))\n",
    "# vae.summary()\n",
    "vae.fit_generator(dg_train_ae(all_noisy_train_data_addr, clean_train_data_addr, batch_size, train_data_len, window_size),\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=epochs_Nu,\n",
    "                              verbose=1)\n",
    "\n",
    "vae.save(enhance_model_save_addr + '55_MRCG96_win10_batch512_mini8_e10_1024sigmoid_512sigmoid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.fit_generator(dg_train_ae(all_noisy_train_data_addr, clean_train_data_addr, batch_size, train_data_len, window_size),\n",
    "#                               steps_per_epoch=steps_per_epoch,\n",
    "#                               epochs=epochs_Nu,\n",
    "#                               verbose=1)\n",
    "\n",
    "# vae.save(enhance_model_save_addr + '55_MRCG96_win10_batch512_mini8_e10_1024sigmoid_512sigmoid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint learning\n",
    "# vec_size = 100\n",
    "# n_units = 10\n",
    "\n",
    "# x_train = np.random.rand(500, 10, vec_size)\n",
    "# y_train = np.random.rand(500, vec_size)\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = enhance_model_save_addr + '1_MRCG96_win10_batch512_mini8_e10_1024sigmoid_512sigmoid.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "\n",
    "vae.fit_generator(dg_train_ae(all_noisy_train_data_addr, clean_train_data_addr, batch_size, train_data_len, window_size),\n",
    "                              callbacks=callbacks_list,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=epochs_Nu,\n",
    "                              verbose=1)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=50, callbacks=callbacks_list)\n",
    "\n",
    "# load the model\n",
    "new_model = load_model(\"model.h5\")\n",
    "assert_allclose(model.predict(x_train),\n",
    "                new_model.predict(x_train),\n",
    "                1e-5)\n",
    "\n",
    "# fit the model\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "new_model.fit(x_train, y_train, epochs=5, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] data generator for prediction version 1 date 3/6\n",
    "def dg_test(all_test_data_addr, batch_size, test_data_len, window_size):\n",
    "    global I\n",
    "    h5f = {}\n",
    "    for i in range(len(all_test_data_addr)):\n",
    "        h5f[i] = h5py.File(all_test_data_addr[i],'r')\n",
    "#         print(h5f[i]['train_data'].shape)\n",
    "#     h5f_label = h5py.File(test_label_addr,'r')\n",
    "    \n",
    "    indx = np.arange(0, int(test_data_len/len(all_test_data_addr)), batch_size)\n",
    "    file_Nu = list(range(len(all_test_data_addr)))\n",
    "    all_data_batch_flag = np.asarray([[n, i] for n in file_Nu for i in indx])\n",
    "#     all_data_batch_flag = shuffle(all_data_batch_flag, random_state=0)\n",
    "    print(all_data_batch_flag.shape)\n",
    "    \n",
    "    while True:             #this line is just because keras needs infinite generators\n",
    "        for I in all_data_batch_flag:\n",
    "            if test_data_len-I[1]<=batch_size+window_size or I[1]<=window_size:\n",
    "                I[1] = copy.deepcopy(batch_size)\n",
    "            else:\n",
    "                print(I)\n",
    "                # some file save data with the name 'train_data' and some of them save data with the name 'test_data'\n",
    "                # some name are wrong\n",
    "                temp = h5f[I[0]]['train_data'][:,I[1]-window_size:I[1]+batch_size+window_size]\n",
    "\n",
    "    #             y_int = h5f_label['train_label'][:,I[1]:I[1]+batch_size]\n",
    "\n",
    "                a = np.reshape(temp[:,:window_size*2+1], (96*(window_size*2+1),1))\n",
    "                for i in range(1,batch_size):\n",
    "                    b = np.reshape(temp[:,i:i+window_size*2+1],(96*(window_size*2+1),1))\n",
    "                    a = np.concatenate((a,b),axis = 1)\n",
    "\n",
    "                if a.shape[1] == batch_size:\n",
    "                    test_data_batch = a.T\n",
    "                    test_data_batch_scale = preprocessing.scale(test_data_batch)\n",
    "    #                 test_label_batch = y_int.T\n",
    "    #                 test_label_batch = to_categorical(test_label_batch)\n",
    "    #                 print(train_label_batch.shape)\n",
    "    #                 print(train_data_batch[:,1])\n",
    "    #                 if test_label_batch.shape[1]==2:\n",
    "                    yield(test_data_batch_scale)\n",
    "    #                 yield(train_data_scale, train_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2680, 2)\n",
      "[  0 256]\n",
      "(256, 2016)\n"
     ]
    }
   ],
   "source": [
    "all_test_data_addr = [test_data_addr]\n",
    "test_data_len = 685845 * len(all_test_data_addr)\n",
    "# test_data_len = ???\n",
    "batch_size = 256\n",
    "steps_per_epoch = int((train_data_len/batch_size))\n",
    "window_size = 10\n",
    "q = next(dg_test(all_test_data_addr, batch_size, test_data_len, window_size))\n",
    "print(q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2679\n"
     ]
    }
   ],
   "source": [
    "test_data_len = 685845 * len(all_test_data_addr)\n",
    "# test_data_len = ???\n",
    "batch_size = 256\n",
    "steps = int((test_data_len/batch_size))\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7341, 2)\n",
      "[  0 256]\n",
      "[  0 512]\n",
      "[  0 768]\n",
      "[   0 1024]\n",
      "[   0 1280]\n",
      "[   0 1536]\n",
      "[   0 1792]\n",
      "[   0 2048]\n",
      "[   0 2304]\n",
      "[   0 2560]\n",
      "[   0 2816]\n",
      "[   0 3072]\n",
      "[   0 3328]\n",
      "[   0 3584]\n",
      "[   0 3840]\n",
      "[   0 4096]\n",
      "[   0 4352]\n",
      "[   0 4608]\n",
      "[   0 4864]\n",
      "[   0 5120]\n",
      "[   0 5376]\n",
      "[   0 5632]\n",
      "[   0 5888]\n",
      "[   0 6144]\n",
      "[   0 6400]\n",
      "[   0 6656]\n",
      "[   0 6912]\n",
      "[   0 7168]\n",
      "[   0 7424]\n",
      "[   0 7680]\n",
      "[   0 7936]\n",
      "[   0 8192]\n",
      "[   0 8448]\n",
      "[   0 8704]\n",
      "[   0 8960]\n",
      "[   0 9216]\n",
      "[   0 9472]\n",
      "[   0 9728]\n",
      "[   0 9984]\n",
      "[    0 10240]\n",
      "[    0 10496]\n",
      "[    0 10752]\n",
      "[    0 11008]\n",
      "[    0 11264]\n",
      "[    0 11520]\n",
      "[    0 11776]\n",
      "[    0 12032]\n",
      "[    0 12288]\n",
      "[    0 12544]\n",
      "[    0 12800]\n",
      "[    0 13056]\n",
      "[    0 13312]\n",
      "[    0 13568]\n",
      "[    0 13824]\n",
      "[    0 14080]\n",
      "[    0 14336]\n",
      "[    0 14592]\n",
      "[    0 14848]\n",
      "[    0 15104]\n",
      "[    0 15360]\n",
      "[    0 15616]\n",
      "[    0 15872]\n",
      "[    0 16128]\n",
      "[    0 16384]\n",
      "[    0 16640]\n",
      "[    0 16896]\n",
      "[    0 17152]\n",
      "[    0 17408]\n",
      "[    0 17664]\n",
      "[    0 17920]\n",
      "[    0 18176]\n",
      "[    0 18432]\n",
      "[    0 18688]\n",
      "[    0 18944]\n",
      "[    0 19200]\n",
      "[    0 19456]\n",
      "[    0 19712]\n",
      "[    0 19968]\n",
      "[    0 20224]\n",
      "[    0 20480]\n",
      "[    0 20736]\n",
      "[    0 20992]\n",
      "[    0 21248]\n",
      "[    0 21504]\n",
      "[    0 21760]\n",
      "[    0 22016]\n",
      "[    0 22272]\n",
      "[    0 22528]\n",
      "[    0 22784]\n",
      "[    0 23040]\n",
      "[    0 23296]\n",
      "[    0 23552]\n",
      "[    0 23808]\n",
      "[    0 24064]\n",
      "[    0 24320]\n",
      "[    0 24576]\n",
      "[    0 24832]\n",
      "[    0 25088]\n",
      "[    0 25344]\n",
      "[    0 25600]\n",
      "[    0 25856]\n",
      "[    0 26112]\n",
      "[    0 26368]\n",
      "[    0 26624]\n",
      "[    0 26880]\n",
      "[    0 27136]\n",
      "[    0 27392]\n",
      "[    0 27648]\n",
      "[    0 27904]\n",
      "[    0 28160]\n",
      "[    0 28416]\n",
      "[    0 28672]\n",
      "[    0 28928]\n",
      "[    0 29184]\n",
      "[    0 29440]\n",
      "[    0 29696]\n",
      "[    0 29952]\n",
      "[    0 30208]\n",
      "[    0 30464]\n",
      "[    0 30720]\n",
      "[    0 30976]\n",
      "[    0 31232]\n",
      "[    0 31488]\n",
      "[    0 31744]\n",
      "[    0 32000]\n",
      "[    0 32256]\n",
      "[    0 32512]\n",
      "[    0 32768]\n",
      "[    0 33024]\n",
      "[    0 33280]\n",
      "[    0 33536]\n",
      "[    0 33792]\n",
      "[    0 34048]\n",
      "[    0 34304]\n",
      "[    0 34560]\n",
      "[    0 34816]\n",
      "[    0 35072]\n",
      "[    0 35328]\n",
      "[    0 35584]\n",
      "[    0 35840]\n",
      "[    0 36096]\n",
      "[    0 36352]\n",
      "[    0 36608]\n",
      "[    0 36864]\n",
      "[    0 37120]\n",
      "[    0 37376]\n",
      "[    0 37632]\n",
      "[    0 37888]\n",
      "[    0 38144]\n",
      "[    0 38400]\n",
      "[    0 38656]\n",
      "[    0 38912]\n",
      "[    0 39168]\n",
      "[    0 39424]\n",
      "[    0 39680]\n",
      "[    0 39936]\n",
      "[    0 40192]\n",
      "[    0 40448]\n",
      "[    0 40704]\n",
      "[    0 40960]\n",
      "[    0 41216]\n",
      "[    0 41472]\n",
      "[    0 41728]\n",
      "[    0 41984]\n",
      "[    0 42240]\n",
      "[    0 42496]\n",
      "[    0 42752]\n",
      "[    0 43008]\n",
      "[    0 43264]\n",
      "[    0 43520]\n",
      "[    0 43776]\n",
      "[    0 44032]\n",
      "[    0 44288]\n",
      "[    0 44544]\n",
      "[    0 44800]\n",
      "[    0 45056]\n",
      "[    0 45312]\n",
      "[    0 45568]\n",
      "[    0 45824]\n",
      "[    0 46080]\n",
      "[    0 46336]\n",
      "[    0 46592]\n",
      "[    0 46848]\n",
      "[    0 47104]\n",
      "[    0 47360]\n",
      "[    0 47616]\n",
      "[    0 47872]\n",
      "[    0 48128]\n",
      "[    0 48384]\n",
      "[    0 48640]\n",
      "[    0 48896]\n",
      "[    0 49152]\n",
      "[    0 49408]\n",
      "[    0 49664]\n",
      "[    0 49920]\n",
      "[    0 50176]\n",
      "[    0 50432]\n",
      "[    0 50688]\n",
      "[    0 50944]\n",
      "[    0 51200]\n",
      "[    0 51456]\n",
      "[    0 51712]\n",
      "[    0 51968]\n",
      "[    0 52224]\n",
      "[    0 52480]\n",
      "[    0 52736]\n",
      "[    0 52992]\n",
      "[    0 53248]\n",
      "[    0 53504]\n",
      "[    0 53760]\n",
      "[    0 54016]\n",
      "[    0 54272]\n",
      "[    0 54528]\n",
      "[    0 54784]\n",
      "[    0 55040]\n",
      "[    0 55296]\n",
      "[    0 55552]\n",
      "[    0 55808]\n",
      "[    0 56064]\n",
      "[    0 56320]\n",
      "[    0 56576]\n",
      "[    0 56832]\n",
      "[    0 57088]\n",
      "[    0 57344]\n",
      "[    0 57600]\n",
      "[    0 57856]\n",
      "[    0 58112]\n",
      "[    0 58368]\n",
      "[    0 58624]\n",
      "[    0 58880]\n",
      "[    0 59136]\n",
      "[    0 59392]\n",
      "[    0 59648]\n",
      "[    0 59904]\n",
      "[    0 60160]\n",
      "[    0 60416]\n",
      "[    0 60672]\n",
      "[    0 60928]\n",
      "[    0 61184]\n",
      "[    0 61440]\n",
      "[    0 61696]\n",
      "[    0 61952]\n",
      "[    0 62208]\n",
      "[    0 62464]\n",
      "[    0 62720]\n",
      "[    0 62976]\n",
      "[    0 63232]\n",
      "[    0 63488]\n",
      "[    0 63744]\n",
      "[    0 64000]\n",
      "[    0 64256]\n",
      "[    0 64512]\n",
      "[    0 64768]\n",
      "[    0 65024]\n",
      "[    0 65280]\n",
      "[    0 65536]\n",
      "[    0 65792]\n",
      "[    0 66048]\n",
      "[    0 66304]\n",
      "[    0 66560]\n",
      "[    0 66816]\n",
      "[    0 67072]\n",
      "[    0 67328]\n",
      "[    0 67584]\n",
      "[    0 67840]\n",
      "[    0 68096]\n",
      "[    0 68352]\n",
      "[    0 68608]\n",
      "[    0 68864]\n",
      "[    0 69120]\n",
      "[    0 69376]\n",
      "[    0 69632]\n",
      "[    0 69888]\n",
      "[    0 70144]\n",
      "[    0 70400]\n",
      "[    0 70656]\n",
      "[    0 70912]\n",
      "[    0 71168]\n",
      "[    0 71424]\n",
      "[    0 71680]\n",
      "[    0 71936]\n",
      "[    0 72192]\n",
      "[    0 72448]\n",
      "[    0 72704]\n",
      "[    0 72960]\n",
      "[    0 73216]\n",
      "[    0 73472]\n",
      "[    0 73728]\n",
      "[    0 73984]\n",
      "[    0 74240]\n",
      "[    0 74496]\n",
      "[    0 74752]\n",
      "[    0 75008]\n",
      "[    0 75264]\n",
      "[    0 75520]\n",
      "[    0 75776]\n",
      "[    0 76032]\n",
      "[    0 76288]\n",
      "[    0 76544]\n",
      "[    0 76800]\n",
      "[    0 77056]\n",
      "[    0 77312]\n",
      "[    0 77568]\n",
      "[    0 77824]\n",
      "[    0 78080]\n",
      "[    0 78336]\n",
      "[    0 78592]\n",
      "[    0 78848]\n",
      "[    0 79104]\n",
      "[    0 79360]\n",
      "[    0 79616]\n",
      "[    0 79872]\n",
      "[    0 80128]\n",
      "[    0 80384]\n",
      "[    0 80640]\n",
      "[    0 80896]\n",
      "[    0 81152]\n",
      "[    0 81408]\n",
      "[    0 81664]\n",
      "[    0 81920]\n",
      "[    0 82176]\n",
      "[    0 82432]\n",
      "[    0 82688]\n",
      "[    0 82944]\n",
      "[    0 83200]\n",
      "[    0 83456]\n",
      "[    0 83712]\n",
      "[    0 83968]\n",
      "[    0 84224]\n",
      "[    0 84480]\n",
      "[    0 84736]\n",
      "[    0 84992]\n",
      "[    0 85248]\n",
      "[    0 85504]\n",
      "[    0 85760]\n",
      "[    0 86016]\n",
      "[    0 86272]\n",
      "[    0 86528]\n",
      "[    0 86784]\n",
      "[    0 87040]\n",
      "[    0 87296]\n",
      "[    0 87552]\n",
      "[    0 87808]\n",
      "[    0 88064]\n",
      "[    0 88320]\n",
      "[    0 88576]\n",
      "[    0 88832]\n",
      "[    0 89088]\n",
      "[    0 89344]\n",
      "[    0 89600]\n",
      "[    0 89856]\n",
      "[    0 90112]\n",
      "[    0 90368]\n",
      "[    0 90624]\n",
      "[    0 90880]\n",
      "[    0 91136]\n",
      "[    0 91392]\n",
      "[    0 91648]\n",
      "[    0 91904]\n",
      "[    0 92160]\n",
      "[    0 92416]\n",
      "[    0 92672]\n",
      "[    0 92928]\n",
      "[    0 93184]\n",
      "[    0 93440]\n",
      "[    0 93696]\n",
      "[    0 93952]\n",
      "[    0 94208]\n",
      "[    0 94464]\n",
      "[    0 94720]\n",
      "[    0 94976]\n",
      "[    0 95232]\n",
      "[    0 95488]\n",
      "[    0 95744]\n",
      "[    0 96000]\n",
      "[    0 96256]\n",
      "[    0 96512]\n",
      "[    0 96768]\n",
      "[    0 97024]\n",
      "[    0 97280]\n",
      "[    0 97536]\n",
      "[    0 97792]\n",
      "[    0 98048]\n",
      "[    0 98304]\n",
      "[    0 98560]\n",
      "[    0 98816]\n",
      "[    0 99072]\n",
      "[    0 99328]\n",
      "[    0 99584]\n",
      "[    0 99840]\n",
      "[     0 100096]\n",
      "[     0 100352]\n",
      "[     0 100608]\n",
      "[     0 100864]\n",
      "[     0 101120]\n",
      "[     0 101376]\n",
      "[     0 101632]\n",
      "[     0 101888]\n",
      "[     0 102144]\n",
      "[     0 102400]\n",
      "[     0 102656]\n",
      "[     0 102912]\n",
      "[     0 103168]\n",
      "[     0 103424]\n",
      "[     0 103680]\n",
      "[     0 103936]\n",
      "[     0 104192]\n",
      "[     0 104448]\n",
      "[     0 104704]\n",
      "[     0 104960]\n",
      "[     0 105216]\n",
      "[     0 105472]\n",
      "[     0 105728]\n",
      "[     0 105984]\n",
      "[     0 106240]\n",
      "[     0 106496]\n",
      "[     0 106752]\n",
      "[     0 107008]\n",
      "[     0 107264]\n",
      "[     0 107520]\n",
      "[     0 107776]\n",
      "[     0 108032]\n",
      "[     0 108288]\n",
      "[     0 108544]\n",
      "[     0 108800]\n",
      "[     0 109056]\n",
      "[     0 109312]\n",
      "[     0 109568]\n",
      "[     0 109824]\n",
      "[     0 110080]\n",
      "[     0 110336]\n",
      "[     0 110592]\n",
      "[     0 110848]\n",
      "[     0 111104]\n",
      "[     0 111360]\n",
      "[     0 111616]\n",
      "[     0 111872]\n",
      "[     0 112128]\n",
      "[     0 112384]\n",
      "[     0 112640]\n",
      "[     0 112896]\n",
      "[     0 113152]\n",
      "[     0 113408]\n",
      "[     0 113664]\n",
      "[     0 113920]\n",
      "[     0 114176]\n",
      "[     0 114432]\n",
      "[     0 114688]\n",
      "[     0 114944]\n",
      "[     0 115200]\n",
      "[     0 115456]\n",
      "[     0 115712]\n",
      "[     0 115968]\n",
      "[     0 116224]\n",
      "[     0 116480]\n",
      "[     0 116736]\n",
      "[     0 116992]\n",
      "[     0 117248]\n",
      "[     0 117504]\n",
      "[     0 117760]\n",
      "[     0 118016]\n",
      "[     0 118272]\n",
      "[     0 118528]\n",
      "[     0 118784]\n",
      "[     0 119040]\n",
      "[     0 119296]\n",
      "[     0 119552]\n",
      "[     0 119808]\n",
      "[     0 120064]\n",
      "[     0 120320]\n",
      "[     0 120576]\n",
      "[     0 120832]\n",
      "[     0 121088]\n",
      "[     0 121344]\n",
      "[     0 121600]\n",
      "[     0 121856]\n",
      "[     0 122112]\n",
      "[     0 122368]\n",
      "[     0 122624]\n",
      "[     0 122880]\n",
      "[     0 123136]\n",
      "[     0 123392]\n",
      "[     0 123648]\n",
      "[     0 123904]\n",
      "[     0 124160]\n",
      "[     0 124416]\n",
      "[     0 124672]\n",
      "[     0 124928]\n",
      "[     0 125184]\n",
      "[     0 125440]\n",
      "[     0 125696]\n",
      "[     0 125952]\n",
      "[     0 126208]\n",
      "[     0 126464]\n",
      "[     0 126720]\n",
      "[     0 126976]\n",
      "[     0 127232]\n",
      "[     0 127488]\n",
      "[     0 127744]\n",
      "[     0 128000]\n",
      "[     0 128256]\n",
      "[     0 128512]\n",
      "[     0 128768]\n",
      "[     0 129024]\n",
      "[     0 129280]\n",
      "[     0 129536]\n",
      "[     0 129792]\n",
      "[     0 130048]\n",
      "[     0 130304]\n",
      "[     0 130560]\n",
      "[     0 130816]\n",
      "[     0 131072]\n",
      "[     0 131328]\n",
      "[     0 131584]\n",
      "[     0 131840]\n",
      "[     0 132096]\n",
      "[     0 132352]\n",
      "[     0 132608]\n",
      "[     0 132864]\n",
      "[     0 133120]\n",
      "[     0 133376]\n",
      "[     0 133632]\n",
      "[     0 133888]\n",
      "[     0 134144]\n",
      "[     0 134400]\n",
      "[     0 134656]\n",
      "[     0 134912]\n",
      "[     0 135168]\n",
      "[     0 135424]\n",
      "[     0 135680]\n",
      "[     0 135936]\n",
      "[     0 136192]\n",
      "[     0 136448]\n",
      "[     0 136704]\n",
      "[     0 136960]\n",
      "[     0 137216]\n",
      "[     0 137472]\n",
      "[     0 137728]\n",
      "[     0 137984]\n",
      "[     0 138240]\n",
      "[     0 138496]\n",
      "[     0 138752]\n",
      "[     0 139008]\n",
      "[     0 139264]\n",
      "[     0 139520]\n",
      "[     0 139776]\n",
      "[     0 140032]\n",
      "[     0 140288]\n",
      "[     0 140544]\n",
      "[     0 140800]\n",
      "[     0 141056]\n",
      "[     0 141312]\n",
      "[     0 141568]\n",
      "[     0 141824]\n",
      "[     0 142080]\n",
      "[     0 142336]\n",
      "[     0 142592]\n",
      "[     0 142848]\n",
      "[     0 143104]\n",
      "[     0 143360]\n",
      "[     0 143616]\n",
      "[     0 143872]\n",
      "[     0 144128]\n",
      "[     0 144384]\n",
      "[     0 144640]\n",
      "[     0 144896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 145152]\n",
      "[     0 145408]\n",
      "[     0 145664]\n",
      "[     0 145920]\n",
      "[     0 146176]\n",
      "[     0 146432]\n",
      "[     0 146688]\n",
      "[     0 146944]\n",
      "[     0 147200]\n",
      "[     0 147456]\n",
      "[     0 147712]\n",
      "[     0 147968]\n",
      "[     0 148224]\n",
      "[     0 148480]\n",
      "[     0 148736]\n",
      "[     0 148992]\n",
      "[     0 149248]\n",
      "[     0 149504]\n",
      "[     0 149760]\n",
      "[     0 150016]\n",
      "[     0 150272]\n",
      "[     0 150528]\n",
      "[     0 150784]\n",
      "[     0 151040]\n",
      "[     0 151296]\n",
      "[     0 151552]\n",
      "[     0 151808]\n",
      "[     0 152064]\n",
      "[     0 152320]\n",
      "[     0 152576]\n",
      "[     0 152832]\n",
      "[     0 153088]\n",
      "[     0 153344]\n",
      "[     0 153600]\n",
      "[     0 153856]\n",
      "[     0 154112]\n",
      "[     0 154368]\n",
      "[     0 154624]\n",
      "[     0 154880]\n",
      "[     0 155136]\n",
      "[     0 155392]\n",
      "[     0 155648]\n",
      "[     0 155904]\n",
      "[     0 156160]\n",
      "[     0 156416]\n",
      "[     0 156672]\n",
      "[     0 156928]\n",
      "[     0 157184]\n",
      "[     0 157440]\n",
      "[     0 157696]\n",
      "[     0 157952]\n",
      "[     0 158208]\n",
      "[     0 158464]\n",
      "[     0 158720]\n",
      "[     0 158976]\n",
      "[     0 159232]\n",
      "[     0 159488]\n",
      "[     0 159744]\n",
      "[     0 160000]\n",
      "[     0 160256]\n",
      "[     0 160512]\n",
      "[     0 160768]\n",
      "[     0 161024]\n",
      "[     0 161280]\n",
      "[     0 161536]\n",
      "[     0 161792]\n",
      "[     0 162048]\n",
      "[     0 162304]\n",
      "[     0 162560]\n",
      "[     0 162816]\n",
      "[     0 163072]\n",
      "[     0 163328]\n",
      "[     0 163584]\n",
      "[     0 163840]\n",
      "[     0 164096]\n",
      "[     0 164352]\n",
      "[     0 164608]\n",
      "[     0 164864]\n",
      "[     0 165120]\n",
      "[     0 165376]\n",
      "[     0 165632]\n",
      "[     0 165888]\n",
      "[     0 166144]\n",
      "[     0 166400]\n",
      "[     0 166656]\n",
      "[     0 166912]\n",
      "[     0 167168]\n",
      "[     0 167424]\n",
      "[     0 167680]\n",
      "[     0 167936]\n",
      "[     0 168192]\n",
      "[     0 168448]\n",
      "[     0 168704]\n",
      "[     0 168960]\n",
      "[     0 169216]\n",
      "[     0 169472]\n",
      "[     0 169728]\n",
      "[     0 169984]\n",
      "[     0 170240]\n",
      "[     0 170496]\n",
      "[     0 170752]\n",
      "[     0 171008]\n",
      "[     0 171264]\n",
      "[     0 171520]\n",
      "[     0 171776]\n",
      "[     0 172032]\n",
      "[     0 172288]\n",
      "[     0 172544]\n",
      "[     0 172800]\n",
      "[     0 173056]\n",
      "[     0 173312]\n",
      "[     0 173568]\n",
      "[     0 173824]\n",
      "[     0 174080]\n",
      "[     0 174336]\n",
      "[     0 174592]\n",
      "[     0 174848]\n",
      "[     0 175104]\n",
      "[     0 175360]\n",
      "[     0 175616]\n",
      "[     0 175872]\n",
      "[     0 176128]\n",
      "[     0 176384]\n",
      "[     0 176640]\n",
      "[     0 176896]\n",
      "[     0 177152]\n",
      "[     0 177408]\n",
      "[     0 177664]\n",
      "[     0 177920]\n",
      "[     0 178176]\n",
      "[     0 178432]\n",
      "[     0 178688]\n",
      "[     0 178944]\n",
      "[     0 179200]\n",
      "[     0 179456]\n",
      "[     0 179712]\n",
      "[     0 179968]\n",
      "[     0 180224]\n",
      "[     0 180480]\n",
      "[     0 180736]\n",
      "[     0 180992]\n",
      "[     0 181248]\n",
      "[     0 181504]\n",
      "[     0 181760]\n",
      "[     0 182016]\n",
      "[     0 182272]\n",
      "[     0 182528]\n",
      "[     0 182784]\n",
      "[     0 183040]\n",
      "[     0 183296]\n",
      "[     0 183552]\n",
      "[     0 183808]\n",
      "[     0 184064]\n",
      "[     0 184320]\n",
      "[     0 184576]\n",
      "[     0 184832]\n",
      "[     0 185088]\n",
      "[     0 185344]\n",
      "[     0 185600]\n",
      "[     0 185856]\n",
      "[     0 186112]\n",
      "[     0 186368]\n",
      "[     0 186624]\n",
      "[     0 186880]\n",
      "[     0 187136]\n",
      "[     0 187392]\n",
      "[     0 187648]\n",
      "[     0 187904]\n",
      "[     0 188160]\n",
      "[     0 188416]\n",
      "[     0 188672]\n",
      "[     0 188928]\n",
      "[     0 189184]\n",
      "[     0 189440]\n",
      "[     0 189696]\n",
      "[     0 189952]\n",
      "[     0 190208]\n",
      "[     0 190464]\n",
      "[     0 190720]\n",
      "[     0 190976]\n",
      "[     0 191232]\n",
      "[     0 191488]\n",
      "[     0 191744]\n",
      "[     0 192000]\n",
      "[     0 192256]\n",
      "[     0 192512]\n",
      "[     0 192768]\n",
      "[     0 193024]\n",
      "[     0 193280]\n",
      "[     0 193536]\n",
      "[     0 193792]\n",
      "[     0 194048]\n",
      "[     0 194304]\n",
      "[     0 194560]\n",
      "[     0 194816]\n",
      "[     0 195072]\n",
      "[     0 195328]\n",
      "[     0 195584]\n",
      "[     0 195840]\n",
      "[     0 196096]\n",
      "[     0 196352]\n",
      "[     0 196608]\n",
      "[     0 196864]\n",
      "[     0 197120]\n",
      "[     0 197376]\n",
      "[     0 197632]\n",
      "[     0 197888]\n",
      "[     0 198144]\n",
      "[     0 198400]\n",
      "[     0 198656]\n",
      "[     0 198912]\n",
      "[     0 199168]\n",
      "[     0 199424]\n",
      "[     0 199680]\n",
      "[     0 199936]\n",
      "[     0 200192]\n",
      "[     0 200448]\n",
      "[     0 200704]\n",
      "[     0 200960]\n",
      "[     0 201216]\n",
      "[     0 201472]\n",
      "[     0 201728]\n",
      "[     0 201984]\n",
      "[     0 202240]\n",
      "[     0 202496]\n",
      "[     0 202752]\n",
      "[     0 203008]\n",
      "[     0 203264]\n",
      "[     0 203520]\n",
      "[     0 203776]\n",
      "[     0 204032]\n",
      "[     0 204288]\n",
      "[     0 204544]\n",
      "[     0 204800]\n",
      "[     0 205056]\n",
      "[     0 205312]\n",
      "[     0 205568]\n",
      "[     0 205824]\n",
      "[     0 206080]\n",
      "[     0 206336]\n",
      "[     0 206592]\n",
      "[     0 206848]\n",
      "[     0 207104]\n",
      "[     0 207360]\n",
      "[     0 207616]\n",
      "[     0 207872]\n",
      "[     0 208128]\n",
      "[     0 208384]\n",
      "[     0 208640]\n",
      "[     0 208896]\n",
      "[     0 209152]\n",
      "[     0 209408]\n",
      "[     0 209664]\n",
      "[     0 209920]\n",
      "[     0 210176]\n",
      "[     0 210432]\n",
      "[     0 210688]\n",
      "[     0 210944]\n",
      "[     0 211200]\n",
      "[     0 211456]\n",
      "[     0 211712]\n",
      "[     0 211968]\n",
      "[     0 212224]\n",
      "[     0 212480]\n",
      "[     0 212736]\n",
      "[     0 212992]\n",
      "[     0 213248]\n",
      "[     0 213504]\n",
      "[     0 213760]\n",
      "[     0 214016]\n",
      "[     0 214272]\n",
      "[     0 214528]\n",
      "[     0 214784]\n",
      "[     0 215040]\n",
      "[     0 215296]\n",
      "[     0 215552]\n",
      "[     0 215808]\n",
      "[     0 216064]\n",
      "[     0 216320]\n",
      "[     0 216576]\n",
      "[     0 216832]\n",
      "[     0 217088]\n",
      "[     0 217344]\n",
      "[     0 217600]\n",
      "[     0 217856]\n",
      "[     0 218112]\n",
      "[     0 218368]\n",
      "[     0 218624]\n",
      "[     0 218880]\n",
      "[     0 219136]\n",
      "[     0 219392]\n",
      "[     0 219648]\n",
      "[     0 219904]\n",
      "[     0 220160]\n",
      "[     0 220416]\n",
      "[     0 220672]\n",
      "[     0 220928]\n",
      "[     0 221184]\n",
      "[     0 221440]\n",
      "[     0 221696]\n",
      "[     0 221952]\n",
      "[     0 222208]\n",
      "[     0 222464]\n",
      "[     0 222720]\n",
      "[     0 222976]\n",
      "[     0 223232]\n",
      "[     0 223488]\n",
      "[     0 223744]\n",
      "[     0 224000]\n",
      "[     0 224256]\n",
      "[     0 224512]\n",
      "[     0 224768]\n",
      "[     0 225024]\n",
      "[     0 225280]\n",
      "[     0 225536]\n",
      "[     0 225792]\n",
      "[     0 226048]\n",
      "[     0 226304]\n",
      "[     0 226560]\n",
      "[     0 226816]\n",
      "[     0 227072]\n",
      "[     0 227328]\n",
      "[     0 227584]\n",
      "[     0 227840]\n",
      "[     0 228096]\n",
      "[     0 228352]\n",
      "[     0 228608]\n",
      "[     0 228864]\n",
      "[     0 229120]\n",
      "[     0 229376]\n",
      "[     0 229632]\n",
      "[     0 229888]\n",
      "[     0 230144]\n",
      "[     0 230400]\n",
      "[     0 230656]\n",
      "[     0 230912]\n",
      "[     0 231168]\n",
      "[     0 231424]\n",
      "[     0 231680]\n",
      "[     0 231936]\n",
      "[     0 232192]\n",
      "[     0 232448]\n",
      "[     0 232704]\n",
      "[     0 232960]\n",
      "[     0 233216]\n",
      "[     0 233472]\n",
      "[     0 233728]\n",
      "[     0 233984]\n",
      "[     0 234240]\n",
      "[     0 234496]\n",
      "[     0 234752]\n",
      "[     0 235008]\n",
      "[     0 235264]\n",
      "[     0 235520]\n",
      "[     0 235776]\n",
      "[     0 236032]\n",
      "[     0 236288]\n",
      "[     0 236544]\n",
      "[     0 236800]\n",
      "[     0 237056]\n",
      "[     0 237312]\n",
      "[     0 237568]\n",
      "[     0 237824]\n",
      "[     0 238080]\n",
      "[     0 238336]\n",
      "[     0 238592]\n",
      "[     0 238848]\n",
      "[     0 239104]\n",
      "[     0 239360]\n",
      "[     0 239616]\n",
      "[     0 239872]\n",
      "[     0 240128]\n",
      "[     0 240384]\n",
      "[     0 240640]\n",
      "[     0 240896]\n",
      "[     0 241152]\n",
      "[     0 241408]\n",
      "[     0 241664]\n",
      "[     0 241920]\n",
      "[     0 242176]\n",
      "[     0 242432]\n",
      "[     0 242688]\n",
      "[     0 242944]\n",
      "[     0 243200]\n",
      "[     0 243456]\n",
      "[     0 243712]\n",
      "[     0 243968]\n",
      "[     0 244224]\n",
      "[     0 244480]\n",
      "[     0 244736]\n",
      "[     0 244992]\n",
      "[     0 245248]\n",
      "[     0 245504]\n",
      "[     0 245760]\n",
      "[     0 246016]\n",
      "[     0 246272]\n",
      "[     0 246528]\n",
      "[     0 246784]\n",
      "[     0 247040]\n",
      "[     0 247296]\n",
      "[     0 247552]\n",
      "[     0 247808]\n",
      "[     0 248064]\n",
      "[     0 248320]\n",
      "[     0 248576]\n",
      "[     0 248832]\n",
      "[     0 249088]\n",
      "[     0 249344]\n",
      "[     0 249600]\n",
      "[     0 249856]\n",
      "[     0 250112]\n",
      "[     0 250368]\n",
      "[     0 250624]\n",
      "[     0 250880]\n",
      "[     0 251136]\n",
      "[     0 251392]\n",
      "[     0 251648]\n",
      "[     0 251904]\n",
      "[     0 252160]\n",
      "[     0 252416]\n",
      "[     0 252672]\n",
      "[     0 252928]\n",
      "[     0 253184]\n",
      "[     0 253440]\n",
      "[     0 253696]\n",
      "[     0 253952]\n",
      "[     0 254208]\n",
      "[     0 254464]\n",
      "[     0 254720]\n",
      "[     0 254976]\n",
      "[     0 255232]\n",
      "[     0 255488]\n",
      "[     0 255744]\n",
      "[     0 256000]\n",
      "[     0 256256]\n",
      "[     0 256512]\n",
      "[     0 256768]\n",
      "[     0 257024]\n",
      "[     0 257280]\n",
      "[     0 257536]\n",
      "[     0 257792]\n",
      "[     0 258048]\n",
      "[     0 258304]\n",
      "[     0 258560]\n",
      "[     0 258816]\n",
      "[     0 259072]\n",
      "[     0 259328]\n",
      "[     0 259584]\n",
      "[     0 259840]\n",
      "[     0 260096]\n",
      "[     0 260352]\n",
      "[     0 260608]\n",
      "[     0 260864]\n",
      "[     0 261120]\n",
      "[     0 261376]\n",
      "[     0 261632]\n",
      "[     0 261888]\n",
      "[     0 262144]\n",
      "[     0 262400]\n",
      "[     0 262656]\n",
      "[     0 262912]\n",
      "[     0 263168]\n",
      "[     0 263424]\n",
      "[     0 263680]\n",
      "[     0 263936]\n",
      "[     0 264192]\n",
      "[     0 264448]\n",
      "[     0 264704]\n",
      "[     0 264960]\n",
      "[     0 265216]\n",
      "[     0 265472]\n",
      "[     0 265728]\n",
      "[     0 265984]\n",
      "[     0 266240]\n",
      "[     0 266496]\n",
      "[     0 266752]\n",
      "[     0 267008]\n",
      "[     0 267264]\n",
      "[     0 267520]\n",
      "[     0 267776]\n",
      "[     0 268032]\n",
      "[     0 268288]\n",
      "[     0 268544]\n",
      "[     0 268800]\n",
      "[     0 269056]\n",
      "[     0 269312]\n",
      "[     0 269568]\n",
      "[     0 269824]\n",
      "[     0 270080]\n",
      "[     0 270336]\n",
      "[     0 270592]\n",
      "[     0 270848]\n",
      "[     0 271104]\n",
      "[     0 271360]\n",
      "[     0 271616]\n",
      "[     0 271872]\n",
      "[     0 272128]\n",
      "[     0 272384]\n",
      "[     0 272640]\n",
      "[     0 272896]\n",
      "[     0 273152]\n",
      "[     0 273408]\n",
      "[     0 273664]\n",
      "[     0 273920]\n",
      "[     0 274176]\n",
      "[     0 274432]\n",
      "[     0 274688]\n",
      "[     0 274944]\n",
      "[     0 275200]\n",
      "[     0 275456]\n",
      "[     0 275712]\n",
      "[     0 275968]\n",
      "[     0 276224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 276480]\n",
      "[     0 276736]\n",
      "[     0 276992]\n",
      "[     0 277248]\n",
      "[     0 277504]\n",
      "[     0 277760]\n",
      "[     0 278016]\n",
      "[     0 278272]\n",
      "[     0 278528]\n",
      "[     0 278784]\n",
      "[     0 279040]\n",
      "[     0 279296]\n",
      "[     0 279552]\n",
      "[     0 279808]\n",
      "[     0 280064]\n",
      "[     0 280320]\n",
      "[     0 280576]\n",
      "[     0 280832]\n",
      "[     0 281088]\n",
      "[     0 281344]\n",
      "[     0 281600]\n",
      "[     0 281856]\n",
      "[     0 282112]\n",
      "[     0 282368]\n",
      "[     0 282624]\n",
      "[     0 282880]\n",
      "[     0 283136]\n",
      "[     0 283392]\n",
      "[     0 283648]\n",
      "[     0 283904]\n",
      "[     0 284160]\n",
      "[     0 284416]\n",
      "[     0 284672]\n",
      "[     0 284928]\n",
      "[     0 285184]\n",
      "[     0 285440]\n",
      "[     0 285696]\n",
      "[     0 285952]\n",
      "[     0 286208]\n",
      "[     0 286464]\n",
      "[     0 286720]\n",
      "[     0 286976]\n",
      "[     0 287232]\n",
      "[     0 287488]\n",
      "[     0 287744]\n",
      "[     0 288000]\n",
      "[     0 288256]\n",
      "[     0 288512]\n",
      "[     0 288768]\n",
      "[     0 289024]\n",
      "[     0 289280]\n",
      "[     0 289536]\n",
      "[     0 289792]\n",
      "[     0 290048]\n",
      "[     0 290304]\n",
      "[     0 290560]\n",
      "[     0 290816]\n",
      "[     0 291072]\n",
      "[     0 291328]\n",
      "[     0 291584]\n",
      "[     0 291840]\n",
      "[     0 292096]\n",
      "[     0 292352]\n",
      "[     0 292608]\n",
      "[     0 292864]\n",
      "[     0 293120]\n",
      "[     0 293376]\n",
      "[     0 293632]\n",
      "[     0 293888]\n",
      "[     0 294144]\n",
      "[     0 294400]\n",
      "[     0 294656]\n",
      "[     0 294912]\n",
      "[     0 295168]\n",
      "[     0 295424]\n",
      "[     0 295680]\n",
      "[     0 295936]\n",
      "[     0 296192]\n",
      "[     0 296448]\n",
      "[     0 296704]\n",
      "[     0 296960]\n",
      "[     0 297216]\n",
      "[     0 297472]\n",
      "[     0 297728]\n",
      "[     0 297984]\n",
      "[     0 298240]\n",
      "[     0 298496]\n",
      "[     0 298752]\n",
      "[     0 299008]\n",
      "[     0 299264]\n",
      "[     0 299520]\n",
      "[     0 299776]\n",
      "[     0 300032]\n",
      "[     0 300288]\n",
      "[     0 300544]\n",
      "[     0 300800]\n",
      "[     0 301056]\n",
      "[     0 301312]\n",
      "[     0 301568]\n",
      "[     0 301824]\n",
      "[     0 302080]\n",
      "[     0 302336]\n",
      "[     0 302592]\n",
      "[     0 302848]\n",
      "[     0 303104]\n",
      "[     0 303360]\n",
      "[     0 303616]\n",
      "[     0 303872]\n",
      "[     0 304128]\n",
      "[     0 304384]\n",
      "[     0 304640]\n",
      "[     0 304896]\n",
      "[     0 305152]\n",
      "[     0 305408]\n",
      "[     0 305664]\n",
      "[     0 305920]\n",
      "[     0 306176]\n",
      "[     0 306432]\n",
      "[     0 306688]\n",
      "[     0 306944]\n",
      "[     0 307200]\n",
      "[     0 307456]\n",
      "[     0 307712]\n",
      "[     0 307968]\n",
      "[     0 308224]\n",
      "[     0 308480]\n",
      "[     0 308736]\n",
      "[     0 308992]\n",
      "[     0 309248]\n",
      "[     0 309504]\n",
      "[     0 309760]\n",
      "[     0 310016]\n",
      "[     0 310272]\n",
      "[     0 310528]\n",
      "[     0 310784]\n",
      "[     0 311040]\n",
      "[     0 311296]\n",
      "[     0 311552]\n",
      "[     0 311808]\n",
      "[     0 312064]\n",
      "[     0 312320]\n",
      "[     0 312576]\n",
      "[     0 312832]\n",
      "[     0 313088]\n",
      "[     0 313344]\n",
      "[     0 313600]\n",
      "[     0 313856]\n",
      "[     0 314112]\n",
      "[     0 314368]\n",
      "[     0 314624]\n",
      "[     0 314880]\n",
      "[     0 315136]\n",
      "[     0 315392]\n",
      "[     0 315648]\n",
      "[     0 315904]\n",
      "[     0 316160]\n",
      "[     0 316416]\n",
      "[     0 316672]\n",
      "[     0 316928]\n",
      "[     0 317184]\n",
      "[     0 317440]\n",
      "[     0 317696]\n",
      "[     0 317952]\n",
      "[     0 318208]\n",
      "[     0 318464]\n",
      "[     0 318720]\n",
      "[     0 318976]\n",
      "[     0 319232]\n",
      "[     0 319488]\n",
      "[     0 319744]\n",
      "[     0 320000]\n",
      "[     0 320256]\n",
      "[     0 320512]\n",
      "[     0 320768]\n",
      "[     0 321024]\n",
      "[     0 321280]\n",
      "[     0 321536]\n",
      "[     0 321792]\n",
      "[     0 322048]\n",
      "[     0 322304]\n",
      "[     0 322560]\n",
      "[     0 322816]\n",
      "[     0 323072]\n",
      "[     0 323328]\n",
      "[     0 323584]\n",
      "[     0 323840]\n",
      "[     0 324096]\n",
      "[     0 324352]\n",
      "[     0 324608]\n",
      "[     0 324864]\n",
      "[     0 325120]\n",
      "[     0 325376]\n",
      "[     0 325632]\n",
      "[     0 325888]\n",
      "[     0 326144]\n",
      "[     0 326400]\n",
      "[     0 326656]\n",
      "[     0 326912]\n",
      "[     0 327168]\n",
      "[     0 327424]\n",
      "[     0 327680]\n",
      "[     0 327936]\n",
      "[     0 328192]\n",
      "[     0 328448]\n",
      "[     0 328704]\n",
      "[     0 328960]\n",
      "[     0 329216]\n",
      "[     0 329472]\n",
      "[     0 329728]\n",
      "[     0 329984]\n",
      "[     0 330240]\n",
      "[     0 330496]\n",
      "[     0 330752]\n",
      "[     0 331008]\n",
      "[     0 331264]\n",
      "[     0 331520]\n",
      "[     0 331776]\n",
      "[     0 332032]\n",
      "[     0 332288]\n",
      "[     0 332544]\n",
      "[     0 332800]\n",
      "[     0 333056]\n",
      "[     0 333312]\n",
      "[     0 333568]\n",
      "[     0 333824]\n",
      "[     0 334080]\n",
      "[     0 334336]\n",
      "[     0 334592]\n",
      "[     0 334848]\n",
      "[     0 335104]\n",
      "[     0 335360]\n",
      "[     0 335616]\n",
      "[     0 335872]\n",
      "[     0 336128]\n",
      "[     0 336384]\n",
      "[     0 336640]\n",
      "[     0 336896]\n",
      "[     0 337152]\n",
      "[     0 337408]\n",
      "[     0 337664]\n",
      "[     0 337920]\n",
      "[     0 338176]\n",
      "[     0 338432]\n",
      "[     0 338688]\n",
      "[     0 338944]\n",
      "[     0 339200]\n",
      "[     0 339456]\n",
      "[     0 339712]\n",
      "[     0 339968]\n",
      "[     0 340224]\n",
      "[     0 340480]\n",
      "[     0 340736]\n",
      "[     0 340992]\n",
      "[     0 341248]\n",
      "[     0 341504]\n",
      "[     0 341760]\n",
      "[     0 342016]\n",
      "[     0 342272]\n",
      "[     0 342528]\n",
      "[     0 342784]\n",
      "[     0 343040]\n",
      "[     0 343296]\n",
      "[     0 343552]\n",
      "[     0 343808]\n",
      "[     0 344064]\n",
      "[     0 344320]\n",
      "[     0 344576]\n",
      "[     0 344832]\n",
      "[     0 345088]\n",
      "[     0 345344]\n",
      "[     0 345600]\n",
      "[     0 345856]\n",
      "[     0 346112]\n",
      "[     0 346368]\n",
      "[     0 346624]\n",
      "[     0 346880]\n",
      "[     0 347136]\n",
      "[     0 347392]\n",
      "[     0 347648]\n",
      "[     0 347904]\n",
      "[     0 348160]\n",
      "[     0 348416]\n",
      "[     0 348672]\n",
      "[     0 348928]\n",
      "[     0 349184]\n",
      "[     0 349440]\n",
      "[     0 349696]\n",
      "[     0 349952]\n",
      "[     0 350208]\n",
      "[     0 350464]\n",
      "[     0 350720]\n",
      "[     0 350976]\n",
      "[     0 351232]\n",
      "[     0 351488]\n",
      "[     0 351744]\n",
      "[     0 352000]\n",
      "[     0 352256]\n",
      "[     0 352512]\n",
      "[     0 352768]\n",
      "[     0 353024]\n",
      "[     0 353280]\n",
      "[     0 353536]\n",
      "[     0 353792]\n",
      "[     0 354048]\n",
      "[     0 354304]\n",
      "[     0 354560]\n",
      "[     0 354816]\n",
      "[     0 355072]\n",
      "[     0 355328]\n",
      "[     0 355584]\n",
      "[     0 355840]\n",
      "[     0 356096]\n",
      "[     0 356352]\n",
      "[     0 356608]\n",
      "[     0 356864]\n",
      "[     0 357120]\n",
      "[     0 357376]\n",
      "[     0 357632]\n",
      "[     0 357888]\n",
      "[     0 358144]\n",
      "[     0 358400]\n",
      "[     0 358656]\n",
      "[     0 358912]\n",
      "[     0 359168]\n",
      "[     0 359424]\n",
      "[     0 359680]\n",
      "[     0 359936]\n",
      "[     0 360192]\n",
      "[     0 360448]\n",
      "[     0 360704]\n",
      "[     0 360960]\n",
      "[     0 361216]\n",
      "[     0 361472]\n",
      "[     0 361728]\n",
      "[     0 361984]\n",
      "[     0 362240]\n",
      "[     0 362496]\n",
      "[     0 362752]\n",
      "[     0 363008]\n",
      "[     0 363264]\n",
      "[     0 363520]\n",
      "[     0 363776]\n",
      "[     0 364032]\n",
      "[     0 364288]\n",
      "[     0 364544]\n",
      "[     0 364800]\n",
      "[     0 365056]\n",
      "[     0 365312]\n",
      "[     0 365568]\n",
      "[     0 365824]\n",
      "[     0 366080]\n",
      "[     0 366336]\n",
      "[     0 366592]\n",
      "[     0 366848]\n",
      "[     0 367104]\n",
      "[     0 367360]\n",
      "[     0 367616]\n",
      "[     0 367872]\n",
      "[     0 368128]\n",
      "[     0 368384]\n",
      "[     0 368640]\n",
      "[     0 368896]\n",
      "[     0 369152]\n",
      "[     0 369408]\n",
      "[     0 369664]\n",
      "[     0 369920]\n",
      "[     0 370176]\n",
      "[     0 370432]\n",
      "[     0 370688]\n",
      "[     0 370944]\n",
      "[     0 371200]\n",
      "[     0 371456]\n",
      "[     0 371712]\n",
      "[     0 371968]\n",
      "[     0 372224]\n",
      "[     0 372480]\n",
      "[     0 372736]\n",
      "[     0 372992]\n",
      "[     0 373248]\n",
      "[     0 373504]\n",
      "[     0 373760]\n",
      "[     0 374016]\n",
      "[     0 374272]\n",
      "[     0 374528]\n",
      "[     0 374784]\n",
      "[     0 375040]\n",
      "[     0 375296]\n",
      "[     0 375552]\n",
      "[     0 375808]\n",
      "[     0 376064]\n",
      "[     0 376320]\n",
      "[     0 376576]\n",
      "[     0 376832]\n",
      "[     0 377088]\n",
      "[     0 377344]\n",
      "[     0 377600]\n",
      "[     0 377856]\n",
      "[     0 378112]\n",
      "[     0 378368]\n",
      "[     0 378624]\n",
      "[     0 378880]\n",
      "[     0 379136]\n",
      "[     0 379392]\n",
      "[     0 379648]\n",
      "[     0 379904]\n",
      "[     0 380160]\n",
      "[     0 380416]\n",
      "[     0 380672]\n",
      "[     0 380928]\n",
      "[     0 381184]\n",
      "[     0 381440]\n",
      "[     0 381696]\n",
      "[     0 381952]\n",
      "[     0 382208]\n",
      "[     0 382464]\n",
      "[     0 382720]\n",
      "[     0 382976]\n",
      "[     0 383232]\n",
      "[     0 383488]\n",
      "[     0 383744]\n",
      "[     0 384000]\n",
      "[     0 384256]\n",
      "[     0 384512]\n",
      "[     0 384768]\n",
      "[     0 385024]\n",
      "[     0 385280]\n",
      "[     0 385536]\n",
      "[     0 385792]\n",
      "[     0 386048]\n",
      "[     0 386304]\n",
      "[     0 386560]\n",
      "[     0 386816]\n",
      "[     0 387072]\n",
      "[     0 387328]\n",
      "[     0 387584]\n",
      "[     0 387840]\n",
      "[     0 388096]\n",
      "[     0 388352]\n",
      "[     0 388608]\n",
      "[     0 388864]\n",
      "[     0 389120]\n",
      "[     0 389376]\n",
      "[     0 389632]\n",
      "[     0 389888]\n",
      "[     0 390144]\n",
      "[     0 390400]\n",
      "[     0 390656]\n",
      "[     0 390912]\n",
      "[     0 391168]\n",
      "[     0 391424]\n",
      "[     0 391680]\n",
      "[     0 391936]\n",
      "[     0 392192]\n",
      "[     0 392448]\n",
      "[     0 392704]\n",
      "[     0 392960]\n",
      "[     0 393216]\n",
      "[     0 393472]\n",
      "[     0 393728]\n",
      "[     0 393984]\n",
      "[     0 394240]\n",
      "[     0 394496]\n",
      "[     0 394752]\n",
      "[     0 395008]\n",
      "[     0 395264]\n",
      "[     0 395520]\n",
      "[     0 395776]\n",
      "[     0 396032]\n",
      "[     0 396288]\n",
      "[     0 396544]\n",
      "[     0 396800]\n",
      "[     0 397056]\n",
      "[     0 397312]\n",
      "[     0 397568]\n",
      "[     0 397824]\n",
      "[     0 398080]\n",
      "[     0 398336]\n",
      "[     0 398592]\n",
      "[     0 398848]\n",
      "[     0 399104]\n",
      "[     0 399360]\n",
      "[     0 399616]\n",
      "[     0 399872]\n",
      "[     0 400128]\n",
      "[     0 400384]\n",
      "[     0 400640]\n",
      "[     0 400896]\n",
      "[     0 401152]\n",
      "[     0 401408]\n",
      "[     0 401664]\n",
      "[     0 401920]\n",
      "[     0 402176]\n",
      "[     0 402432]\n",
      "[     0 402688]\n",
      "[     0 402944]\n",
      "[     0 403200]\n",
      "[     0 403456]\n",
      "[     0 403712]\n",
      "[     0 403968]\n",
      "[     0 404224]\n",
      "[     0 404480]\n",
      "[     0 404736]\n",
      "[     0 404992]\n",
      "[     0 405248]\n",
      "[     0 405504]\n",
      "[     0 405760]\n",
      "[     0 406016]\n",
      "[     0 406272]\n",
      "[     0 406528]\n",
      "[     0 406784]\n",
      "[     0 407040]\n",
      "[     0 407296]\n",
      "[     0 407552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 407808]\n",
      "[     0 408064]\n",
      "[     0 408320]\n",
      "[     0 408576]\n",
      "[     0 408832]\n",
      "[     0 409088]\n",
      "[     0 409344]\n",
      "[     0 409600]\n",
      "[     0 409856]\n",
      "[     0 410112]\n",
      "[     0 410368]\n",
      "[     0 410624]\n",
      "[     0 410880]\n",
      "[     0 411136]\n",
      "[     0 411392]\n",
      "[     0 411648]\n",
      "[     0 411904]\n",
      "[     0 412160]\n",
      "[     0 412416]\n",
      "[     0 412672]\n",
      "[     0 412928]\n",
      "[     0 413184]\n",
      "[     0 413440]\n",
      "[     0 413696]\n",
      "[     0 413952]\n",
      "[     0 414208]\n",
      "[     0 414464]\n",
      "[     0 414720]\n",
      "[     0 414976]\n",
      "[     0 415232]\n",
      "[     0 415488]\n",
      "[     0 415744]\n",
      "[     0 416000]\n",
      "[     0 416256]\n",
      "[     0 416512]\n",
      "[     0 416768]\n",
      "[     0 417024]\n",
      "[     0 417280]\n",
      "[     0 417536]\n",
      "[     0 417792]\n",
      "[     0 418048]\n",
      "[     0 418304]\n",
      "[     0 418560]\n",
      "[     0 418816]\n",
      "[     0 419072]\n",
      "[     0 419328]\n",
      "[     0 419584]\n",
      "[     0 419840]\n",
      "[     0 420096]\n",
      "[     0 420352]\n",
      "[     0 420608]\n",
      "[     0 420864]\n",
      "[     0 421120]\n",
      "[     0 421376]\n",
      "[     0 421632]\n",
      "[     0 421888]\n",
      "[     0 422144]\n",
      "[     0 422400]\n",
      "[     0 422656]\n",
      "[     0 422912]\n",
      "[     0 423168]\n",
      "[     0 423424]\n",
      "[     0 423680]\n",
      "[     0 423936]\n",
      "[     0 424192]\n",
      "[     0 424448]\n",
      "[     0 424704]\n",
      "[     0 424960]\n",
      "[     0 425216]\n",
      "[     0 425472]\n",
      "[     0 425728]\n",
      "[     0 425984]\n",
      "[     0 426240]\n",
      "[     0 426496]\n",
      "[     0 426752]\n",
      "[     0 427008]\n",
      "[     0 427264]\n",
      "[     0 427520]\n",
      "[     0 427776]\n",
      "[     0 428032]\n",
      "[     0 428288]\n",
      "[     0 428544]\n",
      "[     0 428800]\n",
      "[     0 429056]\n",
      "[     0 429312]\n",
      "[     0 429568]\n",
      "[     0 429824]\n",
      "[     0 430080]\n",
      "[     0 430336]\n",
      "[     0 430592]\n",
      "[     0 430848]\n",
      "[     0 431104]\n",
      "[     0 431360]\n",
      "[     0 431616]\n",
      "[     0 431872]\n",
      "[     0 432128]\n",
      "[     0 432384]\n",
      "[     0 432640]\n",
      "[     0 432896]\n",
      "[     0 433152]\n",
      "[     0 433408]\n",
      "[     0 433664]\n",
      "[     0 433920]\n",
      "[     0 434176]\n",
      "[     0 434432]\n",
      "[     0 434688]\n",
      "[     0 434944]\n",
      "[     0 435200]\n",
      "[     0 435456]\n",
      "[     0 435712]\n",
      "[     0 435968]\n",
      "[     0 436224]\n",
      "[     0 436480]\n",
      "[     0 436736]\n",
      "[     0 436992]\n",
      "[     0 437248]\n",
      "[     0 437504]\n",
      "[     0 437760]\n",
      "[     0 438016]\n",
      "[     0 438272]\n",
      "[     0 438528]\n",
      "[     0 438784]\n",
      "[     0 439040]\n",
      "[     0 439296]\n",
      "[     0 439552]\n",
      "[     0 439808]\n",
      "[     0 440064]\n",
      "[     0 440320]\n",
      "[     0 440576]\n",
      "[     0 440832]\n",
      "[     0 441088]\n",
      "[     0 441344]\n",
      "[     0 441600]\n",
      "[     0 441856]\n",
      "[     0 442112]\n",
      "[     0 442368]\n",
      "[     0 442624]\n",
      "[     0 442880]\n",
      "[     0 443136]\n",
      "[     0 443392]\n",
      "[     0 443648]\n",
      "[     0 443904]\n",
      "[     0 444160]\n",
      "[     0 444416]\n",
      "[     0 444672]\n",
      "[     0 444928]\n",
      "[     0 445184]\n",
      "[     0 445440]\n",
      "[     0 445696]\n",
      "[     0 445952]\n",
      "[     0 446208]\n",
      "[     0 446464]\n",
      "[     0 446720]\n",
      "[     0 446976]\n",
      "[     0 447232]\n",
      "[     0 447488]\n",
      "[     0 447744]\n",
      "[     0 448000]\n",
      "[     0 448256]\n",
      "[     0 448512]\n",
      "[     0 448768]\n",
      "[     0 449024]\n",
      "[     0 449280]\n",
      "[     0 449536]\n",
      "[     0 449792]\n",
      "[     0 450048]\n",
      "[     0 450304]\n",
      "[     0 450560]\n",
      "[     0 450816]\n",
      "[     0 451072]\n",
      "[     0 451328]\n",
      "[     0 451584]\n",
      "[     0 451840]\n",
      "[     0 452096]\n",
      "[     0 452352]\n",
      "[     0 452608]\n",
      "[     0 452864]\n",
      "[     0 453120]\n",
      "[     0 453376]\n",
      "[     0 453632]\n",
      "[     0 453888]\n",
      "[     0 454144]\n",
      "[     0 454400]\n",
      "[     0 454656]\n",
      "[     0 454912]\n",
      "[     0 455168]\n",
      "[     0 455424]\n",
      "[     0 455680]\n",
      "[     0 455936]\n",
      "[     0 456192]\n",
      "[     0 456448]\n",
      "[     0 456704]\n",
      "[     0 456960]\n",
      "[     0 457216]\n",
      "[     0 457472]\n",
      "[     0 457728]\n",
      "[     0 457984]\n",
      "[     0 458240]\n",
      "[     0 458496]\n",
      "[     0 458752]\n",
      "[     0 459008]\n",
      "[     0 459264]\n",
      "[     0 459520]\n",
      "[     0 459776]\n",
      "[     0 460032]\n",
      "[     0 460288]\n",
      "[     0 460544]\n",
      "[     0 460800]\n",
      "[     0 461056]\n",
      "[     0 461312]\n",
      "[     0 461568]\n",
      "[     0 461824]\n",
      "[     0 462080]\n",
      "[     0 462336]\n",
      "[     0 462592]\n",
      "[     0 462848]\n",
      "[     0 463104]\n",
      "[     0 463360]\n",
      "[     0 463616]\n",
      "[     0 463872]\n",
      "[     0 464128]\n",
      "[     0 464384]\n",
      "[     0 464640]\n",
      "[     0 464896]\n",
      "[     0 465152]\n",
      "[     0 465408]\n",
      "[     0 465664]\n",
      "[     0 465920]\n",
      "[     0 466176]\n",
      "[     0 466432]\n",
      "[     0 466688]\n",
      "[     0 466944]\n",
      "[     0 467200]\n",
      "[     0 467456]\n",
      "[     0 467712]\n",
      "[     0 467968]\n",
      "[     0 468224]\n",
      "[     0 468480]\n",
      "[     0 468736]\n",
      "[     0 468992]\n",
      "[     0 469248]\n",
      "[     0 469504]\n",
      "[     0 469760]\n",
      "[     0 470016]\n",
      "[     0 470272]\n",
      "[     0 470528]\n",
      "[     0 470784]\n",
      "[     0 471040]\n",
      "[     0 471296]\n",
      "[     0 471552]\n",
      "[     0 471808]\n",
      "[     0 472064]\n",
      "[     0 472320]\n",
      "[     0 472576]\n",
      "[     0 472832]\n",
      "[     0 473088]\n",
      "[     0 473344]\n",
      "[     0 473600]\n",
      "[     0 473856]\n",
      "[     0 474112]\n",
      "[     0 474368]\n",
      "[     0 474624]\n",
      "[     0 474880]\n",
      "[     0 475136]\n",
      "[     0 475392]\n",
      "[     0 475648]\n",
      "[     0 475904]\n",
      "[     0 476160]\n",
      "[     0 476416]\n",
      "[     0 476672]\n",
      "[     0 476928]\n",
      "[     0 477184]\n",
      "[     0 477440]\n",
      "[     0 477696]\n",
      "[     0 477952]\n",
      "[     0 478208]\n",
      "[     0 478464]\n",
      "[     0 478720]\n",
      "[     0 478976]\n",
      "[     0 479232]\n",
      "[     0 479488]\n",
      "[     0 479744]\n",
      "[     0 480000]\n",
      "[     0 480256]\n",
      "[     0 480512]\n",
      "[     0 480768]\n",
      "[     0 481024]\n",
      "[     0 481280]\n",
      "[     0 481536]\n",
      "[     0 481792]\n",
      "[     0 482048]\n",
      "[     0 482304]\n",
      "[     0 482560]\n",
      "[     0 482816]\n",
      "[     0 483072]\n",
      "[     0 483328]\n",
      "[     0 483584]\n",
      "[     0 483840]\n",
      "[     0 484096]\n",
      "[     0 484352]\n",
      "[     0 484608]\n",
      "[     0 484864]\n",
      "[     0 485120]\n",
      "[     0 485376]\n",
      "[     0 485632]\n",
      "[     0 485888]\n",
      "[     0 486144]\n",
      "[     0 486400]\n",
      "[     0 486656]\n",
      "[     0 486912]\n",
      "[     0 487168]\n",
      "[     0 487424]\n",
      "[     0 487680]\n",
      "[     0 487936]\n",
      "[     0 488192]\n",
      "[     0 488448]\n",
      "[     0 488704]\n",
      "[     0 488960]\n",
      "[     0 489216]\n",
      "[     0 489472]\n",
      "[     0 489728]\n",
      "[     0 489984]\n",
      "[     0 490240]\n",
      "[     0 490496]\n",
      "[     0 490752]\n",
      "[     0 491008]\n",
      "[     0 491264]\n",
      "[     0 491520]\n",
      "[     0 491776]\n",
      "[     0 492032]\n",
      "[     0 492288]\n",
      "[     0 492544]\n",
      "[     0 492800]\n",
      "[     0 493056]\n",
      "[     0 493312]\n",
      "[     0 493568]\n",
      "[     0 493824]\n",
      "[     0 494080]\n",
      "[     0 494336]\n",
      "[     0 494592]\n",
      "[     0 494848]\n",
      "[     0 495104]\n",
      "[     0 495360]\n",
      "[     0 495616]\n",
      "[     0 495872]\n",
      "[     0 496128]\n",
      "[     0 496384]\n",
      "[     0 496640]\n",
      "[     0 496896]\n",
      "[     0 497152]\n",
      "[     0 497408]\n",
      "[     0 497664]\n",
      "[     0 497920]\n",
      "[     0 498176]\n",
      "[     0 498432]\n",
      "[     0 498688]\n",
      "[     0 498944]\n",
      "[     0 499200]\n",
      "[     0 499456]\n",
      "[     0 499712]\n",
      "[     0 499968]\n",
      "[     0 500224]\n",
      "[     0 500480]\n",
      "[     0 500736]\n",
      "[     0 500992]\n",
      "[     0 501248]\n",
      "[     0 501504]\n",
      "[     0 501760]\n",
      "[     0 502016]\n",
      "[     0 502272]\n",
      "[     0 502528]\n",
      "[     0 502784]\n",
      "[     0 503040]\n",
      "[     0 503296]\n",
      "[     0 503552]\n",
      "[     0 503808]\n",
      "[     0 504064]\n",
      "[     0 504320]\n",
      "[     0 504576]\n",
      "[     0 504832]\n",
      "[     0 505088]\n",
      "[     0 505344]\n",
      "[     0 505600]\n",
      "[     0 505856]\n",
      "[     0 506112]\n",
      "[     0 506368]\n",
      "[     0 506624]\n",
      "[     0 506880]\n",
      "[     0 507136]\n",
      "[     0 507392]\n",
      "[     0 507648]\n",
      "[     0 507904]\n",
      "[     0 508160]\n",
      "[     0 508416]\n",
      "[     0 508672]\n",
      "[     0 508928]\n",
      "[     0 509184]\n",
      "[     0 509440]\n",
      "[     0 509696]\n",
      "[     0 509952]\n",
      "[     0 510208]\n",
      "[     0 510464]\n",
      "[     0 510720]\n",
      "[     0 510976]\n",
      "[     0 511232]\n",
      "[     0 511488]\n",
      "[     0 511744]\n",
      "[     0 512000]\n",
      "[     0 512256]\n",
      "[     0 512512]\n",
      "[     0 512768]\n",
      "[     0 513024]\n",
      "[     0 513280]\n",
      "[     0 513536]\n",
      "[     0 513792]\n",
      "[     0 514048]\n",
      "[     0 514304]\n",
      "[     0 514560]\n",
      "[     0 514816]\n",
      "[     0 515072]\n",
      "[     0 515328]\n",
      "[     0 515584]\n",
      "[     0 515840]\n",
      "[     0 516096]\n",
      "[     0 516352]\n",
      "[     0 516608]\n",
      "[     0 516864]\n",
      "[     0 517120]\n",
      "[     0 517376]\n",
      "[     0 517632]\n",
      "[     0 517888]\n",
      "[     0 518144]\n",
      "[     0 518400]\n",
      "[     0 518656]\n",
      "[     0 518912]\n",
      "[     0 519168]\n",
      "[     0 519424]\n",
      "[     0 519680]\n",
      "[     0 519936]\n",
      "[     0 520192]\n",
      "[     0 520448]\n",
      "[     0 520704]\n",
      "[     0 520960]\n",
      "[     0 521216]\n",
      "[     0 521472]\n",
      "[     0 521728]\n",
      "[     0 521984]\n",
      "[     0 522240]\n",
      "[     0 522496]\n",
      "[     0 522752]\n",
      "[     0 523008]\n",
      "[     0 523264]\n",
      "[     0 523520]\n",
      "[     0 523776]\n",
      "[     0 524032]\n",
      "[     0 524288]\n",
      "[     0 524544]\n",
      "[     0 524800]\n",
      "[     0 525056]\n",
      "[     0 525312]\n",
      "[     0 525568]\n",
      "[     0 525824]\n",
      "[     0 526080]\n",
      "[     0 526336]\n",
      "[     0 526592]\n",
      "[     0 526848]\n",
      "[     0 527104]\n",
      "[     0 527360]\n",
      "[     0 527616]\n",
      "[     0 527872]\n",
      "[     0 528128]\n",
      "[     0 528384]\n",
      "[     0 528640]\n",
      "[     0 528896]\n",
      "[     0 529152]\n",
      "[     0 529408]\n",
      "[     0 529664]\n",
      "[     0 529920]\n",
      "[     0 530176]\n",
      "[     0 530432]\n",
      "[     0 530688]\n",
      "[     0 530944]\n",
      "[     0 531200]\n",
      "[     0 531456]\n",
      "[     0 531712]\n",
      "[     0 531968]\n",
      "[     0 532224]\n",
      "[     0 532480]\n",
      "[     0 532736]\n",
      "[     0 532992]\n",
      "[     0 533248]\n",
      "[     0 533504]\n",
      "[     0 533760]\n",
      "[     0 534016]\n",
      "[     0 534272]\n",
      "[     0 534528]\n",
      "[     0 534784]\n",
      "[     0 535040]\n",
      "[     0 535296]\n",
      "[     0 535552]\n",
      "[     0 535808]\n",
      "[     0 536064]\n",
      "[     0 536320]\n",
      "[     0 536576]\n",
      "[     0 536832]\n",
      "[     0 537088]\n",
      "[     0 537344]\n",
      "[     0 537600]\n",
      "[     0 537856]\n",
      "[     0 538112]\n",
      "[     0 538368]\n",
      "[     0 538624]\n",
      "[     0 538880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 539136]\n",
      "[     0 539392]\n",
      "[     0 539648]\n",
      "[     0 539904]\n",
      "[     0 540160]\n",
      "[     0 540416]\n",
      "[     0 540672]\n",
      "[     0 540928]\n",
      "[     0 541184]\n",
      "[     0 541440]\n",
      "[     0 541696]\n",
      "[     0 541952]\n",
      "[     0 542208]\n",
      "[     0 542464]\n",
      "[     0 542720]\n",
      "[     0 542976]\n",
      "[     0 543232]\n",
      "[     0 543488]\n",
      "[     0 543744]\n",
      "[     0 544000]\n",
      "[     0 544256]\n",
      "[     0 544512]\n",
      "[     0 544768]\n",
      "[     0 545024]\n",
      "[     0 545280]\n",
      "[     0 545536]\n",
      "[     0 545792]\n",
      "[     0 546048]\n",
      "[     0 546304]\n",
      "[     0 546560]\n",
      "[     0 546816]\n",
      "[     0 547072]\n",
      "[     0 547328]\n",
      "[     0 547584]\n",
      "[     0 547840]\n",
      "[     0 548096]\n",
      "[     0 548352]\n",
      "[     0 548608]\n",
      "[     0 548864]\n",
      "[     0 549120]\n",
      "[     0 549376]\n",
      "[     0 549632]\n",
      "[     0 549888]\n",
      "[     0 550144]\n",
      "[     0 550400]\n",
      "[     0 550656]\n",
      "[     0 550912]\n",
      "[     0 551168]\n",
      "[     0 551424]\n",
      "[     0 551680]\n",
      "[     0 551936]\n",
      "[     0 552192]\n",
      "[     0 552448]\n",
      "[     0 552704]\n",
      "[     0 552960]\n",
      "[     0 553216]\n",
      "[     0 553472]\n",
      "[     0 553728]\n",
      "[     0 553984]\n",
      "[     0 554240]\n",
      "[     0 554496]\n",
      "[     0 554752]\n",
      "[     0 555008]\n",
      "[     0 555264]\n",
      "[     0 555520]\n",
      "[     0 555776]\n",
      "[     0 556032]\n",
      "[     0 556288]\n",
      "[     0 556544]\n",
      "[     0 556800]\n",
      "[     0 557056]\n",
      "[     0 557312]\n",
      "[     0 557568]\n",
      "[     0 557824]\n",
      "[     0 558080]\n",
      "[     0 558336]\n",
      "[     0 558592]\n",
      "[     0 558848]\n",
      "[     0 559104]\n",
      "[     0 559360]\n",
      "[     0 559616]\n",
      "[     0 559872]\n",
      "[     0 560128]\n",
      "[     0 560384]\n",
      "[     0 560640]\n",
      "[     0 560896]\n",
      "[     0 561152]\n",
      "[     0 561408]\n",
      "[     0 561664]\n",
      "[     0 561920]\n",
      "[     0 562176]\n",
      "[     0 562432]\n",
      "[     0 562688]\n",
      "[     0 562944]\n",
      "[     0 563200]\n",
      "[     0 563456]\n",
      "[     0 563712]\n",
      "[     0 563968]\n",
      "[     0 564224]\n",
      "[     0 564480]\n",
      "[     0 564736]\n",
      "[     0 564992]\n",
      "[     0 565248]\n",
      "[     0 565504]\n",
      "[     0 565760]\n",
      "[     0 566016]\n",
      "[     0 566272]\n",
      "[     0 566528]\n",
      "[     0 566784]\n",
      "[     0 567040]\n",
      "[     0 567296]\n",
      "[     0 567552]\n",
      "[     0 567808]\n",
      "[     0 568064]\n",
      "[     0 568320]\n",
      "[     0 568576]\n",
      "[     0 568832]\n",
      "[     0 569088]\n",
      "[     0 569344]\n",
      "[     0 569600]\n",
      "[     0 569856]\n",
      "[     0 570112]\n",
      "[     0 570368]\n",
      "[     0 570624]\n",
      "[     0 570880]\n",
      "[     0 571136]\n",
      "[     0 571392]\n",
      "[     0 571648]\n",
      "[     0 571904]\n",
      "[     0 572160]\n",
      "[     0 572416]\n",
      "[     0 572672]\n",
      "[     0 572928]\n",
      "[     0 573184]\n",
      "[     0 573440]\n",
      "[     0 573696]\n",
      "[     0 573952]\n",
      "[     0 574208]\n",
      "[     0 574464]\n",
      "[     0 574720]\n",
      "[     0 574976]\n",
      "[     0 575232]\n",
      "[     0 575488]\n",
      "[     0 575744]\n",
      "[     0 576000]\n",
      "[     0 576256]\n",
      "[     0 576512]\n",
      "[     0 576768]\n",
      "[     0 577024]\n",
      "[     0 577280]\n",
      "[     0 577536]\n",
      "[     0 577792]\n",
      "[     0 578048]\n",
      "[     0 578304]\n",
      "[     0 578560]\n",
      "[     0 578816]\n",
      "[     0 579072]\n",
      "[     0 579328]\n",
      "[     0 579584]\n",
      "[     0 579840]\n",
      "[     0 580096]\n",
      "[     0 580352]\n",
      "[     0 580608]\n",
      "[     0 580864]\n",
      "[     0 581120]\n",
      "[     0 581376]\n",
      "[     0 581632]\n",
      "[     0 581888]\n",
      "[     0 582144]\n",
      "[     0 582400]\n",
      "[     0 582656]\n",
      "[     0 582912]\n",
      "[     0 583168]\n",
      "[     0 583424]\n",
      "[     0 583680]\n",
      "[     0 583936]\n",
      "[     0 584192]\n",
      "[     0 584448]\n",
      "[     0 584704]\n",
      "[     0 584960]\n",
      "[     0 585216]\n",
      "[     0 585472]\n",
      "[     0 585728]\n",
      "[     0 585984]\n",
      "[     0 586240]\n",
      "[     0 586496]\n",
      "[     0 586752]\n",
      "[     0 587008]\n",
      "[     0 587264]\n",
      "[     0 587520]\n",
      "[     0 587776]\n",
      "[     0 588032]\n",
      "[     0 588288]\n",
      "[     0 588544]\n",
      "[     0 588800]\n",
      "[     0 589056]\n",
      "[     0 589312]\n",
      "[     0 589568]\n",
      "[     0 589824]\n",
      "[     0 590080]\n",
      "[     0 590336]\n",
      "[     0 590592]\n",
      "[     0 590848]\n",
      "[     0 591104]\n",
      "[     0 591360]\n",
      "[     0 591616]\n",
      "[     0 591872]\n",
      "[     0 592128]\n",
      "[     0 592384]\n",
      "[     0 592640]\n",
      "[     0 592896]\n",
      "[     0 593152]\n",
      "[     0 593408]\n",
      "[     0 593664]\n",
      "[     0 593920]\n",
      "[     0 594176]\n",
      "[     0 594432]\n",
      "[     0 594688]\n",
      "[     0 594944]\n",
      "[     0 595200]\n",
      "[     0 595456]\n",
      "[     0 595712]\n",
      "[     0 595968]\n",
      "[     0 596224]\n",
      "[     0 596480]\n",
      "[     0 596736]\n",
      "[     0 596992]\n",
      "[     0 597248]\n",
      "[     0 597504]\n",
      "[     0 597760]\n",
      "[     0 598016]\n",
      "[     0 598272]\n",
      "[     0 598528]\n",
      "[     0 598784]\n",
      "[     0 599040]\n",
      "[     0 599296]\n",
      "[     0 599552]\n",
      "[     0 599808]\n",
      "[     0 600064]\n",
      "[     0 600320]\n",
      "[     0 600576]\n",
      "[     0 600832]\n",
      "[     0 601088]\n",
      "[     0 601344]\n",
      "[     0 601600]\n",
      "[     0 601856]\n",
      "[     0 602112]\n",
      "[     0 602368]\n",
      "[     0 602624]\n",
      "[     0 602880]\n",
      "[     0 603136]\n",
      "[     0 603392]\n",
      "[     0 603648]\n",
      "[     0 603904]\n",
      "[     0 604160]\n",
      "[     0 604416]\n",
      "[     0 604672]\n",
      "[     0 604928]\n",
      "[     0 605184]\n",
      "[     0 605440]\n",
      "[     0 605696]\n",
      "[     0 605952]\n",
      "[     0 606208]\n",
      "[     0 606464]\n",
      "[     0 606720]\n",
      "[     0 606976]\n",
      "[     0 607232]\n",
      "[     0 607488]\n",
      "[     0 607744]\n",
      "[     0 608000]\n",
      "[     0 608256]\n",
      "[     0 608512]\n",
      "[     0 608768]\n",
      "[     0 609024]\n",
      "[     0 609280]\n",
      "[     0 609536]\n",
      "[     0 609792]\n",
      "[     0 610048]\n",
      "[     0 610304]\n",
      "[     0 610560]\n",
      "[     0 610816]\n",
      "[     0 611072]\n",
      "[     0 611328]\n",
      "[     0 611584]\n",
      "[     0 611840]\n",
      "[     0 612096]\n",
      "[     0 612352]\n",
      "[     0 612608]\n",
      "[     0 612864]\n",
      "[     0 613120]\n",
      "[     0 613376]\n",
      "[     0 613632]\n",
      "[     0 613888]\n",
      "[     0 614144]\n",
      "[     0 614400]\n",
      "[     0 614656]\n",
      "[     0 614912]\n",
      "[     0 615168]\n",
      "[     0 615424]\n",
      "[     0 615680]\n",
      "[     0 615936]\n",
      "[     0 616192]\n",
      "[     0 616448]\n",
      "[     0 616704]\n",
      "[     0 616960]\n",
      "[     0 617216]\n",
      "[     0 617472]\n",
      "[     0 617728]\n",
      "[     0 617984]\n",
      "[     0 618240]\n",
      "[     0 618496]\n",
      "[     0 618752]\n",
      "[     0 619008]\n",
      "[     0 619264]\n",
      "[     0 619520]\n",
      "[     0 619776]\n",
      "[     0 620032]\n",
      "[     0 620288]\n",
      "[     0 620544]\n",
      "[     0 620800]\n",
      "[     0 621056]\n",
      "[     0 621312]\n",
      "[     0 621568]\n",
      "[     0 621824]\n",
      "[     0 622080]\n",
      "[     0 622336]\n",
      "[     0 622592]\n",
      "[     0 622848]\n",
      "[     0 623104]\n",
      "[     0 623360]\n",
      "[     0 623616]\n",
      "[     0 623872]\n",
      "[     0 624128]\n",
      "[     0 624384]\n",
      "[     0 624640]\n",
      "[     0 624896]\n",
      "[     0 625152]\n",
      "[     0 625408]\n",
      "[     0 625664]\n",
      "[     0 625920]\n",
      "[     0 626176]\n",
      "[     0 626432]\n",
      "[     0 626688]\n",
      "[     0 626944]\n",
      "[     0 627200]\n",
      "[     0 627456]\n",
      "[     0 627712]\n",
      "[     0 627968]\n",
      "[     0 628224]\n",
      "[     0 628480]\n",
      "[     0 628736]\n",
      "[     0 628992]\n",
      "[     0 629248]\n",
      "[     0 629504]\n",
      "[     0 629760]\n",
      "[     0 630016]\n",
      "[     0 630272]\n",
      "[     0 630528]\n",
      "[     0 630784]\n",
      "[     0 631040]\n",
      "[     0 631296]\n",
      "[     0 631552]\n",
      "[     0 631808]\n",
      "[     0 632064]\n",
      "[     0 632320]\n",
      "[     0 632576]\n",
      "[     0 632832]\n",
      "[     0 633088]\n",
      "[     0 633344]\n",
      "[     0 633600]\n",
      "[     0 633856]\n",
      "[     0 634112]\n",
      "[     0 634368]\n",
      "[     0 634624]\n",
      "[     0 634880]\n",
      "[     0 635136]\n",
      "[     0 635392]\n",
      "[     0 635648]\n",
      "[     0 635904]\n",
      "[     0 636160]\n",
      "[     0 636416]\n",
      "[     0 636672]\n",
      "[     0 636928]\n",
      "[     0 637184]\n",
      "[     0 637440]\n",
      "[     0 637696]\n",
      "[     0 637952]\n",
      "[     0 638208]\n",
      "[     0 638464]\n",
      "[     0 638720]\n",
      "[     0 638976]\n",
      "[     0 639232]\n",
      "[     0 639488]\n",
      "[     0 639744]\n",
      "[     0 640000]\n",
      "[     0 640256]\n",
      "[     0 640512]\n",
      "[     0 640768]\n",
      "[     0 641024]\n",
      "[     0 641280]\n",
      "[     0 641536]\n",
      "[     0 641792]\n",
      "[     0 642048]\n",
      "[     0 642304]\n",
      "[     0 642560]\n",
      "[     0 642816]\n",
      "[     0 643072]\n",
      "[     0 643328]\n",
      "[     0 643584]\n",
      "[     0 643840]\n",
      "[     0 644096]\n",
      "[     0 644352]\n",
      "[     0 644608]\n",
      "[     0 644864]\n",
      "[     0 645120]\n",
      "[     0 645376]\n",
      "[     0 645632]\n",
      "[     0 645888]\n",
      "[     0 646144]\n",
      "[     0 646400]\n",
      "[     0 646656]\n",
      "[     0 646912]\n",
      "[     0 647168]\n",
      "[     0 647424]\n",
      "[     0 647680]\n",
      "[     0 647936]\n",
      "[     0 648192]\n",
      "[     0 648448]\n",
      "[     0 648704]\n",
      "[     0 648960]\n",
      "[     0 649216]\n",
      "[     0 649472]\n",
      "[     0 649728]\n",
      "[     0 649984]\n",
      "[     0 650240]\n",
      "[     0 650496]\n",
      "[     0 650752]\n",
      "[     0 651008]\n",
      "[     0 651264]\n",
      "[     0 651520]\n",
      "[     0 651776]\n",
      "[     0 652032]\n",
      "[     0 652288]\n",
      "[     0 652544]\n",
      "[     0 652800]\n",
      "[     0 653056]\n",
      "[     0 653312]\n",
      "[     0 653568]\n",
      "[     0 653824]\n",
      "[     0 654080]\n",
      "[     0 654336]\n",
      "[     0 654592]\n",
      "[     0 654848]\n",
      "[     0 655104]\n",
      "[     0 655360]\n",
      "[     0 655616]\n",
      "[     0 655872]\n",
      "[     0 656128]\n",
      "[     0 656384]\n",
      "[     0 656640]\n",
      "[     0 656896]\n",
      "[     0 657152]\n",
      "[     0 657408]\n",
      "[     0 657664]\n",
      "[     0 657920]\n",
      "[     0 658176]\n",
      "[     0 658432]\n",
      "[     0 658688]\n",
      "[     0 658944]\n",
      "[     0 659200]\n",
      "[     0 659456]\n",
      "[     0 659712]\n",
      "[     0 659968]\n",
      "[     0 660224]\n",
      "[     0 660480]\n",
      "[     0 660736]\n",
      "[     0 660992]\n",
      "[     0 661248]\n",
      "[     0 661504]\n",
      "[     0 661760]\n",
      "[     0 662016]\n",
      "[     0 662272]\n",
      "[     0 662528]\n",
      "[     0 662784]\n",
      "[     0 663040]\n",
      "[     0 663296]\n",
      "[     0 663552]\n",
      "[     0 663808]\n",
      "[     0 664064]\n",
      "[     0 664320]\n",
      "[     0 664576]\n",
      "[     0 664832]\n",
      "[     0 665088]\n",
      "[     0 665344]\n",
      "[     0 665600]\n",
      "[     0 665856]\n",
      "[     0 666112]\n",
      "[     0 666368]\n",
      "[     0 666624]\n",
      "[     0 666880]\n",
      "[     0 667136]\n",
      "[     0 667392]\n",
      "[     0 667648]\n",
      "[     0 667904]\n",
      "[     0 668160]\n",
      "[     0 668416]\n",
      "[     0 668672]\n",
      "[     0 668928]\n",
      "[     0 669184]\n",
      "[     0 669440]\n",
      "[     0 669696]\n",
      "[     0 669952]\n",
      "[     0 670208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 670464]\n",
      "[     0 670720]\n",
      "[     0 670976]\n",
      "[     0 671232]\n",
      "[     0 671488]\n",
      "[     0 671744]\n",
      "[     0 672000]\n",
      "[     0 672256]\n",
      "[     0 672512]\n",
      "[     0 672768]\n",
      "[     0 673024]\n",
      "[     0 673280]\n",
      "[     0 673536]\n",
      "[     0 673792]\n",
      "[     0 674048]\n",
      "[     0 674304]\n",
      "[     0 674560]\n",
      "[     0 674816]\n",
      "[     0 675072]\n",
      "[     0 675328]\n",
      "[     0 675584]\n",
      "[     0 675840]\n",
      "[     0 676096]\n",
      "[     0 676352]\n",
      "[     0 676608]\n",
      "[     0 676864]\n",
      "[     0 677120]\n",
      "[     0 677376]\n",
      "[     0 677632]\n",
      "[     0 677888]\n",
      "[     0 678144]\n",
      "[     0 678400]\n",
      "[     0 678656]\n",
      "[     0 678912]\n",
      "[     0 679168]\n",
      "[     0 679424]\n",
      "[     0 679680]\n",
      "[     0 679936]\n",
      "[     0 680192]\n",
      "[     0 680448]\n",
      "[     0 680704]\n",
      "[     0 680960]\n",
      "[     0 681216]\n",
      "[     0 681472]\n",
      "[     0 681728]\n",
      "[     0 681984]\n",
      "[     0 682240]\n",
      "[     0 682496]\n",
      "[     0 682752]\n",
      "[     0 683008]\n",
      "[     0 683264]\n",
      "[     0 683520]\n",
      "[     0 683776]\n",
      "[     0 684032]\n",
      "[     0 684288]\n",
      "[     0 684544]\n",
      "[     0 684800]\n",
      "[     0 685056]\n",
      "[     0 685312]\n",
      "[     0 685568]\n",
      "[     0 685824]\n",
      "[     0 686080]\n",
      "[     0 686336]\n",
      "[     0 686592]\n",
      "[     0 686848]\n",
      "[     0 687104]\n",
      "[     0 687360]\n",
      "[     0 687616]\n",
      "[     0 687872]\n",
      "[     0 688128]\n",
      "[     0 688384]\n",
      "[     0 688640]\n",
      "[     0 688896]\n",
      "[     0 689152]\n",
      "[     0 689408]\n",
      "[     0 689664]\n",
      "[     0 689920]\n",
      "[     0 690176]\n",
      "[     0 690432]\n",
      "[     0 690688]\n",
      "[     0 690944]\n",
      "[     0 691200]\n",
      "[     0 691456]\n",
      "[     0 691712]\n",
      "[     0 691968]\n",
      "[     0 692224]\n",
      "[     0 692480]\n",
      "[     0 692736]\n",
      "[     0 692992]\n",
      "[     0 693248]\n",
      "[     0 693504]\n",
      "[     0 693760]\n",
      "[     0 694016]\n",
      "[     0 694272]\n",
      "[     0 694528]\n",
      "[     0 694784]\n",
      "[     0 695040]\n",
      "[     0 695296]\n",
      "[     0 695552]\n",
      "[     0 695808]\n",
      "[     0 696064]\n",
      "[     0 696320]\n",
      "[     0 696576]\n",
      "[     0 696832]\n",
      "[     0 697088]\n",
      "[     0 697344]\n",
      "[     0 697600]\n",
      "[     0 697856]\n",
      "[     0 698112]\n",
      "[     0 698368]\n",
      "[     0 698624]\n",
      "[     0 698880]\n",
      "[     0 699136]\n",
      "[     0 699392]\n",
      "[     0 699648]\n",
      "[     0 699904]\n",
      "[     0 700160]\n",
      "[     0 700416]\n",
      "[     0 700672]\n",
      "[     0 700928]\n",
      "[     0 701184]\n",
      "[     0 701440]\n",
      "[     0 701696]\n",
      "[     0 701952]\n",
      "[     0 702208]\n",
      "[     0 702464]\n",
      "[     0 702720]\n",
      "[     0 702976]\n",
      "[     0 703232]\n",
      "[     0 703488]\n",
      "[     0 703744]\n",
      "[     0 704000]\n",
      "[     0 704256]\n",
      "[     0 704512]\n",
      "[     0 704768]\n",
      "[     0 705024]\n",
      "[     0 705280]\n",
      "[     0 705536]\n",
      "[     0 705792]\n",
      "[     0 706048]\n",
      "[     0 706304]\n",
      "[     0 706560]\n",
      "[     0 706816]\n",
      "[     0 707072]\n",
      "[     0 707328]\n",
      "[     0 707584]\n",
      "[     0 707840]\n",
      "[     0 708096]\n",
      "[     0 708352]\n",
      "[     0 708608]\n",
      "[     0 708864]\n",
      "[     0 709120]\n",
      "[     0 709376]\n",
      "[     0 709632]\n",
      "[     0 709888]\n",
      "[     0 710144]\n",
      "[     0 710400]\n",
      "[     0 710656]\n",
      "[     0 710912]\n",
      "[     0 711168]\n",
      "[     0 711424]\n",
      "[     0 711680]\n",
      "[     0 711936]\n",
      "[     0 712192]\n",
      "[     0 712448]\n",
      "[     0 712704]\n",
      "[     0 712960]\n",
      "[     0 713216]\n",
      "[     0 713472]\n",
      "[     0 713728]\n",
      "[     0 713984]\n",
      "[     0 714240]\n",
      "[     0 714496]\n",
      "[     0 714752]\n",
      "[     0 715008]\n",
      "[     0 715264]\n",
      "[     0 715520]\n",
      "[     0 715776]\n",
      "[     0 716032]\n",
      "[     0 716288]\n",
      "[     0 716544]\n",
      "[     0 716800]\n",
      "[     0 717056]\n",
      "[     0 717312]\n",
      "[     0 717568]\n",
      "[     0 717824]\n",
      "[     0 718080]\n",
      "[     0 718336]\n",
      "[     0 718592]\n",
      "[     0 718848]\n",
      "[     0 719104]\n",
      "[     0 719360]\n",
      "[     0 719616]\n",
      "[     0 719872]\n",
      "[     0 720128]\n",
      "[     0 720384]\n",
      "[     0 720640]\n",
      "[     0 720896]\n",
      "[     0 721152]\n",
      "[     0 721408]\n",
      "[     0 721664]\n",
      "[     0 721920]\n",
      "[     0 722176]\n",
      "[     0 722432]\n",
      "[     0 722688]\n",
      "[     0 722944]\n",
      "[     0 723200]\n",
      "[     0 723456]\n",
      "[     0 723712]\n",
      "[     0 723968]\n",
      "[     0 724224]\n",
      "[     0 724480]\n",
      "[     0 724736]\n",
      "[     0 724992]\n",
      "[     0 725248]\n",
      "[     0 725504]\n",
      "[     0 725760]\n",
      "[     0 726016]\n",
      "[     0 726272]\n",
      "[     0 726528]\n",
      "[     0 726784]\n",
      "[     0 727040]\n",
      "[     0 727296]\n",
      "[     0 727552]\n",
      "[     0 727808]\n",
      "[     0 728064]\n",
      "[     0 728320]\n",
      "[     0 728576]\n",
      "[     0 728832]\n",
      "[     0 729088]\n",
      "[     0 729344]\n",
      "[     0 729600]\n",
      "[     0 729856]\n",
      "[     0 730112]\n",
      "[     0 730368]\n",
      "[     0 730624]\n",
      "[     0 730880]\n",
      "[     0 731136]\n",
      "[     0 731392]\n",
      "[     0 731648]\n",
      "[     0 731904]\n",
      "[     0 732160]\n",
      "[     0 732416]\n",
      "[     0 732672]\n",
      "[     0 732928]\n",
      "[     0 733184]\n",
      "[     0 733440]\n",
      "[     0 733696]\n",
      "[     0 733952]\n",
      "[     0 734208]\n",
      "[     0 734464]\n",
      "[     0 734720]\n",
      "[     0 734976]\n",
      "[     0 735232]\n",
      "[     0 735488]\n",
      "[     0 735744]\n",
      "[     0 736000]\n",
      "[     0 736256]\n",
      "[     0 736512]\n",
      "[     0 736768]\n",
      "[     0 737024]\n",
      "[     0 737280]\n",
      "[     0 737536]\n",
      "[     0 737792]\n",
      "[     0 738048]\n",
      "[     0 738304]\n",
      "[     0 738560]\n",
      "[     0 738816]\n",
      "[     0 739072]\n",
      "[     0 739328]\n",
      "[     0 739584]\n",
      "[     0 739840]\n",
      "[     0 740096]\n",
      "[     0 740352]\n",
      "[     0 740608]\n",
      "[     0 740864]\n",
      "[     0 741120]\n",
      "[     0 741376]\n",
      "[     0 741632]\n",
      "[     0 741888]\n",
      "[     0 742144]\n",
      "[     0 742400]\n",
      "[     0 742656]\n",
      "[     0 742912]\n",
      "[     0 743168]\n",
      "[     0 743424]\n",
      "[     0 743680]\n",
      "[     0 743936]\n",
      "[     0 744192]\n",
      "[     0 744448]\n",
      "[     0 744704]\n",
      "[     0 744960]\n",
      "[     0 745216]\n",
      "[     0 745472]\n",
      "[     0 745728]\n",
      "[     0 745984]\n",
      "[     0 746240]\n",
      "[     0 746496]\n",
      "[     0 746752]\n",
      "[     0 747008]\n",
      "[     0 747264]\n",
      "[     0 747520]\n",
      "[     0 747776]\n",
      "[     0 748032]\n",
      "[     0 748288]\n",
      "[     0 748544]\n",
      "[     0 748800]\n",
      "[     0 749056]\n",
      "[     0 749312]\n",
      "[     0 749568]\n",
      "[     0 749824]\n",
      "[     0 750080]\n",
      "[     0 750336]\n",
      "[     0 750592]\n",
      "[     0 750848]\n",
      "[     0 751104]\n",
      "[     0 751360]\n",
      "[     0 751616]\n",
      "[     0 751872]\n",
      "[     0 752128]\n",
      "[     0 752384]\n",
      "[     0 752640]\n",
      "[     0 752896]\n",
      "[     0 753152]\n",
      "[     0 753408]\n",
      "[     0 753664]\n",
      "[     0 753920]\n",
      "[     0 754176]\n",
      "[     0 754432]\n",
      "[     0 754688]\n",
      "[     0 754944]\n",
      "[     0 755200]\n",
      "[     0 755456]\n",
      "[     0 755712]\n",
      "[     0 755968]\n",
      "[     0 756224]\n",
      "[     0 756480]\n",
      "[     0 756736]\n",
      "[     0 756992]\n",
      "[     0 757248]\n",
      "[     0 757504]\n",
      "[     0 757760]\n",
      "[     0 758016]\n",
      "[     0 758272]\n",
      "[     0 758528]\n",
      "[     0 758784]\n",
      "[     0 759040]\n",
      "[     0 759296]\n",
      "[     0 759552]\n",
      "[     0 759808]\n",
      "[     0 760064]\n",
      "[     0 760320]\n",
      "[     0 760576]\n",
      "[     0 760832]\n",
      "[     0 761088]\n",
      "[     0 761344]\n",
      "[     0 761600]\n",
      "[     0 761856]\n",
      "[     0 762112]\n",
      "[     0 762368]\n",
      "[     0 762624]\n",
      "[     0 762880]\n",
      "[     0 763136]\n",
      "[     0 763392]\n",
      "[     0 763648]\n",
      "[     0 763904]\n",
      "[     0 764160]\n",
      "[     0 764416]\n",
      "[     0 764672]\n",
      "[     0 764928]\n",
      "[     0 765184]\n",
      "[     0 765440]\n",
      "[     0 765696]\n",
      "[     0 765952]\n",
      "[     0 766208]\n",
      "[     0 766464]\n",
      "[     0 766720]\n",
      "[     0 766976]\n",
      "[     0 767232]\n",
      "[     0 767488]\n",
      "[     0 767744]\n",
      "[     0 768000]\n",
      "[     0 768256]\n",
      "[     0 768512]\n",
      "[     0 768768]\n",
      "[     0 769024]\n",
      "[     0 769280]\n",
      "[     0 769536]\n",
      "[     0 769792]\n",
      "[     0 770048]\n",
      "[     0 770304]\n",
      "[     0 770560]\n",
      "[     0 770816]\n",
      "[     0 771072]\n",
      "[     0 771328]\n",
      "[     0 771584]\n",
      "[     0 771840]\n",
      "[     0 772096]\n",
      "[     0 772352]\n",
      "[     0 772608]\n",
      "[     0 772864]\n",
      "[     0 773120]\n",
      "[     0 773376]\n",
      "[     0 773632]\n",
      "[     0 773888]\n",
      "[     0 774144]\n",
      "[     0 774400]\n",
      "[     0 774656]\n",
      "[     0 774912]\n",
      "[     0 775168]\n",
      "[     0 775424]\n",
      "[     0 775680]\n",
      "[     0 775936]\n",
      "[     0 776192]\n",
      "[     0 776448]\n",
      "[     0 776704]\n",
      "[     0 776960]\n",
      "[     0 777216]\n",
      "[     0 777472]\n",
      "[     0 777728]\n",
      "[     0 777984]\n",
      "[     0 778240]\n",
      "[     0 778496]\n",
      "[     0 778752]\n",
      "[     0 779008]\n",
      "[     0 779264]\n",
      "[     0 779520]\n",
      "[     0 779776]\n",
      "[     0 780032]\n",
      "[     0 780288]\n",
      "[     0 780544]\n",
      "[     0 780800]\n",
      "[     0 781056]\n",
      "[     0 781312]\n",
      "[     0 781568]\n",
      "[     0 781824]\n",
      "[     0 782080]\n",
      "[     0 782336]\n",
      "[     0 782592]\n",
      "[     0 782848]\n",
      "[     0 783104]\n",
      "[     0 783360]\n",
      "[     0 783616]\n",
      "[     0 783872]\n",
      "[     0 784128]\n",
      "[     0 784384]\n",
      "[     0 784640]\n",
      "[     0 784896]\n",
      "[     0 785152]\n",
      "[     0 785408]\n",
      "[     0 785664]\n",
      "[     0 785920]\n",
      "[     0 786176]\n",
      "[     0 786432]\n",
      "[     0 786688]\n",
      "[     0 786944]\n",
      "[     0 787200]\n",
      "[     0 787456]\n",
      "[     0 787712]\n",
      "[     0 787968]\n",
      "[     0 788224]\n",
      "[     0 788480]\n",
      "[     0 788736]\n",
      "[     0 788992]\n",
      "[     0 789248]\n",
      "[     0 789504]\n",
      "[     0 789760]\n",
      "[     0 790016]\n",
      "[     0 790272]\n",
      "[     0 790528]\n",
      "[     0 790784]\n",
      "[     0 791040]\n",
      "[     0 791296]\n",
      "[     0 791552]\n",
      "[     0 791808]\n",
      "[     0 792064]\n",
      "[     0 792320]\n",
      "[     0 792576]\n",
      "[     0 792832]\n",
      "[     0 793088]\n",
      "[     0 793344]\n",
      "[     0 793600]\n",
      "[     0 793856]\n",
      "[     0 794112]\n",
      "[     0 794368]\n",
      "[     0 794624]\n",
      "[     0 794880]\n",
      "[     0 795136]\n",
      "[     0 795392]\n",
      "[     0 795648]\n",
      "[     0 795904]\n",
      "[     0 796160]\n",
      "[     0 796416]\n",
      "[     0 796672]\n",
      "[     0 796928]\n",
      "[     0 797184]\n",
      "[     0 797440]\n",
      "[     0 797696]\n",
      "[     0 797952]\n",
      "[     0 798208]\n",
      "[     0 798464]\n",
      "[     0 798720]\n",
      "[     0 798976]\n",
      "[     0 799232]\n",
      "[     0 799488]\n",
      "[     0 799744]\n",
      "[     0 800000]\n",
      "[     0 800256]\n",
      "[     0 800512]\n",
      "[     0 800768]\n",
      "[     0 801024]\n",
      "[     0 801280]\n",
      "[     0 801536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 801792]\n",
      "[     0 802048]\n",
      "[     0 802304]\n",
      "[     0 802560]\n",
      "[     0 802816]\n",
      "[     0 803072]\n",
      "[     0 803328]\n",
      "[     0 803584]\n",
      "[     0 803840]\n",
      "[     0 804096]\n",
      "[     0 804352]\n",
      "[     0 804608]\n",
      "[     0 804864]\n",
      "[     0 805120]\n",
      "[     0 805376]\n",
      "[     0 805632]\n",
      "[     0 805888]\n",
      "[     0 806144]\n",
      "[     0 806400]\n",
      "[     0 806656]\n",
      "[     0 806912]\n",
      "[     0 807168]\n",
      "[     0 807424]\n",
      "[     0 807680]\n",
      "[     0 807936]\n",
      "[     0 808192]\n",
      "[     0 808448]\n",
      "[     0 808704]\n",
      "[     0 808960]\n",
      "[     0 809216]\n",
      "[     0 809472]\n",
      "[     0 809728]\n",
      "[     0 809984]\n",
      "[     0 810240]\n",
      "[     0 810496]\n",
      "[     0 810752]\n",
      "[     0 811008]\n",
      "[     0 811264]\n",
      "[     0 811520]\n",
      "[     0 811776]\n",
      "[     0 812032]\n",
      "[     0 812288]\n",
      "[     0 812544]\n",
      "[     0 812800]\n",
      "[     0 813056]\n",
      "[     0 813312]\n",
      "[     0 813568]\n",
      "[     0 813824]\n",
      "[     0 814080]\n",
      "[     0 814336]\n",
      "[     0 814592]\n",
      "[     0 814848]\n",
      "[     0 815104]\n",
      "[     0 815360]\n",
      "[     0 815616]\n",
      "[     0 815872]\n",
      "[     0 816128]\n",
      "[     0 816384]\n",
      "[     0 816640]\n",
      "[     0 816896]\n",
      "[     0 817152]\n",
      "[     0 817408]\n",
      "[     0 817664]\n",
      "[     0 817920]\n",
      "[     0 818176]\n",
      "[     0 818432]\n",
      "[     0 818688]\n",
      "[     0 818944]\n",
      "[     0 819200]\n",
      "[     0 819456]\n",
      "[     0 819712]\n",
      "[     0 819968]\n",
      "[     0 820224]\n",
      "[     0 820480]\n",
      "[     0 820736]\n",
      "[     0 820992]\n",
      "[     0 821248]\n",
      "[     0 821504]\n",
      "[     0 821760]\n",
      "[     0 822016]\n",
      "[     0 822272]\n",
      "[     0 822528]\n",
      "[     0 822784]\n",
      "[     0 823040]\n",
      "[     0 823296]\n",
      "[     0 823552]\n",
      "[     0 823808]\n",
      "[     0 824064]\n",
      "[     0 824320]\n",
      "[     0 824576]\n",
      "[     0 824832]\n",
      "[     0 825088]\n",
      "[     0 825344]\n",
      "[     0 825600]\n",
      "[     0 825856]\n",
      "[     0 826112]\n",
      "[     0 826368]\n",
      "[     0 826624]\n",
      "[     0 826880]\n",
      "[     0 827136]\n",
      "[     0 827392]\n",
      "[     0 827648]\n",
      "[     0 827904]\n",
      "[     0 828160]\n",
      "[     0 828416]\n",
      "[     0 828672]\n",
      "[     0 828928]\n",
      "[     0 829184]\n",
      "[     0 829440]\n",
      "[     0 829696]\n",
      "[     0 829952]\n",
      "[     0 830208]\n",
      "[     0 830464]\n",
      "[     0 830720]\n",
      "[     0 830976]\n",
      "[     0 831232]\n",
      "[     0 831488]\n",
      "[     0 831744]\n",
      "[     0 832000]\n",
      "[     0 832256]\n",
      "[     0 832512]\n",
      "[     0 832768]\n",
      "[     0 833024]\n",
      "[     0 833280]\n",
      "[     0 833536]\n",
      "[     0 833792]\n",
      "[     0 834048]\n",
      "[     0 834304]\n",
      "[     0 834560]\n",
      "[     0 834816]\n",
      "[     0 835072]\n",
      "[     0 835328]\n",
      "[     0 835584]\n",
      "[     0 835840]\n",
      "[     0 836096]\n",
      "[     0 836352]\n",
      "[     0 836608]\n",
      "[     0 836864]\n",
      "[     0 837120]\n",
      "[     0 837376]\n",
      "[     0 837632]\n",
      "[     0 837888]\n",
      "[     0 838144]\n",
      "[     0 838400]\n",
      "[     0 838656]\n",
      "[     0 838912]\n",
      "[     0 839168]\n",
      "[     0 839424]\n",
      "[     0 839680]\n",
      "[     0 839936]\n",
      "[     0 840192]\n",
      "[     0 840448]\n",
      "[     0 840704]\n",
      "[     0 840960]\n",
      "[     0 841216]\n",
      "[     0 841472]\n",
      "[     0 841728]\n",
      "[     0 841984]\n",
      "[     0 842240]\n",
      "[     0 842496]\n",
      "[     0 842752]\n",
      "[     0 843008]\n",
      "[     0 843264]\n",
      "[     0 843520]\n",
      "[     0 843776]\n",
      "[     0 844032]\n",
      "[     0 844288]\n",
      "[     0 844544]\n",
      "[     0 844800]\n",
      "[     0 845056]\n",
      "[     0 845312]\n",
      "[     0 845568]\n",
      "[     0 845824]\n",
      "[     0 846080]\n",
      "[     0 846336]\n",
      "[     0 846592]\n",
      "[     0 846848]\n",
      "[     0 847104]\n",
      "[     0 847360]\n",
      "[     0 847616]\n",
      "[     0 847872]\n",
      "[     0 848128]\n",
      "[     0 848384]\n",
      "[     0 848640]\n",
      "[     0 848896]\n",
      "[     0 849152]\n",
      "[     0 849408]\n",
      "[     0 849664]\n",
      "[     0 849920]\n",
      "[     0 850176]\n",
      "[     0 850432]\n",
      "[     0 850688]\n",
      "[     0 850944]\n",
      "[     0 851200]\n",
      "[     0 851456]\n",
      "[     0 851712]\n",
      "[     0 851968]\n",
      "[     0 852224]\n",
      "[     0 852480]\n",
      "[     0 852736]\n",
      "[     0 852992]\n",
      "[     0 853248]\n",
      "[     0 853504]\n",
      "[     0 853760]\n",
      "[     0 854016]\n",
      "[     0 854272]\n",
      "[     0 854528]\n",
      "[     0 854784]\n",
      "[     0 855040]\n",
      "[     0 855296]\n",
      "[     0 855552]\n",
      "[     0 855808]\n",
      "[     0 856064]\n",
      "[     0 856320]\n",
      "[     0 856576]\n",
      "[     0 856832]\n",
      "[     0 857088]\n",
      "[     0 857344]\n",
      "[     0 857600]\n",
      "[     0 857856]\n",
      "[     0 858112]\n",
      "[     0 858368]\n",
      "[     0 858624]\n",
      "[     0 858880]\n",
      "[     0 859136]\n",
      "[     0 859392]\n",
      "[     0 859648]\n",
      "[     0 859904]\n",
      "[     0 860160]\n",
      "[     0 860416]\n",
      "[     0 860672]\n",
      "[     0 860928]\n",
      "[     0 861184]\n",
      "[     0 861440]\n",
      "[     0 861696]\n",
      "[     0 861952]\n",
      "[     0 862208]\n",
      "[     0 862464]\n",
      "[     0 862720]\n",
      "[     0 862976]\n",
      "[     0 863232]\n",
      "[     0 863488]\n",
      "[     0 863744]\n",
      "[     0 864000]\n",
      "[     0 864256]\n",
      "[     0 864512]\n",
      "[     0 864768]\n",
      "[     0 865024]\n",
      "[     0 865280]\n",
      "[     0 865536]\n",
      "[     0 865792]\n",
      "[     0 866048]\n",
      "[     0 866304]\n",
      "[     0 866560]\n",
      "[     0 866816]\n",
      "[     0 867072]\n",
      "[     0 867328]\n",
      "[     0 867584]\n",
      "[     0 867840]\n",
      "[     0 868096]\n",
      "[     0 868352]\n",
      "[     0 868608]\n",
      "[     0 868864]\n",
      "[     0 869120]\n",
      "[     0 869376]\n",
      "[     0 869632]\n",
      "[     0 869888]\n",
      "[     0 870144]\n",
      "[     0 870400]\n",
      "[     0 870656]\n",
      "[     0 870912]\n",
      "[     0 871168]\n",
      "[     0 871424]\n",
      "[     0 871680]\n",
      "[     0 871936]\n",
      "[     0 872192]\n",
      "[     0 872448]\n",
      "[     0 872704]\n",
      "[     0 872960]\n",
      "[     0 873216]\n",
      "[     0 873472]\n",
      "[     0 873728]\n",
      "[     0 873984]\n",
      "[     0 874240]\n",
      "[     0 874496]\n",
      "[     0 874752]\n",
      "[     0 875008]\n",
      "[     0 875264]\n",
      "[     0 875520]\n",
      "[     0 875776]\n",
      "[     0 876032]\n",
      "[     0 876288]\n",
      "[     0 876544]\n",
      "[     0 876800]\n",
      "[     0 877056]\n",
      "[     0 877312]\n",
      "[     0 877568]\n",
      "[     0 877824]\n",
      "[     0 878080]\n",
      "[     0 878336]\n",
      "[     0 878592]\n",
      "[     0 878848]\n",
      "[     0 879104]\n",
      "[     0 879360]\n",
      "[     0 879616]\n",
      "[     0 879872]\n",
      "[     0 880128]\n",
      "[     0 880384]\n",
      "[     0 880640]\n",
      "[     0 880896]\n",
      "[     0 881152]\n",
      "[     0 881408]\n",
      "[     0 881664]\n",
      "[     0 881920]\n",
      "[     0 882176]\n",
      "[     0 882432]\n",
      "[     0 882688]\n",
      "[     0 882944]\n",
      "[     0 883200]\n",
      "[     0 883456]\n",
      "[     0 883712]\n",
      "[     0 883968]\n",
      "[     0 884224]\n",
      "[     0 884480]\n",
      "[     0 884736]\n",
      "[     0 884992]\n",
      "[     0 885248]\n",
      "[     0 885504]\n",
      "[     0 885760]\n",
      "[     0 886016]\n",
      "[     0 886272]\n",
      "[     0 886528]\n",
      "[     0 886784]\n",
      "[     0 887040]\n",
      "[     0 887296]\n",
      "[     0 887552]\n",
      "[     0 887808]\n",
      "[     0 888064]\n",
      "[     0 888320]\n",
      "[     0 888576]\n",
      "[     0 888832]\n",
      "[     0 889088]\n",
      "[     0 889344]\n",
      "[     0 889600]\n",
      "[     0 889856]\n",
      "[     0 890112]\n",
      "[     0 890368]\n",
      "[     0 890624]\n",
      "[     0 890880]\n",
      "[     0 891136]\n",
      "[     0 891392]\n",
      "[     0 891648]\n",
      "[     0 891904]\n",
      "[     0 892160]\n",
      "[     0 892416]\n",
      "[     0 892672]\n",
      "[     0 892928]\n",
      "[     0 893184]\n",
      "[     0 893440]\n",
      "[     0 893696]\n",
      "[     0 893952]\n",
      "[     0 894208]\n",
      "[     0 894464]\n",
      "[     0 894720]\n",
      "[     0 894976]\n",
      "[     0 895232]\n",
      "[     0 895488]\n",
      "[     0 895744]\n",
      "[     0 896000]\n",
      "[     0 896256]\n",
      "[     0 896512]\n",
      "[     0 896768]\n",
      "[     0 897024]\n",
      "[     0 897280]\n",
      "[     0 897536]\n",
      "[     0 897792]\n",
      "[     0 898048]\n",
      "[     0 898304]\n",
      "[     0 898560]\n",
      "[     0 898816]\n",
      "[     0 899072]\n",
      "[     0 899328]\n",
      "[     0 899584]\n",
      "[     0 899840]\n",
      "[     0 900096]\n",
      "[     0 900352]\n",
      "[     0 900608]\n",
      "[     0 900864]\n",
      "[     0 901120]\n",
      "[     0 901376]\n",
      "[     0 901632]\n",
      "[     0 901888]\n",
      "[     0 902144]\n",
      "[     0 902400]\n",
      "[     0 902656]\n",
      "[     0 902912]\n",
      "[     0 903168]\n",
      "[     0 903424]\n",
      "[     0 903680]\n",
      "[     0 903936]\n",
      "[     0 904192]\n",
      "[     0 904448]\n",
      "[     0 904704]\n",
      "[     0 904960]\n",
      "[     0 905216]\n",
      "[     0 905472]\n",
      "[     0 905728]\n",
      "[     0 905984]\n",
      "[     0 906240]\n",
      "[     0 906496]\n",
      "[     0 906752]\n",
      "[     0 907008]\n",
      "[     0 907264]\n",
      "[     0 907520]\n",
      "[     0 907776]\n",
      "[     0 908032]\n",
      "[     0 908288]\n",
      "[     0 908544]\n",
      "[     0 908800]\n",
      "[     0 909056]\n",
      "[     0 909312]\n",
      "[     0 909568]\n",
      "[     0 909824]\n",
      "[     0 910080]\n",
      "[     0 910336]\n",
      "[     0 910592]\n",
      "[     0 910848]\n",
      "[     0 911104]\n",
      "[     0 911360]\n",
      "[     0 911616]\n",
      "[     0 911872]\n",
      "[     0 912128]\n",
      "[     0 912384]\n",
      "[     0 912640]\n",
      "[     0 912896]\n",
      "[     0 913152]\n",
      "[     0 913408]\n",
      "[     0 913664]\n",
      "[     0 913920]\n",
      "[     0 914176]\n",
      "[     0 914432]\n",
      "[     0 914688]\n",
      "[     0 914944]\n",
      "[     0 915200]\n",
      "[     0 915456]\n",
      "[     0 915712]\n",
      "[     0 915968]\n",
      "[     0 916224]\n",
      "[     0 916480]\n",
      "[     0 916736]\n",
      "[     0 916992]\n",
      "[     0 917248]\n",
      "[     0 917504]\n",
      "[     0 917760]\n",
      "[     0 918016]\n",
      "[     0 918272]\n",
      "[     0 918528]\n",
      "[     0 918784]\n",
      "[     0 919040]\n",
      "[     0 919296]\n",
      "[     0 919552]\n",
      "[     0 919808]\n",
      "[     0 920064]\n",
      "[     0 920320]\n",
      "[     0 920576]\n",
      "[     0 920832]\n",
      "[     0 921088]\n",
      "[     0 921344]\n",
      "[     0 921600]\n",
      "[     0 921856]\n",
      "[     0 922112]\n",
      "[     0 922368]\n",
      "[     0 922624]\n",
      "[     0 922880]\n",
      "[     0 923136]\n",
      "[     0 923392]\n",
      "[     0 923648]\n",
      "[     0 923904]\n",
      "[     0 924160]\n",
      "[     0 924416]\n",
      "[     0 924672]\n",
      "[     0 924928]\n",
      "[     0 925184]\n",
      "[     0 925440]\n",
      "[     0 925696]\n",
      "[     0 925952]\n",
      "[     0 926208]\n",
      "[     0 926464]\n",
      "[     0 926720]\n",
      "[     0 926976]\n",
      "[     0 927232]\n",
      "[     0 927488]\n",
      "[     0 927744]\n",
      "[     0 928000]\n",
      "[     0 928256]\n",
      "[     0 928512]\n",
      "[     0 928768]\n",
      "[     0 929024]\n",
      "[     0 929280]\n",
      "[     0 929536]\n",
      "[     0 929792]\n",
      "[     0 930048]\n",
      "[     0 930304]\n",
      "[     0 930560]\n",
      "[     0 930816]\n",
      "[     0 931072]\n",
      "[     0 931328]\n",
      "[     0 931584]\n",
      "[     0 931840]\n",
      "[     0 932096]\n",
      "[     0 932352]\n",
      "[     0 932608]\n",
      "[     0 932864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 933120]\n",
      "[     0 933376]\n",
      "[     0 933632]\n",
      "[     0 933888]\n",
      "[     0 934144]\n",
      "[     0 934400]\n",
      "[     0 934656]\n",
      "[     0 934912]\n",
      "[     0 935168]\n",
      "[     0 935424]\n",
      "[     0 935680]\n",
      "[     0 935936]\n",
      "[     0 936192]\n",
      "[     0 936448]\n",
      "[     0 936704]\n",
      "[     0 936960]\n",
      "[     0 937216]\n",
      "[     0 937472]\n",
      "[     0 937728]\n",
      "[     0 937984]\n",
      "[     0 938240]\n",
      "[     0 938496]\n",
      "[     0 938752]\n",
      "[     0 939008]\n",
      "[     0 939264]\n",
      "[     0 939520]\n",
      "[     0 939776]\n",
      "[     0 940032]\n",
      "[     0 940288]\n",
      "[     0 940544]\n",
      "[     0 940800]\n",
      "[     0 941056]\n",
      "[     0 941312]\n",
      "[     0 941568]\n",
      "[     0 941824]\n",
      "[     0 942080]\n",
      "[     0 942336]\n",
      "[     0 942592]\n",
      "[     0 942848]\n",
      "[     0 943104]\n",
      "[     0 943360]\n",
      "[     0 943616]\n",
      "[     0 943872]\n",
      "[     0 944128]\n",
      "[     0 944384]\n",
      "[     0 944640]\n",
      "[     0 944896]\n",
      "[     0 945152]\n",
      "[     0 945408]\n",
      "[     0 945664]\n",
      "[     0 945920]\n",
      "[     0 946176]\n",
      "[     0 946432]\n",
      "[     0 946688]\n",
      "[     0 946944]\n",
      "[     0 947200]\n",
      "[     0 947456]\n",
      "[     0 947712]\n",
      "[     0 947968]\n",
      "[     0 948224]\n",
      "[     0 948480]\n",
      "[     0 948736]\n",
      "[     0 948992]\n",
      "[     0 949248]\n",
      "[     0 949504]\n",
      "[     0 949760]\n",
      "[     0 950016]\n",
      "[     0 950272]\n",
      "[     0 950528]\n",
      "[     0 950784]\n",
      "[     0 951040]\n",
      "[     0 951296]\n",
      "[     0 951552]\n",
      "[     0 951808]\n",
      "[     0 952064]\n",
      "[     0 952320]\n",
      "[     0 952576]\n",
      "[     0 952832]\n",
      "[     0 953088]\n",
      "[     0 953344]\n",
      "[     0 953600]\n",
      "[     0 953856]\n",
      "[     0 954112]\n",
      "[     0 954368]\n",
      "[     0 954624]\n",
      "[     0 954880]\n",
      "[     0 955136]\n",
      "[     0 955392]\n",
      "[     0 955648]\n",
      "[     0 955904]\n",
      "[     0 956160]\n",
      "[     0 956416]\n",
      "[     0 956672]\n",
      "[     0 956928]\n",
      "[     0 957184]\n",
      "[     0 957440]\n",
      "[     0 957696]\n",
      "[     0 957952]\n",
      "[     0 958208]\n",
      "[     0 958464]\n",
      "[     0 958720]\n",
      "[     0 958976]\n",
      "[     0 959232]\n",
      "[     0 959488]\n",
      "[     0 959744]\n",
      "[     0 960000]\n",
      "[     0 960256]\n",
      "[     0 960512]\n",
      "[     0 960768]\n",
      "[     0 961024]\n",
      "[     0 961280]\n",
      "[     0 961536]\n",
      "[     0 961792]\n",
      "[     0 962048]\n",
      "[     0 962304]\n",
      "[     0 962560]\n",
      "[     0 962816]\n",
      "[     0 963072]\n",
      "[     0 963328]\n",
      "[     0 963584]\n",
      "[     0 963840]\n",
      "[     0 964096]\n",
      "[     0 964352]\n",
      "[     0 964608]\n",
      "[     0 964864]\n",
      "[     0 965120]\n",
      "[     0 965376]\n",
      "[     0 965632]\n",
      "[     0 965888]\n",
      "[     0 966144]\n",
      "[     0 966400]\n",
      "[     0 966656]\n",
      "[     0 966912]\n",
      "[     0 967168]\n",
      "[     0 967424]\n",
      "[     0 967680]\n",
      "[     0 967936]\n",
      "[     0 968192]\n",
      "[     0 968448]\n",
      "[     0 968704]\n",
      "[     0 968960]\n",
      "[     0 969216]\n",
      "[     0 969472]\n",
      "[     0 969728]\n",
      "[     0 969984]\n",
      "[     0 970240]\n",
      "[     0 970496]\n",
      "[     0 970752]\n",
      "[     0 971008]\n",
      "[     0 971264]\n",
      "[     0 971520]\n",
      "[     0 971776]\n",
      "[     0 972032]\n",
      "[     0 972288]\n",
      "[     0 972544]\n",
      "[     0 972800]\n",
      "[     0 973056]\n",
      "[     0 973312]\n",
      "[     0 973568]\n",
      "[     0 973824]\n",
      "[     0 974080]\n",
      "[     0 974336]\n",
      "[     0 974592]\n",
      "[     0 974848]\n",
      "[     0 975104]\n",
      "[     0 975360]\n",
      "[     0 975616]\n",
      "[     0 975872]\n",
      "[     0 976128]\n",
      "[     0 976384]\n",
      "[     0 976640]\n",
      "[     0 976896]\n",
      "[     0 977152]\n",
      "[     0 977408]\n",
      "[     0 977664]\n",
      "[     0 977920]\n",
      "[     0 978176]\n",
      "[     0 978432]\n",
      "[     0 978688]\n",
      "[     0 978944]\n",
      "[     0 979200]\n",
      "[     0 979456]\n",
      "[     0 979712]\n",
      "[     0 979968]\n",
      "[     0 980224]\n",
      "[     0 980480]\n",
      "[     0 980736]\n",
      "[     0 980992]\n",
      "[     0 981248]\n",
      "[     0 981504]\n",
      "[     0 981760]\n",
      "[     0 982016]\n",
      "[     0 982272]\n",
      "[     0 982528]\n",
      "[     0 982784]\n",
      "[     0 983040]\n",
      "[     0 983296]\n",
      "[     0 983552]\n",
      "[     0 983808]\n",
      "[     0 984064]\n",
      "[     0 984320]\n",
      "[     0 984576]\n",
      "[     0 984832]\n",
      "[     0 985088]\n",
      "[     0 985344]\n",
      "[     0 985600]\n",
      "[     0 985856]\n",
      "[     0 986112]\n",
      "[     0 986368]\n",
      "[     0 986624]\n",
      "[     0 986880]\n",
      "[     0 987136]\n",
      "[     0 987392]\n",
      "[     0 987648]\n",
      "[     0 987904]\n",
      "[     0 988160]\n",
      "[     0 988416]\n",
      "[     0 988672]\n",
      "[     0 988928]\n",
      "[     0 989184]\n",
      "[     0 989440]\n",
      "[     0 989696]\n",
      "[     0 989952]\n",
      "[     0 990208]\n",
      "[     0 990464]\n",
      "[     0 990720]\n",
      "[     0 990976]\n",
      "[     0 991232]\n",
      "[     0 991488]\n",
      "[     0 991744]\n",
      "[     0 992000]\n",
      "[     0 992256]\n",
      "[     0 992512]\n",
      "[     0 992768]\n",
      "[     0 993024]\n",
      "[     0 993280]\n",
      "[     0 993536]\n",
      "[     0 993792]\n",
      "[     0 994048]\n",
      "[     0 994304]\n",
      "[     0 994560]\n",
      "[     0 994816]\n",
      "[     0 995072]\n",
      "[     0 995328]\n",
      "[     0 995584]\n",
      "[     0 995840]\n",
      "[     0 996096]\n",
      "[     0 996352]\n",
      "[     0 996608]\n",
      "[     0 996864]\n",
      "[     0 997120]\n",
      "[     0 997376]\n",
      "[     0 997632]\n",
      "[     0 997888]\n",
      "[     0 998144]\n",
      "[     0 998400]\n",
      "[     0 998656]\n",
      "[     0 998912]\n",
      "[     0 999168]\n",
      "[     0 999424]\n",
      "[     0 999680]\n",
      "[     0 999936]\n",
      "[      0 1000192]\n",
      "[      0 1000448]\n",
      "[      0 1000704]\n",
      "[      0 1000960]\n",
      "[      0 1001216]\n",
      "[      0 1001472]\n",
      "[      0 1001728]\n",
      "[      0 1001984]\n",
      "[      0 1002240]\n",
      "[      0 1002496]\n",
      "[      0 1002752]\n",
      "[      0 1003008]\n",
      "[      0 1003264]\n",
      "[      0 1003520]\n",
      "[      0 1003776]\n",
      "[      0 1004032]\n",
      "[      0 1004288]\n",
      "[      0 1004544]\n",
      "[      0 1004800]\n",
      "[      0 1005056]\n",
      "[      0 1005312]\n",
      "[      0 1005568]\n",
      "[      0 1005824]\n",
      "[      0 1006080]\n",
      "[      0 1006336]\n",
      "[      0 1006592]\n",
      "[      0 1006848]\n",
      "[      0 1007104]\n",
      "[      0 1007360]\n",
      "[      0 1007616]\n",
      "[      0 1007872]\n",
      "[      0 1008128]\n",
      "[      0 1008384]\n",
      "[      0 1008640]\n",
      "[      0 1008896]\n",
      "[      0 1009152]\n",
      "[      0 1009408]\n",
      "[      0 1009664]\n",
      "[      0 1009920]\n",
      "[      0 1010176]\n",
      "[      0 1010432]\n",
      "[      0 1010688]\n",
      "[      0 1010944]\n",
      "[      0 1011200]\n",
      "[      0 1011456]\n",
      "[      0 1011712]\n",
      "[      0 1011968]\n",
      "[      0 1012224]\n",
      "[      0 1012480]\n",
      "[      0 1012736]\n",
      "[      0 1012992]\n",
      "[      0 1013248]\n",
      "[      0 1013504]\n",
      "[      0 1013760]\n",
      "[      0 1014016]\n",
      "[      0 1014272]\n",
      "[      0 1014528]\n",
      "[      0 1014784]\n",
      "[      0 1015040]\n",
      "[      0 1015296]\n",
      "[      0 1015552]\n",
      "[      0 1015808]\n",
      "[      0 1016064]\n",
      "[      0 1016320]\n",
      "[      0 1016576]\n",
      "[      0 1016832]\n",
      "[      0 1017088]\n",
      "[      0 1017344]\n",
      "[      0 1017600]\n",
      "[      0 1017856]\n",
      "[      0 1018112]\n",
      "[      0 1018368]\n",
      "[      0 1018624]\n",
      "[      0 1018880]\n",
      "[      0 1019136]\n",
      "[      0 1019392]\n",
      "[      0 1019648]\n",
      "[      0 1019904]\n",
      "[      0 1020160]\n",
      "[      0 1020416]\n",
      "[      0 1020672]\n",
      "[      0 1020928]\n",
      "[      0 1021184]\n",
      "[      0 1021440]\n",
      "[      0 1021696]\n",
      "[      0 1021952]\n",
      "[      0 1022208]\n",
      "[      0 1022464]\n",
      "[      0 1022720]\n",
      "[      0 1022976]\n",
      "[      0 1023232]\n",
      "[      0 1023488]\n",
      "[      0 1023744]\n",
      "[      0 1024000]\n",
      "[      0 1024256]\n",
      "[      0 1024512]\n",
      "[      0 1024768]\n",
      "[      0 1025024]\n",
      "[      0 1025280]\n",
      "[      0 1025536]\n",
      "[      0 1025792]\n",
      "[      0 1026048]\n",
      "[      0 1026304]\n",
      "[      0 1026560]\n",
      "[      0 1026816]\n",
      "[      0 1027072]\n",
      "[      0 1027328]\n",
      "[      0 1027584]\n",
      "[      0 1027840]\n",
      "[      0 1028096]\n",
      "[      0 1028352]\n",
      "[      0 1028608]\n",
      "[      0 1028864]\n",
      "[      0 1029120]\n",
      "[      0 1029376]\n",
      "[      0 1029632]\n",
      "[      0 1029888]\n",
      "[      0 1030144]\n",
      "[      0 1030400]\n",
      "[      0 1030656]\n",
      "[      0 1030912]\n",
      "[      0 1031168]\n",
      "[      0 1031424]\n",
      "[      0 1031680]\n",
      "[      0 1031936]\n",
      "[      0 1032192]\n",
      "[      0 1032448]\n",
      "[      0 1032704]\n",
      "[      0 1032960]\n",
      "[      0 1033216]\n",
      "[      0 1033472]\n",
      "[      0 1033728]\n",
      "[      0 1033984]\n",
      "[      0 1034240]\n",
      "[      0 1034496]\n",
      "[      0 1034752]\n",
      "[      0 1035008]\n",
      "[      0 1035264]\n",
      "[      0 1035520]\n",
      "[      0 1035776]\n",
      "[      0 1036032]\n",
      "[      0 1036288]\n",
      "[      0 1036544]\n",
      "[      0 1036800]\n",
      "[      0 1037056]\n",
      "[      0 1037312]\n",
      "[      0 1037568]\n",
      "[      0 1037824]\n",
      "[      0 1038080]\n",
      "[      0 1038336]\n",
      "[      0 1038592]\n",
      "[      0 1038848]\n",
      "[      0 1039104]\n",
      "[      0 1039360]\n",
      "[      0 1039616]\n",
      "[      0 1039872]\n",
      "[      0 1040128]\n",
      "[      0 1040384]\n",
      "[      0 1040640]\n",
      "[      0 1040896]\n",
      "[      0 1041152]\n",
      "[      0 1041408]\n",
      "[      0 1041664]\n",
      "[      0 1041920]\n",
      "[      0 1042176]\n",
      "[      0 1042432]\n",
      "[      0 1042688]\n",
      "[      0 1042944]\n",
      "[      0 1043200]\n",
      "[      0 1043456]\n",
      "[      0 1043712]\n",
      "[      0 1043968]\n",
      "[      0 1044224]\n",
      "[      0 1044480]\n",
      "[      0 1044736]\n",
      "[      0 1044992]\n",
      "[      0 1045248]\n",
      "[      0 1045504]\n",
      "[      0 1045760]\n",
      "[      0 1046016]\n",
      "[      0 1046272]\n",
      "[      0 1046528]\n",
      "[      0 1046784]\n",
      "[      0 1047040]\n",
      "[      0 1047296]\n",
      "[      0 1047552]\n",
      "[      0 1047808]\n",
      "[      0 1048064]\n",
      "[      0 1048320]\n",
      "[      0 1048576]\n",
      "[      0 1048832]\n",
      "[      0 1049088]\n",
      "[      0 1049344]\n",
      "[      0 1049600]\n",
      "[      0 1049856]\n",
      "[      0 1050112]\n",
      "[      0 1050368]\n",
      "[      0 1050624]\n",
      "[      0 1050880]\n",
      "[      0 1051136]\n",
      "[      0 1051392]\n",
      "[      0 1051648]\n",
      "[      0 1051904]\n",
      "[      0 1052160]\n",
      "[      0 1052416]\n",
      "[      0 1052672]\n",
      "[      0 1052928]\n",
      "[      0 1053184]\n",
      "[      0 1053440]\n",
      "[      0 1053696]\n",
      "[      0 1053952]\n",
      "[      0 1054208]\n",
      "[      0 1054464]\n",
      "[      0 1054720]\n",
      "[      0 1054976]\n",
      "[      0 1055232]\n",
      "[      0 1055488]\n",
      "[      0 1055744]\n",
      "[      0 1056000]\n",
      "[      0 1056256]\n",
      "[      0 1056512]\n",
      "[      0 1056768]\n",
      "[      0 1057024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1057280]\n",
      "[      0 1057536]\n",
      "[      0 1057792]\n",
      "[      0 1058048]\n",
      "[      0 1058304]\n",
      "[      0 1058560]\n",
      "[      0 1058816]\n",
      "[      0 1059072]\n",
      "[      0 1059328]\n",
      "[      0 1059584]\n",
      "[      0 1059840]\n",
      "[      0 1060096]\n",
      "[      0 1060352]\n",
      "[      0 1060608]\n",
      "[      0 1060864]\n",
      "[      0 1061120]\n",
      "[      0 1061376]\n",
      "[      0 1061632]\n",
      "[      0 1061888]\n",
      "[      0 1062144]\n",
      "[      0 1062400]\n",
      "[      0 1062656]\n",
      "[      0 1062912]\n",
      "[      0 1063168]\n",
      "[      0 1063424]\n",
      "[      0 1063680]\n",
      "[      0 1063936]\n",
      "[      0 1064192]\n",
      "[      0 1064448]\n",
      "[      0 1064704]\n",
      "[      0 1064960]\n",
      "[      0 1065216]\n",
      "[      0 1065472]\n",
      "[      0 1065728]\n",
      "[      0 1065984]\n",
      "[      0 1066240]\n",
      "[      0 1066496]\n",
      "[      0 1066752]\n",
      "[      0 1067008]\n",
      "[      0 1067264]\n",
      "[      0 1067520]\n",
      "[      0 1067776]\n",
      "[      0 1068032]\n",
      "[      0 1068288]\n",
      "[      0 1068544]\n",
      "[      0 1068800]\n",
      "[      0 1069056]\n",
      "[      0 1069312]\n",
      "[      0 1069568]\n",
      "[      0 1069824]\n",
      "[      0 1070080]\n",
      "[      0 1070336]\n",
      "[      0 1070592]\n",
      "[      0 1070848]\n",
      "[      0 1071104]\n",
      "[      0 1071360]\n",
      "[      0 1071616]\n",
      "[      0 1071872]\n",
      "[      0 1072128]\n",
      "[      0 1072384]\n",
      "[      0 1072640]\n",
      "[      0 1072896]\n",
      "[      0 1073152]\n",
      "[      0 1073408]\n",
      "[      0 1073664]\n",
      "[      0 1073920]\n",
      "[      0 1074176]\n",
      "[      0 1074432]\n",
      "[      0 1074688]\n",
      "[      0 1074944]\n",
      "[      0 1075200]\n",
      "[      0 1075456]\n",
      "[      0 1075712]\n",
      "[      0 1075968]\n",
      "[      0 1076224]\n",
      "[      0 1076480]\n",
      "[      0 1076736]\n",
      "[      0 1076992]\n",
      "[      0 1077248]\n",
      "[      0 1077504]\n",
      "[      0 1077760]\n",
      "[      0 1078016]\n",
      "[      0 1078272]\n",
      "[      0 1078528]\n",
      "[      0 1078784]\n",
      "[      0 1079040]\n",
      "[      0 1079296]\n",
      "[      0 1079552]\n",
      "[      0 1079808]\n",
      "[      0 1080064]\n",
      "[      0 1080320]\n",
      "[      0 1080576]\n",
      "[      0 1080832]\n",
      "[      0 1081088]\n",
      "[      0 1081344]\n",
      "[      0 1081600]\n",
      "[      0 1081856]\n",
      "[      0 1082112]\n",
      "[      0 1082368]\n",
      "[      0 1082624]\n",
      "[      0 1082880]\n",
      "[      0 1083136]\n",
      "[      0 1083392]\n",
      "[      0 1083648]\n",
      "[      0 1083904]\n",
      "[      0 1084160]\n",
      "[      0 1084416]\n",
      "[      0 1084672]\n",
      "[      0 1084928]\n",
      "[      0 1085184]\n",
      "[      0 1085440]\n",
      "[      0 1085696]\n",
      "[      0 1085952]\n",
      "[      0 1086208]\n",
      "[      0 1086464]\n",
      "[      0 1086720]\n",
      "[      0 1086976]\n",
      "[      0 1087232]\n",
      "[      0 1087488]\n",
      "[      0 1087744]\n",
      "[      0 1088000]\n",
      "[      0 1088256]\n",
      "[      0 1088512]\n",
      "[      0 1088768]\n",
      "[      0 1089024]\n",
      "[      0 1089280]\n",
      "[      0 1089536]\n",
      "[      0 1089792]\n",
      "[      0 1090048]\n",
      "[      0 1090304]\n",
      "[      0 1090560]\n",
      "[      0 1090816]\n",
      "[      0 1091072]\n",
      "[      0 1091328]\n",
      "[      0 1091584]\n",
      "[      0 1091840]\n",
      "[      0 1092096]\n",
      "[      0 1092352]\n",
      "[      0 1092608]\n",
      "[      0 1092864]\n",
      "[      0 1093120]\n",
      "[      0 1093376]\n",
      "[      0 1093632]\n",
      "[      0 1093888]\n",
      "[      0 1094144]\n",
      "[      0 1094400]\n",
      "[      0 1094656]\n",
      "[      0 1094912]\n",
      "[      0 1095168]\n",
      "[      0 1095424]\n",
      "[      0 1095680]\n",
      "[      0 1095936]\n",
      "[      0 1096192]\n",
      "[      0 1096448]\n",
      "[      0 1096704]\n",
      "[      0 1096960]\n",
      "[      0 1097216]\n",
      "[      0 1097472]\n",
      "[      0 1097728]\n",
      "[      0 1097984]\n",
      "[      0 1098240]\n",
      "[      0 1098496]\n",
      "[      0 1098752]\n",
      "[      0 1099008]\n",
      "[      0 1099264]\n",
      "[      0 1099520]\n",
      "[      0 1099776]\n",
      "[      0 1100032]\n",
      "[      0 1100288]\n",
      "[      0 1100544]\n",
      "[      0 1100800]\n",
      "[      0 1101056]\n",
      "[      0 1101312]\n",
      "[      0 1101568]\n",
      "[      0 1101824]\n",
      "[      0 1102080]\n",
      "[      0 1102336]\n",
      "[      0 1102592]\n",
      "[      0 1102848]\n",
      "[      0 1103104]\n",
      "[      0 1103360]\n",
      "[      0 1103616]\n",
      "[      0 1103872]\n",
      "[      0 1104128]\n",
      "[      0 1104384]\n",
      "[      0 1104640]\n",
      "[      0 1104896]\n",
      "[      0 1105152]\n",
      "[      0 1105408]\n",
      "[      0 1105664]\n",
      "[      0 1105920]\n",
      "[      0 1106176]\n",
      "[      0 1106432]\n",
      "[      0 1106688]\n",
      "[      0 1106944]\n",
      "[      0 1107200]\n",
      "[      0 1107456]\n",
      "[      0 1107712]\n",
      "[      0 1107968]\n",
      "[      0 1108224]\n",
      "[      0 1108480]\n",
      "[      0 1108736]\n",
      "[      0 1108992]\n",
      "[      0 1109248]\n",
      "[      0 1109504]\n",
      "[      0 1109760]\n",
      "[      0 1110016]\n",
      "[      0 1110272]\n",
      "[      0 1110528]\n",
      "[      0 1110784]\n",
      "[      0 1111040]\n",
      "[      0 1111296]\n",
      "[      0 1111552]\n",
      "[      0 1111808]\n",
      "[      0 1112064]\n",
      "[      0 1112320]\n",
      "[      0 1112576]\n",
      "[      0 1112832]\n",
      "[      0 1113088]\n",
      "[      0 1113344]\n",
      "[      0 1113600]\n",
      "[      0 1113856]\n",
      "[      0 1114112]\n",
      "[      0 1114368]\n",
      "[      0 1114624]\n",
      "[      0 1114880]\n",
      "[      0 1115136]\n",
      "[      0 1115392]\n",
      "[      0 1115648]\n",
      "[      0 1115904]\n",
      "[      0 1116160]\n",
      "[      0 1116416]\n",
      "[      0 1116672]\n",
      "[      0 1116928]\n",
      "[      0 1117184]\n",
      "[      0 1117440]\n",
      "[      0 1117696]\n",
      "[      0 1117952]\n",
      "[      0 1118208]\n",
      "[      0 1118464]\n",
      "[      0 1118720]\n",
      "[      0 1118976]\n",
      "[      0 1119232]\n",
      "[      0 1119488]\n",
      "[      0 1119744]\n",
      "[      0 1120000]\n",
      "[      0 1120256]\n",
      "[      0 1120512]\n",
      "[      0 1120768]\n",
      "[      0 1121024]\n",
      "[      0 1121280]\n",
      "[      0 1121536]\n",
      "[      0 1121792]\n",
      "[      0 1122048]\n",
      "[      0 1122304]\n",
      "[      0 1122560]\n",
      "[      0 1122816]\n",
      "[      0 1123072]\n",
      "[      0 1123328]\n",
      "[      0 1123584]\n",
      "[      0 1123840]\n",
      "[      0 1124096]\n",
      "[      0 1124352]\n",
      "[      0 1124608]\n",
      "[      0 1124864]\n",
      "[      0 1125120]\n",
      "[      0 1125376]\n",
      "[      0 1125632]\n",
      "[      0 1125888]\n",
      "[      0 1126144]\n",
      "[      0 1126400]\n",
      "[      0 1126656]\n",
      "[      0 1126912]\n",
      "[      0 1127168]\n",
      "[      0 1127424]\n",
      "[      0 1127680]\n",
      "[      0 1127936]\n",
      "[      0 1128192]\n",
      "[      0 1128448]\n",
      "[      0 1128704]\n",
      "[      0 1128960]\n",
      "[      0 1129216]\n",
      "[      0 1129472]\n",
      "[      0 1129728]\n",
      "[      0 1129984]\n",
      "[      0 1130240]\n",
      "[      0 1130496]\n",
      "[      0 1130752]\n",
      "[      0 1131008]\n",
      "[      0 1131264]\n",
      "[      0 1131520]\n",
      "[      0 1131776]\n",
      "[      0 1132032]\n",
      "[      0 1132288]\n",
      "[      0 1132544]\n",
      "[      0 1132800]\n",
      "[      0 1133056]\n",
      "[      0 1133312]\n",
      "[      0 1133568]\n",
      "[      0 1133824]\n",
      "[      0 1134080]\n",
      "[      0 1134336]\n",
      "[      0 1134592]\n",
      "[      0 1134848]\n",
      "[      0 1135104]\n",
      "[      0 1135360]\n",
      "[      0 1135616]\n",
      "[      0 1135872]\n",
      "[      0 1136128]\n",
      "[      0 1136384]\n",
      "[      0 1136640]\n",
      "[      0 1136896]\n",
      "[      0 1137152]\n",
      "[      0 1137408]\n",
      "[      0 1137664]\n",
      "[      0 1137920]\n",
      "[      0 1138176]\n",
      "[      0 1138432]\n",
      "[      0 1138688]\n",
      "[      0 1138944]\n",
      "[      0 1139200]\n",
      "[      0 1139456]\n",
      "[      0 1139712]\n",
      "[      0 1139968]\n",
      "[      0 1140224]\n",
      "[      0 1140480]\n",
      "[      0 1140736]\n",
      "[      0 1140992]\n",
      "[      0 1141248]\n",
      "[      0 1141504]\n",
      "[      0 1141760]\n",
      "[      0 1142016]\n",
      "[      0 1142272]\n",
      "[      0 1142528]\n",
      "[      0 1142784]\n",
      "[      0 1143040]\n",
      "[      0 1143296]\n",
      "[      0 1143552]\n",
      "[      0 1143808]\n",
      "[      0 1144064]\n",
      "[      0 1144320]\n",
      "[      0 1144576]\n",
      "[      0 1144832]\n",
      "[      0 1145088]\n",
      "[      0 1145344]\n",
      "[      0 1145600]\n",
      "[      0 1145856]\n",
      "[      0 1146112]\n",
      "[      0 1146368]\n",
      "[      0 1146624]\n",
      "[      0 1146880]\n",
      "[      0 1147136]\n",
      "[      0 1147392]\n",
      "[      0 1147648]\n",
      "[      0 1147904]\n",
      "[      0 1148160]\n",
      "[      0 1148416]\n",
      "[      0 1148672]\n",
      "[      0 1148928]\n",
      "[      0 1149184]\n",
      "[      0 1149440]\n",
      "[      0 1149696]\n",
      "[      0 1149952]\n",
      "[      0 1150208]\n",
      "[      0 1150464]\n",
      "[      0 1150720]\n",
      "[      0 1150976]\n",
      "[      0 1151232]\n",
      "[      0 1151488]\n",
      "[      0 1151744]\n",
      "[      0 1152000]\n",
      "[      0 1152256]\n",
      "[      0 1152512]\n",
      "[      0 1152768]\n",
      "[      0 1153024]\n",
      "[      0 1153280]\n",
      "[      0 1153536]\n",
      "[      0 1153792]\n",
      "[      0 1154048]\n",
      "[      0 1154304]\n",
      "[      0 1154560]\n",
      "[      0 1154816]\n",
      "[      0 1155072]\n",
      "[      0 1155328]\n",
      "[      0 1155584]\n",
      "[      0 1155840]\n",
      "[      0 1156096]\n",
      "[      0 1156352]\n",
      "[      0 1156608]\n",
      "[      0 1156864]\n",
      "[      0 1157120]\n",
      "[      0 1157376]\n",
      "[      0 1157632]\n",
      "[      0 1157888]\n",
      "[      0 1158144]\n",
      "[      0 1158400]\n",
      "[      0 1158656]\n",
      "[      0 1158912]\n",
      "[      0 1159168]\n",
      "[      0 1159424]\n",
      "[      0 1159680]\n",
      "[      0 1159936]\n",
      "[      0 1160192]\n",
      "[      0 1160448]\n",
      "[      0 1160704]\n",
      "[      0 1160960]\n",
      "[      0 1161216]\n",
      "[      0 1161472]\n",
      "[      0 1161728]\n",
      "[      0 1161984]\n",
      "[      0 1162240]\n",
      "[      0 1162496]\n",
      "[      0 1162752]\n",
      "[      0 1163008]\n",
      "[      0 1163264]\n",
      "[      0 1163520]\n",
      "[      0 1163776]\n",
      "[      0 1164032]\n",
      "[      0 1164288]\n",
      "[      0 1164544]\n",
      "[      0 1164800]\n",
      "[      0 1165056]\n",
      "[      0 1165312]\n",
      "[      0 1165568]\n",
      "[      0 1165824]\n",
      "[      0 1166080]\n",
      "[      0 1166336]\n",
      "[      0 1166592]\n",
      "[      0 1166848]\n",
      "[      0 1167104]\n",
      "[      0 1167360]\n",
      "[      0 1167616]\n",
      "[      0 1167872]\n",
      "[      0 1168128]\n",
      "[      0 1168384]\n",
      "[      0 1168640]\n",
      "[      0 1168896]\n",
      "[      0 1169152]\n",
      "[      0 1169408]\n",
      "[      0 1169664]\n",
      "[      0 1169920]\n",
      "[      0 1170176]\n",
      "[      0 1170432]\n",
      "[      0 1170688]\n",
      "[      0 1170944]\n",
      "[      0 1171200]\n",
      "[      0 1171456]\n",
      "[      0 1171712]\n",
      "[      0 1171968]\n",
      "[      0 1172224]\n",
      "[      0 1172480]\n",
      "[      0 1172736]\n",
      "[      0 1172992]\n",
      "[      0 1173248]\n",
      "[      0 1173504]\n",
      "[      0 1173760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1174016]\n",
      "[      0 1174272]\n",
      "[      0 1174528]\n",
      "[      0 1174784]\n",
      "[      0 1175040]\n",
      "[      0 1175296]\n",
      "[      0 1175552]\n",
      "[      0 1175808]\n",
      "[      0 1176064]\n",
      "[      0 1176320]\n",
      "[      0 1176576]\n",
      "[      0 1176832]\n",
      "[      0 1177088]\n",
      "[      0 1177344]\n",
      "[      0 1177600]\n",
      "[      0 1177856]\n",
      "[      0 1178112]\n",
      "[      0 1178368]\n",
      "[      0 1178624]\n",
      "[      0 1178880]\n",
      "[      0 1179136]\n",
      "[      0 1179392]\n",
      "[      0 1179648]\n",
      "[      0 1179904]\n",
      "[      0 1180160]\n",
      "[      0 1180416]\n",
      "[      0 1180672]\n",
      "[      0 1180928]\n",
      "[      0 1181184]\n",
      "[      0 1181440]\n",
      "[      0 1181696]\n",
      "[      0 1181952]\n",
      "[      0 1182208]\n",
      "[      0 1182464]\n",
      "[      0 1182720]\n",
      "[      0 1182976]\n",
      "[      0 1183232]\n",
      "[      0 1183488]\n",
      "[      0 1183744]\n",
      "[      0 1184000]\n",
      "[      0 1184256]\n",
      "[      0 1184512]\n",
      "[      0 1184768]\n",
      "[      0 1185024]\n",
      "[      0 1185280]\n",
      "[      0 1185536]\n",
      "[      0 1185792]\n",
      "[      0 1186048]\n",
      "[      0 1186304]\n",
      "[      0 1186560]\n",
      "[      0 1186816]\n",
      "[      0 1187072]\n",
      "[      0 1187328]\n",
      "[      0 1187584]\n",
      "[      0 1187840]\n",
      "[      0 1188096]\n",
      "[      0 1188352]\n",
      "[      0 1188608]\n",
      "[      0 1188864]\n",
      "[      0 1189120]\n",
      "[      0 1189376]\n",
      "[      0 1189632]\n",
      "[      0 1189888]\n",
      "[      0 1190144]\n",
      "[      0 1190400]\n",
      "[      0 1190656]\n",
      "[      0 1190912]\n",
      "[      0 1191168]\n",
      "[      0 1191424]\n",
      "[      0 1191680]\n",
      "[      0 1191936]\n",
      "[      0 1192192]\n",
      "[      0 1192448]\n",
      "[      0 1192704]\n",
      "[      0 1192960]\n",
      "[      0 1193216]\n",
      "[      0 1193472]\n",
      "[      0 1193728]\n",
      "[      0 1193984]\n",
      "[      0 1194240]\n",
      "[      0 1194496]\n",
      "[      0 1194752]\n",
      "[      0 1195008]\n",
      "[      0 1195264]\n",
      "[      0 1195520]\n",
      "[      0 1195776]\n",
      "[      0 1196032]\n",
      "[      0 1196288]\n",
      "[      0 1196544]\n",
      "[      0 1196800]\n",
      "[      0 1197056]\n",
      "[      0 1197312]\n",
      "[      0 1197568]\n",
      "[      0 1197824]\n",
      "[      0 1198080]\n",
      "[      0 1198336]\n",
      "[      0 1198592]\n",
      "[      0 1198848]\n",
      "[      0 1199104]\n",
      "[      0 1199360]\n",
      "[      0 1199616]\n",
      "[      0 1199872]\n",
      "[      0 1200128]\n",
      "[      0 1200384]\n",
      "[      0 1200640]\n",
      "[      0 1200896]\n",
      "[      0 1201152]\n",
      "[      0 1201408]\n",
      "[      0 1201664]\n",
      "[      0 1201920]\n",
      "[      0 1202176]\n",
      "[      0 1202432]\n",
      "[      0 1202688]\n",
      "[      0 1202944]\n",
      "[      0 1203200]\n",
      "[      0 1203456]\n",
      "[      0 1203712]\n",
      "[      0 1203968]\n",
      "[      0 1204224]\n",
      "[      0 1204480]\n",
      "[      0 1204736]\n",
      "[      0 1204992]\n",
      "[      0 1205248]\n",
      "[      0 1205504]\n",
      "[      0 1205760]\n",
      "[      0 1206016]\n",
      "[      0 1206272]\n",
      "[      0 1206528]\n",
      "[      0 1206784]\n",
      "[      0 1207040]\n",
      "[      0 1207296]\n",
      "[      0 1207552]\n",
      "[      0 1207808]\n",
      "[      0 1208064]\n",
      "[      0 1208320]\n",
      "[      0 1208576]\n",
      "[      0 1208832]\n",
      "[      0 1209088]\n",
      "[      0 1209344]\n",
      "[      0 1209600]\n",
      "[      0 1209856]\n",
      "[      0 1210112]\n",
      "[      0 1210368]\n",
      "[      0 1210624]\n",
      "[      0 1210880]\n",
      "[      0 1211136]\n",
      "[      0 1211392]\n",
      "[      0 1211648]\n",
      "[      0 1211904]\n",
      "[      0 1212160]\n",
      "[      0 1212416]\n",
      "[      0 1212672]\n",
      "[      0 1212928]\n",
      "[      0 1213184]\n",
      "[      0 1213440]\n",
      "[      0 1213696]\n",
      "[      0 1213952]\n",
      "[      0 1214208]\n",
      "[      0 1214464]\n",
      "[      0 1214720]\n",
      "[      0 1214976]\n",
      "[      0 1215232]\n",
      "[      0 1215488]\n",
      "[      0 1215744]\n",
      "[      0 1216000]\n",
      "[      0 1216256]\n",
      "[      0 1216512]\n",
      "[      0 1216768]\n",
      "[      0 1217024]\n",
      "[      0 1217280]\n",
      "[      0 1217536]\n",
      "[      0 1217792]\n",
      "[      0 1218048]\n",
      "[      0 1218304]\n",
      "[      0 1218560]\n",
      "[      0 1218816]\n",
      "[      0 1219072]\n",
      "[      0 1219328]\n",
      "[      0 1219584]\n",
      "[      0 1219840]\n",
      "[      0 1220096]\n",
      "[      0 1220352]\n",
      "[      0 1220608]\n",
      "[      0 1220864]\n",
      "[      0 1221120]\n",
      "[      0 1221376]\n",
      "[      0 1221632]\n",
      "[      0 1221888]\n",
      "[      0 1222144]\n",
      "[      0 1222400]\n",
      "[      0 1222656]\n",
      "[      0 1222912]\n",
      "[      0 1223168]\n",
      "[      0 1223424]\n",
      "[      0 1223680]\n",
      "[      0 1223936]\n",
      "[      0 1224192]\n",
      "[      0 1224448]\n",
      "[      0 1224704]\n",
      "[      0 1224960]\n",
      "[      0 1225216]\n",
      "[      0 1225472]\n",
      "[      0 1225728]\n",
      "[      0 1225984]\n",
      "[      0 1226240]\n",
      "[      0 1226496]\n",
      "[      0 1226752]\n",
      "[      0 1227008]\n",
      "[      0 1227264]\n",
      "[      0 1227520]\n",
      "[      0 1227776]\n",
      "[      0 1228032]\n",
      "[      0 1228288]\n",
      "[      0 1228544]\n",
      "[      0 1228800]\n",
      "[      0 1229056]\n",
      "[      0 1229312]\n",
      "[      0 1229568]\n",
      "[      0 1229824]\n",
      "[      0 1230080]\n",
      "[      0 1230336]\n",
      "[      0 1230592]\n",
      "[      0 1230848]\n",
      "[      0 1231104]\n",
      "[      0 1231360]\n",
      "[      0 1231616]\n",
      "[      0 1231872]\n",
      "[      0 1232128]\n",
      "[      0 1232384]\n",
      "[      0 1232640]\n",
      "[      0 1232896]\n",
      "[      0 1233152]\n",
      "[      0 1233408]\n",
      "[      0 1233664]\n",
      "[      0 1233920]\n",
      "[      0 1234176]\n",
      "[      0 1234432]\n",
      "[      0 1234688]\n",
      "[      0 1234944]\n",
      "[      0 1235200]\n",
      "[      0 1235456]\n",
      "[      0 1235712]\n",
      "[      0 1235968]\n",
      "[      0 1236224]\n",
      "[      0 1236480]\n",
      "[      0 1236736]\n",
      "[      0 1236992]\n",
      "[      0 1237248]\n",
      "[      0 1237504]\n",
      "[      0 1237760]\n",
      "[      0 1238016]\n",
      "[      0 1238272]\n",
      "[      0 1238528]\n",
      "[      0 1238784]\n",
      "[      0 1239040]\n",
      "[      0 1239296]\n",
      "[      0 1239552]\n",
      "[      0 1239808]\n",
      "[      0 1240064]\n",
      "[      0 1240320]\n",
      "[      0 1240576]\n",
      "[      0 1240832]\n",
      "[      0 1241088]\n",
      "[      0 1241344]\n",
      "[      0 1241600]\n",
      "[      0 1241856]\n",
      "[      0 1242112]\n",
      "[      0 1242368]\n",
      "[      0 1242624]\n",
      "[      0 1242880]\n",
      "[      0 1243136]\n",
      "[      0 1243392]\n",
      "[      0 1243648]\n",
      "[      0 1243904]\n",
      "[      0 1244160]\n",
      "[      0 1244416]\n",
      "[      0 1244672]\n",
      "[      0 1244928]\n",
      "[      0 1245184]\n",
      "[      0 1245440]\n",
      "[      0 1245696]\n",
      "[      0 1245952]\n",
      "[      0 1246208]\n",
      "[      0 1246464]\n",
      "[      0 1246720]\n",
      "[      0 1246976]\n",
      "[      0 1247232]\n",
      "[      0 1247488]\n",
      "[      0 1247744]\n",
      "[      0 1248000]\n",
      "[      0 1248256]\n",
      "[      0 1248512]\n",
      "[      0 1248768]\n",
      "[      0 1249024]\n",
      "[      0 1249280]\n",
      "[      0 1249536]\n",
      "[      0 1249792]\n",
      "[      0 1250048]\n",
      "[      0 1250304]\n",
      "[      0 1250560]\n",
      "[      0 1250816]\n",
      "[      0 1251072]\n",
      "[      0 1251328]\n",
      "[      0 1251584]\n",
      "[      0 1251840]\n",
      "[      0 1252096]\n",
      "[      0 1252352]\n",
      "[      0 1252608]\n",
      "[      0 1252864]\n",
      "[      0 1253120]\n",
      "[      0 1253376]\n",
      "[      0 1253632]\n",
      "[      0 1253888]\n",
      "[      0 1254144]\n",
      "[      0 1254400]\n",
      "[      0 1254656]\n",
      "[      0 1254912]\n",
      "[      0 1255168]\n",
      "[      0 1255424]\n",
      "[      0 1255680]\n",
      "[      0 1255936]\n",
      "[      0 1256192]\n",
      "[      0 1256448]\n",
      "[      0 1256704]\n",
      "[      0 1256960]\n",
      "[      0 1257216]\n",
      "[      0 1257472]\n",
      "[      0 1257728]\n",
      "[      0 1257984]\n",
      "[      0 1258240]\n",
      "[      0 1258496]\n",
      "[      0 1258752]\n",
      "[      0 1259008]\n",
      "[      0 1259264]\n",
      "[      0 1259520]\n",
      "[      0 1259776]\n",
      "[      0 1260032]\n",
      "[      0 1260288]\n",
      "[      0 1260544]\n",
      "[      0 1260800]\n",
      "[      0 1261056]\n",
      "[      0 1261312]\n",
      "[      0 1261568]\n",
      "[      0 1261824]\n",
      "[      0 1262080]\n",
      "[      0 1262336]\n",
      "[      0 1262592]\n",
      "[      0 1262848]\n",
      "[      0 1263104]\n",
      "[      0 1263360]\n",
      "[      0 1263616]\n",
      "[      0 1263872]\n",
      "[      0 1264128]\n",
      "[      0 1264384]\n",
      "[      0 1264640]\n",
      "[      0 1264896]\n",
      "[      0 1265152]\n",
      "[      0 1265408]\n",
      "[      0 1265664]\n",
      "[      0 1265920]\n",
      "[      0 1266176]\n",
      "[      0 1266432]\n",
      "[      0 1266688]\n",
      "[      0 1266944]\n",
      "[      0 1267200]\n",
      "[      0 1267456]\n",
      "[      0 1267712]\n",
      "[      0 1267968]\n",
      "[      0 1268224]\n",
      "[      0 1268480]\n",
      "[      0 1268736]\n",
      "[      0 1268992]\n",
      "[      0 1269248]\n",
      "[      0 1269504]\n",
      "[      0 1269760]\n",
      "[      0 1270016]\n",
      "[      0 1270272]\n",
      "[      0 1270528]\n",
      "[      0 1270784]\n",
      "[      0 1271040]\n",
      "[      0 1271296]\n",
      "[      0 1271552]\n",
      "[      0 1271808]\n",
      "[      0 1272064]\n",
      "[      0 1272320]\n",
      "[      0 1272576]\n",
      "[      0 1272832]\n",
      "[      0 1273088]\n",
      "[      0 1273344]\n",
      "[      0 1273600]\n",
      "[      0 1273856]\n",
      "[      0 1274112]\n",
      "[      0 1274368]\n",
      "[      0 1274624]\n",
      "[      0 1274880]\n",
      "[      0 1275136]\n",
      "[      0 1275392]\n",
      "[      0 1275648]\n",
      "[      0 1275904]\n",
      "[      0 1276160]\n",
      "[      0 1276416]\n",
      "[      0 1276672]\n",
      "[      0 1276928]\n",
      "[      0 1277184]\n",
      "[      0 1277440]\n",
      "[      0 1277696]\n",
      "[      0 1277952]\n",
      "[      0 1278208]\n",
      "[      0 1278464]\n",
      "[      0 1278720]\n",
      "[      0 1278976]\n",
      "[      0 1279232]\n",
      "[      0 1279488]\n",
      "[      0 1279744]\n",
      "[      0 1280000]\n",
      "[      0 1280256]\n",
      "[      0 1280512]\n",
      "[      0 1280768]\n",
      "[      0 1281024]\n",
      "[      0 1281280]\n",
      "[      0 1281536]\n",
      "[      0 1281792]\n",
      "[      0 1282048]\n",
      "[      0 1282304]\n",
      "[      0 1282560]\n",
      "[      0 1282816]\n",
      "[      0 1283072]\n",
      "[      0 1283328]\n",
      "[      0 1283584]\n",
      "[      0 1283840]\n",
      "[      0 1284096]\n",
      "[      0 1284352]\n",
      "[      0 1284608]\n",
      "[      0 1284864]\n",
      "[      0 1285120]\n",
      "[      0 1285376]\n",
      "[      0 1285632]\n",
      "[      0 1285888]\n",
      "[      0 1286144]\n",
      "[      0 1286400]\n",
      "[      0 1286656]\n",
      "[      0 1286912]\n",
      "[      0 1287168]\n",
      "[      0 1287424]\n",
      "[      0 1287680]\n",
      "[      0 1287936]\n",
      "[      0 1288192]\n",
      "[      0 1288448]\n",
      "[      0 1288704]\n",
      "[      0 1288960]\n",
      "[      0 1289216]\n",
      "[      0 1289472]\n",
      "[      0 1289728]\n",
      "[      0 1289984]\n",
      "[      0 1290240]\n",
      "[      0 1290496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1290752]\n",
      "[      0 1291008]\n",
      "[      0 1291264]\n",
      "[      0 1291520]\n",
      "[      0 1291776]\n",
      "[      0 1292032]\n",
      "[      0 1292288]\n",
      "[      0 1292544]\n",
      "[      0 1292800]\n",
      "[      0 1293056]\n",
      "[      0 1293312]\n",
      "[      0 1293568]\n",
      "[      0 1293824]\n",
      "[      0 1294080]\n",
      "[      0 1294336]\n",
      "[      0 1294592]\n",
      "[      0 1294848]\n",
      "[      0 1295104]\n",
      "[      0 1295360]\n",
      "[      0 1295616]\n",
      "[      0 1295872]\n",
      "[      0 1296128]\n",
      "[      0 1296384]\n",
      "[      0 1296640]\n",
      "[      0 1296896]\n",
      "[      0 1297152]\n",
      "[      0 1297408]\n",
      "[      0 1297664]\n",
      "[      0 1297920]\n",
      "[      0 1298176]\n",
      "[      0 1298432]\n",
      "[      0 1298688]\n",
      "[      0 1298944]\n",
      "[      0 1299200]\n",
      "[      0 1299456]\n",
      "[      0 1299712]\n",
      "[      0 1299968]\n",
      "[      0 1300224]\n",
      "[      0 1300480]\n",
      "[      0 1300736]\n",
      "[      0 1300992]\n",
      "[      0 1301248]\n",
      "[      0 1301504]\n",
      "[      0 1301760]\n",
      "[      0 1302016]\n",
      "[      0 1302272]\n",
      "[      0 1302528]\n",
      "[      0 1302784]\n",
      "[      0 1303040]\n",
      "[      0 1303296]\n",
      "[      0 1303552]\n",
      "[      0 1303808]\n",
      "[      0 1304064]\n",
      "[      0 1304320]\n",
      "[      0 1304576]\n",
      "[      0 1304832]\n",
      "[      0 1305088]\n",
      "[      0 1305344]\n",
      "[      0 1305600]\n",
      "[      0 1305856]\n",
      "[      0 1306112]\n",
      "[      0 1306368]\n",
      "[      0 1306624]\n",
      "[      0 1306880]\n",
      "[      0 1307136]\n",
      "[      0 1307392]\n",
      "[      0 1307648]\n",
      "[      0 1307904]\n",
      "[      0 1308160]\n",
      "[      0 1308416]\n",
      "[      0 1308672]\n",
      "[      0 1308928]\n",
      "[      0 1309184]\n",
      "[      0 1309440]\n",
      "[      0 1309696]\n",
      "[      0 1309952]\n",
      "[      0 1310208]\n",
      "[      0 1310464]\n",
      "[      0 1310720]\n",
      "[      0 1310976]\n",
      "[      0 1311232]\n",
      "[      0 1311488]\n",
      "[      0 1311744]\n",
      "[      0 1312000]\n",
      "[      0 1312256]\n",
      "[      0 1312512]\n",
      "[      0 1312768]\n",
      "[      0 1313024]\n",
      "[      0 1313280]\n",
      "[      0 1313536]\n",
      "[      0 1313792]\n",
      "[      0 1314048]\n",
      "[      0 1314304]\n",
      "[      0 1314560]\n",
      "[      0 1314816]\n",
      "[      0 1315072]\n",
      "[      0 1315328]\n",
      "[      0 1315584]\n",
      "[      0 1315840]\n",
      "[      0 1316096]\n",
      "[      0 1316352]\n",
      "[      0 1316608]\n",
      "[      0 1316864]\n",
      "[      0 1317120]\n",
      "[      0 1317376]\n",
      "[      0 1317632]\n",
      "[      0 1317888]\n",
      "[      0 1318144]\n",
      "[      0 1318400]\n",
      "[      0 1318656]\n",
      "[      0 1318912]\n",
      "[      0 1319168]\n",
      "[      0 1319424]\n",
      "[      0 1319680]\n",
      "[      0 1319936]\n",
      "[      0 1320192]\n",
      "[      0 1320448]\n",
      "[      0 1320704]\n",
      "[      0 1320960]\n",
      "[      0 1321216]\n",
      "[      0 1321472]\n",
      "[      0 1321728]\n",
      "[      0 1321984]\n",
      "[      0 1322240]\n",
      "[      0 1322496]\n",
      "[      0 1322752]\n",
      "[      0 1323008]\n",
      "[      0 1323264]\n",
      "[      0 1323520]\n",
      "[      0 1323776]\n",
      "[      0 1324032]\n",
      "[      0 1324288]\n",
      "[      0 1324544]\n",
      "[      0 1324800]\n",
      "[      0 1325056]\n",
      "[      0 1325312]\n",
      "[      0 1325568]\n",
      "[      0 1325824]\n",
      "[      0 1326080]\n",
      "[      0 1326336]\n",
      "[      0 1326592]\n",
      "[      0 1326848]\n",
      "[      0 1327104]\n",
      "[      0 1327360]\n",
      "[      0 1327616]\n",
      "[      0 1327872]\n",
      "[      0 1328128]\n",
      "[      0 1328384]\n",
      "[      0 1328640]\n",
      "[      0 1328896]\n",
      "[      0 1329152]\n",
      "[      0 1329408]\n",
      "[      0 1329664]\n",
      "[      0 1329920]\n",
      "[      0 1330176]\n",
      "[      0 1330432]\n",
      "[      0 1330688]\n",
      "[      0 1330944]\n",
      "[      0 1331200]\n",
      "[      0 1331456]\n",
      "[      0 1331712]\n",
      "[      0 1331968]\n",
      "[      0 1332224]\n",
      "[      0 1332480]\n",
      "[      0 1332736]\n",
      "[      0 1332992]\n",
      "[      0 1333248]\n",
      "[      0 1333504]\n",
      "[      0 1333760]\n",
      "[      0 1334016]\n",
      "[      0 1334272]\n",
      "[      0 1334528]\n",
      "[      0 1334784]\n",
      "[      0 1335040]\n",
      "[      0 1335296]\n",
      "[      0 1335552]\n",
      "[      0 1335808]\n",
      "[      0 1336064]\n",
      "[      0 1336320]\n",
      "[      0 1336576]\n",
      "[      0 1336832]\n",
      "[      0 1337088]\n",
      "[      0 1337344]\n",
      "[      0 1337600]\n",
      "[      0 1337856]\n",
      "[      0 1338112]\n",
      "[      0 1338368]\n",
      "[      0 1338624]\n",
      "[      0 1338880]\n",
      "[      0 1339136]\n",
      "[      0 1339392]\n",
      "[      0 1339648]\n",
      "[      0 1339904]\n",
      "[      0 1340160]\n",
      "[      0 1340416]\n",
      "[      0 1340672]\n",
      "[      0 1340928]\n",
      "[      0 1341184]\n",
      "[      0 1341440]\n",
      "[      0 1341696]\n",
      "[      0 1341952]\n",
      "[      0 1342208]\n",
      "[      0 1342464]\n",
      "[      0 1342720]\n",
      "[      0 1342976]\n",
      "[      0 1343232]\n",
      "[      0 1343488]\n",
      "[      0 1343744]\n",
      "[      0 1344000]\n",
      "[      0 1344256]\n",
      "[      0 1344512]\n",
      "[      0 1344768]\n",
      "[      0 1345024]\n",
      "[      0 1345280]\n",
      "[      0 1345536]\n",
      "[      0 1345792]\n",
      "[      0 1346048]\n",
      "[      0 1346304]\n",
      "[      0 1346560]\n",
      "[      0 1346816]\n",
      "[      0 1347072]\n",
      "[      0 1347328]\n",
      "[      0 1347584]\n",
      "[      0 1347840]\n",
      "[      0 1348096]\n",
      "[      0 1348352]\n",
      "[      0 1348608]\n",
      "[      0 1348864]\n",
      "[      0 1349120]\n",
      "[      0 1349376]\n",
      "[      0 1349632]\n",
      "[      0 1349888]\n",
      "[      0 1350144]\n",
      "[      0 1350400]\n",
      "[      0 1350656]\n",
      "[      0 1350912]\n",
      "[      0 1351168]\n",
      "[      0 1351424]\n",
      "[      0 1351680]\n",
      "[      0 1351936]\n",
      "[      0 1352192]\n",
      "[      0 1352448]\n",
      "[      0 1352704]\n",
      "[      0 1352960]\n",
      "[      0 1353216]\n",
      "[      0 1353472]\n",
      "[      0 1353728]\n",
      "[      0 1353984]\n",
      "[      0 1354240]\n",
      "[      0 1354496]\n",
      "[      0 1354752]\n",
      "[      0 1355008]\n",
      "[      0 1355264]\n",
      "[      0 1355520]\n",
      "[      0 1355776]\n",
      "[      0 1356032]\n",
      "[      0 1356288]\n",
      "[      0 1356544]\n",
      "[      0 1356800]\n",
      "[      0 1357056]\n",
      "[      0 1357312]\n",
      "[      0 1357568]\n",
      "[      0 1357824]\n",
      "[      0 1358080]\n",
      "[      0 1358336]\n",
      "[      0 1358592]\n",
      "[      0 1358848]\n",
      "[      0 1359104]\n",
      "[      0 1359360]\n",
      "[      0 1359616]\n",
      "[      0 1359872]\n",
      "[      0 1360128]\n",
      "[      0 1360384]\n",
      "[      0 1360640]\n",
      "[      0 1360896]\n",
      "[      0 1361152]\n",
      "[      0 1361408]\n",
      "[      0 1361664]\n",
      "[      0 1361920]\n",
      "[      0 1362176]\n",
      "[      0 1362432]\n",
      "[      0 1362688]\n",
      "[      0 1362944]\n",
      "[      0 1363200]\n",
      "[      0 1363456]\n",
      "[      0 1363712]\n",
      "[      0 1363968]\n",
      "[      0 1364224]\n",
      "[      0 1364480]\n",
      "[      0 1364736]\n",
      "[      0 1364992]\n",
      "[      0 1365248]\n",
      "[      0 1365504]\n",
      "[      0 1365760]\n",
      "[      0 1366016]\n",
      "[      0 1366272]\n",
      "[      0 1366528]\n",
      "[      0 1366784]\n",
      "[      0 1367040]\n",
      "[      0 1367296]\n",
      "[      0 1367552]\n",
      "[      0 1367808]\n",
      "[      0 1368064]\n",
      "[      0 1368320]\n",
      "[      0 1368576]\n",
      "[      0 1368832]\n",
      "[      0 1369088]\n",
      "[      0 1369344]\n",
      "[      0 1369600]\n",
      "[      0 1369856]\n",
      "[      0 1370112]\n",
      "[      0 1370368]\n",
      "[      0 1370624]\n",
      "[      0 1370880]\n",
      "[      0 1371136]\n",
      "[      0 1371392]\n",
      "[      0 1371648]\n",
      "[      0 1371904]\n",
      "[      0 1372160]\n",
      "[      0 1372416]\n",
      "[      0 1372672]\n",
      "[      0 1372928]\n",
      "[      0 1373184]\n",
      "[      0 1373440]\n",
      "[      0 1373696]\n",
      "[      0 1373952]\n",
      "[      0 1374208]\n",
      "[      0 1374464]\n",
      "[      0 1374720]\n",
      "[      0 1374976]\n",
      "[      0 1375232]\n",
      "[      0 1375488]\n",
      "[      0 1375744]\n",
      "[      0 1376000]\n",
      "[      0 1376256]\n",
      "[      0 1376512]\n",
      "[      0 1376768]\n",
      "[      0 1377024]\n",
      "[      0 1377280]\n",
      "[      0 1377536]\n",
      "[      0 1377792]\n",
      "[      0 1378048]\n",
      "[      0 1378304]\n",
      "[      0 1378560]\n",
      "[      0 1378816]\n",
      "[      0 1379072]\n",
      "[      0 1379328]\n",
      "[      0 1379584]\n",
      "[      0 1379840]\n",
      "[      0 1380096]\n",
      "[      0 1380352]\n",
      "[      0 1380608]\n",
      "[      0 1380864]\n",
      "[      0 1381120]\n",
      "[      0 1381376]\n",
      "[      0 1381632]\n",
      "[      0 1381888]\n",
      "[      0 1382144]\n",
      "[      0 1382400]\n",
      "[      0 1382656]\n",
      "[      0 1382912]\n",
      "[      0 1383168]\n",
      "[      0 1383424]\n",
      "[      0 1383680]\n",
      "[      0 1383936]\n",
      "[      0 1384192]\n",
      "[      0 1384448]\n",
      "[      0 1384704]\n",
      "[      0 1384960]\n",
      "[      0 1385216]\n",
      "[      0 1385472]\n",
      "[      0 1385728]\n",
      "[      0 1385984]\n",
      "[      0 1386240]\n",
      "[      0 1386496]\n",
      "[      0 1386752]\n",
      "[      0 1387008]\n",
      "[      0 1387264]\n",
      "[      0 1387520]\n",
      "[      0 1387776]\n",
      "[      0 1388032]\n",
      "[      0 1388288]\n",
      "[      0 1388544]\n",
      "[      0 1388800]\n",
      "[      0 1389056]\n",
      "[      0 1389312]\n",
      "[      0 1389568]\n",
      "[      0 1389824]\n",
      "[      0 1390080]\n",
      "[      0 1390336]\n",
      "[      0 1390592]\n",
      "[      0 1390848]\n",
      "[      0 1391104]\n",
      "[      0 1391360]\n",
      "[      0 1391616]\n",
      "[      0 1391872]\n",
      "[      0 1392128]\n",
      "[      0 1392384]\n",
      "[      0 1392640]\n",
      "[      0 1392896]\n",
      "[      0 1393152]\n",
      "[      0 1393408]\n",
      "[      0 1393664]\n",
      "[      0 1393920]\n",
      "[      0 1394176]\n",
      "[      0 1394432]\n",
      "[      0 1394688]\n",
      "[      0 1394944]\n",
      "[      0 1395200]\n",
      "[      0 1395456]\n",
      "[      0 1395712]\n",
      "[      0 1395968]\n",
      "[      0 1396224]\n",
      "[      0 1396480]\n",
      "[      0 1396736]\n",
      "[      0 1396992]\n",
      "[      0 1397248]\n",
      "[      0 1397504]\n",
      "[      0 1397760]\n",
      "[      0 1398016]\n",
      "[      0 1398272]\n",
      "[      0 1398528]\n",
      "[      0 1398784]\n",
      "[      0 1399040]\n",
      "[      0 1399296]\n",
      "[      0 1399552]\n",
      "[      0 1399808]\n",
      "[      0 1400064]\n",
      "[      0 1400320]\n",
      "[      0 1400576]\n",
      "[      0 1400832]\n",
      "[      0 1401088]\n",
      "[      0 1401344]\n",
      "[      0 1401600]\n",
      "[      0 1401856]\n",
      "[      0 1402112]\n",
      "[      0 1402368]\n",
      "[      0 1402624]\n",
      "[      0 1402880]\n",
      "[      0 1403136]\n",
      "[      0 1403392]\n",
      "[      0 1403648]\n",
      "[      0 1403904]\n",
      "[      0 1404160]\n",
      "[      0 1404416]\n",
      "[      0 1404672]\n",
      "[      0 1404928]\n",
      "[      0 1405184]\n",
      "[      0 1405440]\n",
      "[      0 1405696]\n",
      "[      0 1405952]\n",
      "[      0 1406208]\n",
      "[      0 1406464]\n",
      "[      0 1406720]\n",
      "[      0 1406976]\n",
      "[      0 1407232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1407488]\n",
      "[      0 1407744]\n",
      "[      0 1408000]\n",
      "[      0 1408256]\n",
      "[      0 1408512]\n",
      "[      0 1408768]\n",
      "[      0 1409024]\n",
      "[      0 1409280]\n",
      "[      0 1409536]\n",
      "[      0 1409792]\n",
      "[      0 1410048]\n",
      "[      0 1410304]\n",
      "[      0 1410560]\n",
      "[      0 1410816]\n",
      "[      0 1411072]\n",
      "[      0 1411328]\n",
      "[      0 1411584]\n",
      "[      0 1411840]\n",
      "[      0 1412096]\n",
      "[      0 1412352]\n",
      "[      0 1412608]\n",
      "[      0 1412864]\n",
      "[      0 1413120]\n",
      "[      0 1413376]\n",
      "[      0 1413632]\n",
      "[      0 1413888]\n",
      "[      0 1414144]\n",
      "[      0 1414400]\n",
      "[      0 1414656]\n",
      "[      0 1414912]\n",
      "[      0 1415168]\n",
      "[      0 1415424]\n",
      "[      0 1415680]\n",
      "[      0 1415936]\n",
      "[      0 1416192]\n",
      "[      0 1416448]\n",
      "[      0 1416704]\n",
      "[      0 1416960]\n",
      "[      0 1417216]\n",
      "[      0 1417472]\n",
      "[      0 1417728]\n",
      "[      0 1417984]\n",
      "[      0 1418240]\n",
      "[      0 1418496]\n",
      "[      0 1418752]\n",
      "[      0 1419008]\n",
      "[      0 1419264]\n",
      "[      0 1419520]\n",
      "[      0 1419776]\n",
      "[      0 1420032]\n",
      "[      0 1420288]\n",
      "[      0 1420544]\n",
      "[      0 1420800]\n",
      "[      0 1421056]\n",
      "[      0 1421312]\n",
      "[      0 1421568]\n",
      "[      0 1421824]\n",
      "[      0 1422080]\n",
      "[      0 1422336]\n",
      "[      0 1422592]\n",
      "[      0 1422848]\n",
      "[      0 1423104]\n",
      "[      0 1423360]\n",
      "[      0 1423616]\n",
      "[      0 1423872]\n",
      "[      0 1424128]\n",
      "[      0 1424384]\n",
      "[      0 1424640]\n",
      "[      0 1424896]\n",
      "[      0 1425152]\n",
      "[      0 1425408]\n",
      "[      0 1425664]\n",
      "[      0 1425920]\n",
      "[      0 1426176]\n",
      "[      0 1426432]\n",
      "[      0 1426688]\n",
      "[      0 1426944]\n",
      "[      0 1427200]\n",
      "[      0 1427456]\n",
      "[      0 1427712]\n",
      "[      0 1427968]\n",
      "[      0 1428224]\n",
      "[      0 1428480]\n",
      "[      0 1428736]\n",
      "[      0 1428992]\n",
      "[      0 1429248]\n",
      "[      0 1429504]\n",
      "[      0 1429760]\n",
      "[      0 1430016]\n",
      "[      0 1430272]\n",
      "[      0 1430528]\n",
      "[      0 1430784]\n",
      "[      0 1431040]\n",
      "[      0 1431296]\n",
      "[      0 1431552]\n",
      "[      0 1431808]\n",
      "[      0 1432064]\n",
      "[      0 1432320]\n",
      "[      0 1432576]\n",
      "[      0 1432832]\n",
      "[      0 1433088]\n",
      "[      0 1433344]\n",
      "[      0 1433600]\n",
      "[      0 1433856]\n",
      "[      0 1434112]\n",
      "[      0 1434368]\n",
      "[      0 1434624]\n",
      "[      0 1434880]\n",
      "[      0 1435136]\n",
      "[      0 1435392]\n",
      "[      0 1435648]\n",
      "[      0 1435904]\n",
      "[      0 1436160]\n",
      "[      0 1436416]\n",
      "[      0 1436672]\n",
      "[      0 1436928]\n",
      "[      0 1437184]\n",
      "[      0 1437440]\n",
      "[      0 1437696]\n",
      "[      0 1437952]\n",
      "[      0 1438208]\n",
      "[      0 1438464]\n",
      "[      0 1438720]\n",
      "[      0 1438976]\n",
      "[      0 1439232]\n",
      "[      0 1439488]\n",
      "[      0 1439744]\n",
      "[      0 1440000]\n",
      "[      0 1440256]\n",
      "[      0 1440512]\n",
      "[      0 1440768]\n",
      "[      0 1441024]\n",
      "[      0 1441280]\n",
      "[      0 1441536]\n",
      "[      0 1441792]\n",
      "[      0 1442048]\n",
      "[      0 1442304]\n",
      "[      0 1442560]\n",
      "[      0 1442816]\n",
      "[      0 1443072]\n",
      "[      0 1443328]\n",
      "[      0 1443584]\n",
      "[      0 1443840]\n",
      "[      0 1444096]\n",
      "[      0 1444352]\n",
      "[      0 1444608]\n",
      "[      0 1444864]\n",
      "[      0 1445120]\n",
      "[      0 1445376]\n",
      "[      0 1445632]\n",
      "[      0 1445888]\n",
      "[      0 1446144]\n",
      "[      0 1446400]\n",
      "[      0 1446656]\n",
      "[      0 1446912]\n",
      "[      0 1447168]\n",
      "[      0 1447424]\n",
      "[      0 1447680]\n",
      "[      0 1447936]\n",
      "[      0 1448192]\n",
      "[      0 1448448]\n",
      "[      0 1448704]\n",
      "[      0 1448960]\n",
      "[      0 1449216]\n",
      "[      0 1449472]\n",
      "[      0 1449728]\n",
      "[      0 1449984]\n",
      "[      0 1450240]\n",
      "[      0 1450496]\n",
      "[      0 1450752]\n",
      "[      0 1451008]\n",
      "[      0 1451264]\n",
      "[      0 1451520]\n",
      "[      0 1451776]\n",
      "[      0 1452032]\n",
      "[      0 1452288]\n",
      "[      0 1452544]\n",
      "[      0 1452800]\n",
      "[      0 1453056]\n",
      "[      0 1453312]\n",
      "[      0 1453568]\n",
      "[      0 1453824]\n",
      "[      0 1454080]\n",
      "[      0 1454336]\n",
      "[      0 1454592]\n",
      "[      0 1454848]\n",
      "[      0 1455104]\n",
      "[      0 1455360]\n",
      "[      0 1455616]\n",
      "[      0 1455872]\n",
      "[      0 1456128]\n",
      "[      0 1456384]\n",
      "[      0 1456640]\n",
      "[      0 1456896]\n",
      "[      0 1457152]\n",
      "[      0 1457408]\n",
      "[      0 1457664]\n",
      "[      0 1457920]\n",
      "[      0 1458176]\n",
      "[      0 1458432]\n",
      "[      0 1458688]\n",
      "[      0 1458944]\n",
      "[      0 1459200]\n",
      "[      0 1459456]\n",
      "[      0 1459712]\n",
      "[      0 1459968]\n",
      "[      0 1460224]\n",
      "[      0 1460480]\n",
      "[      0 1460736]\n",
      "[      0 1460992]\n",
      "[      0 1461248]\n",
      "[      0 1461504]\n",
      "[      0 1461760]\n",
      "[      0 1462016]\n",
      "[      0 1462272]\n",
      "[      0 1462528]\n",
      "[      0 1462784]\n",
      "[      0 1463040]\n",
      "[      0 1463296]\n",
      "[      0 1463552]\n",
      "[      0 1463808]\n",
      "[      0 1464064]\n",
      "[      0 1464320]\n",
      "[      0 1464576]\n",
      "[      0 1464832]\n",
      "[      0 1465088]\n",
      "[      0 1465344]\n",
      "[      0 1465600]\n",
      "[      0 1465856]\n",
      "[      0 1466112]\n",
      "[      0 1466368]\n",
      "[      0 1466624]\n",
      "[      0 1466880]\n",
      "[      0 1467136]\n",
      "[      0 1467392]\n",
      "[      0 1467648]\n",
      "[      0 1467904]\n",
      "[      0 1468160]\n",
      "[      0 1468416]\n",
      "[      0 1468672]\n",
      "[      0 1468928]\n",
      "[      0 1469184]\n",
      "[      0 1469440]\n",
      "[      0 1469696]\n",
      "[      0 1469952]\n",
      "[      0 1470208]\n",
      "[      0 1470464]\n",
      "[      0 1470720]\n",
      "[      0 1470976]\n",
      "[      0 1471232]\n",
      "[      0 1471488]\n",
      "[      0 1471744]\n",
      "[      0 1472000]\n",
      "[      0 1472256]\n",
      "[      0 1472512]\n",
      "[      0 1472768]\n",
      "[      0 1473024]\n",
      "[      0 1473280]\n",
      "[      0 1473536]\n",
      "[      0 1473792]\n",
      "[      0 1474048]\n",
      "[      0 1474304]\n",
      "[      0 1474560]\n",
      "[      0 1474816]\n",
      "[      0 1475072]\n",
      "[      0 1475328]\n",
      "[      0 1475584]\n",
      "[      0 1475840]\n",
      "[      0 1476096]\n",
      "[      0 1476352]\n",
      "[      0 1476608]\n",
      "[      0 1476864]\n",
      "[      0 1477120]\n",
      "[      0 1477376]\n",
      "[      0 1477632]\n",
      "[      0 1477888]\n",
      "[      0 1478144]\n",
      "[      0 1478400]\n",
      "[      0 1478656]\n",
      "[      0 1478912]\n",
      "[      0 1479168]\n",
      "[      0 1479424]\n",
      "[      0 1479680]\n",
      "[      0 1479936]\n",
      "[      0 1480192]\n",
      "[      0 1480448]\n",
      "[      0 1480704]\n",
      "[      0 1480960]\n",
      "[      0 1481216]\n",
      "[      0 1481472]\n",
      "[      0 1481728]\n",
      "[      0 1481984]\n",
      "[      0 1482240]\n",
      "[      0 1482496]\n",
      "[      0 1482752]\n",
      "[      0 1483008]\n",
      "[      0 1483264]\n",
      "[      0 1483520]\n",
      "[      0 1483776]\n",
      "[      0 1484032]\n",
      "[      0 1484288]\n",
      "[      0 1484544]\n",
      "[      0 1484800]\n",
      "[      0 1485056]\n",
      "[      0 1485312]\n",
      "[      0 1485568]\n",
      "[      0 1485824]\n",
      "[      0 1486080]\n",
      "[      0 1486336]\n",
      "[      0 1486592]\n",
      "[      0 1486848]\n",
      "[      0 1487104]\n",
      "[      0 1487360]\n",
      "[      0 1487616]\n",
      "[      0 1487872]\n",
      "[      0 1488128]\n",
      "[      0 1488384]\n",
      "[      0 1488640]\n",
      "[      0 1488896]\n",
      "[      0 1489152]\n",
      "[      0 1489408]\n",
      "[      0 1489664]\n",
      "[      0 1489920]\n",
      "[      0 1490176]\n",
      "[      0 1490432]\n",
      "[      0 1490688]\n",
      "[      0 1490944]\n",
      "[      0 1491200]\n",
      "[      0 1491456]\n",
      "[      0 1491712]\n",
      "[      0 1491968]\n",
      "[      0 1492224]\n",
      "[      0 1492480]\n",
      "[      0 1492736]\n",
      "[      0 1492992]\n",
      "[      0 1493248]\n",
      "[      0 1493504]\n",
      "[      0 1493760]\n",
      "[      0 1494016]\n",
      "[      0 1494272]\n",
      "[      0 1494528]\n",
      "[      0 1494784]\n",
      "[      0 1495040]\n",
      "[      0 1495296]\n",
      "[      0 1495552]\n",
      "[      0 1495808]\n",
      "[      0 1496064]\n",
      "[      0 1496320]\n",
      "[      0 1496576]\n",
      "[      0 1496832]\n",
      "[      0 1497088]\n",
      "[      0 1497344]\n",
      "[      0 1497600]\n",
      "[      0 1497856]\n",
      "[      0 1498112]\n",
      "[      0 1498368]\n",
      "[      0 1498624]\n",
      "[      0 1498880]\n",
      "[      0 1499136]\n",
      "[      0 1499392]\n",
      "[      0 1499648]\n",
      "[      0 1499904]\n",
      "[      0 1500160]\n",
      "[      0 1500416]\n",
      "[      0 1500672]\n",
      "[      0 1500928]\n",
      "[      0 1501184]\n",
      "[      0 1501440]\n",
      "[      0 1501696]\n",
      "[      0 1501952]\n",
      "[      0 1502208]\n",
      "[      0 1502464]\n",
      "[      0 1502720]\n",
      "[      0 1502976]\n",
      "[      0 1503232]\n",
      "[      0 1503488]\n",
      "[      0 1503744]\n",
      "[      0 1504000]\n",
      "[      0 1504256]\n",
      "[      0 1504512]\n",
      "[      0 1504768]\n",
      "[      0 1505024]\n",
      "[      0 1505280]\n",
      "[      0 1505536]\n",
      "[      0 1505792]\n",
      "[      0 1506048]\n",
      "[      0 1506304]\n",
      "[      0 1506560]\n",
      "[      0 1506816]\n",
      "[      0 1507072]\n",
      "[      0 1507328]\n",
      "[      0 1507584]\n",
      "[      0 1507840]\n",
      "[      0 1508096]\n",
      "[      0 1508352]\n",
      "[      0 1508608]\n",
      "[      0 1508864]\n",
      "[      0 1509120]\n",
      "[      0 1509376]\n",
      "[      0 1509632]\n",
      "[      0 1509888]\n",
      "[      0 1510144]\n",
      "[      0 1510400]\n",
      "[      0 1510656]\n",
      "[      0 1510912]\n",
      "[      0 1511168]\n",
      "[      0 1511424]\n",
      "[      0 1511680]\n",
      "[      0 1511936]\n",
      "[      0 1512192]\n",
      "[      0 1512448]\n",
      "[      0 1512704]\n",
      "[      0 1512960]\n",
      "[      0 1513216]\n",
      "[      0 1513472]\n",
      "[      0 1513728]\n",
      "[      0 1513984]\n",
      "[      0 1514240]\n",
      "[      0 1514496]\n",
      "[      0 1514752]\n",
      "[      0 1515008]\n",
      "[      0 1515264]\n",
      "[      0 1515520]\n",
      "[      0 1515776]\n",
      "[      0 1516032]\n",
      "[      0 1516288]\n",
      "[      0 1516544]\n",
      "[      0 1516800]\n",
      "[      0 1517056]\n",
      "[      0 1517312]\n",
      "[      0 1517568]\n",
      "[      0 1517824]\n",
      "[      0 1518080]\n",
      "[      0 1518336]\n",
      "[      0 1518592]\n",
      "[      0 1518848]\n",
      "[      0 1519104]\n",
      "[      0 1519360]\n",
      "[      0 1519616]\n",
      "[      0 1519872]\n",
      "[      0 1520128]\n",
      "[      0 1520384]\n",
      "[      0 1520640]\n",
      "[      0 1520896]\n",
      "[      0 1521152]\n",
      "[      0 1521408]\n",
      "[      0 1521664]\n",
      "[      0 1521920]\n",
      "[      0 1522176]\n",
      "[      0 1522432]\n",
      "[      0 1522688]\n",
      "[      0 1522944]\n",
      "[      0 1523200]\n",
      "[      0 1523456]\n",
      "[      0 1523712]\n",
      "[      0 1523968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1524224]\n",
      "[      0 1524480]\n",
      "[      0 1524736]\n",
      "[      0 1524992]\n",
      "[      0 1525248]\n",
      "[      0 1525504]\n",
      "[      0 1525760]\n",
      "[      0 1526016]\n",
      "[      0 1526272]\n",
      "[      0 1526528]\n",
      "[      0 1526784]\n",
      "[      0 1527040]\n",
      "[      0 1527296]\n",
      "[      0 1527552]\n",
      "[      0 1527808]\n",
      "[      0 1528064]\n",
      "[      0 1528320]\n",
      "[      0 1528576]\n",
      "[      0 1528832]\n",
      "[      0 1529088]\n",
      "[      0 1529344]\n",
      "[      0 1529600]\n",
      "[      0 1529856]\n",
      "[      0 1530112]\n",
      "[      0 1530368]\n",
      "[      0 1530624]\n",
      "[      0 1530880]\n",
      "[      0 1531136]\n",
      "[      0 1531392]\n",
      "[      0 1531648]\n",
      "[      0 1531904]\n",
      "[      0 1532160]\n",
      "[      0 1532416]\n",
      "[      0 1532672]\n",
      "[      0 1532928]\n",
      "[      0 1533184]\n",
      "[      0 1533440]\n",
      "[      0 1533696]\n",
      "[      0 1533952]\n",
      "[      0 1534208]\n",
      "[      0 1534464]\n",
      "[      0 1534720]\n",
      "[      0 1534976]\n",
      "[      0 1535232]\n",
      "[      0 1535488]\n",
      "[      0 1535744]\n",
      "[      0 1536000]\n",
      "[      0 1536256]\n",
      "[      0 1536512]\n",
      "[      0 1536768]\n",
      "[      0 1537024]\n",
      "[      0 1537280]\n",
      "[      0 1537536]\n",
      "[      0 1537792]\n",
      "[      0 1538048]\n",
      "[      0 1538304]\n",
      "[      0 1538560]\n",
      "[      0 1538816]\n",
      "[      0 1539072]\n",
      "[      0 1539328]\n",
      "[      0 1539584]\n",
      "[      0 1539840]\n",
      "[      0 1540096]\n",
      "[      0 1540352]\n",
      "[      0 1540608]\n",
      "[      0 1540864]\n",
      "[      0 1541120]\n",
      "[      0 1541376]\n",
      "[      0 1541632]\n",
      "[      0 1541888]\n",
      "[      0 1542144]\n",
      "[      0 1542400]\n",
      "[      0 1542656]\n",
      "[      0 1542912]\n",
      "[      0 1543168]\n",
      "[      0 1543424]\n",
      "[      0 1543680]\n",
      "[      0 1543936]\n",
      "[      0 1544192]\n",
      "[      0 1544448]\n",
      "[      0 1544704]\n",
      "[      0 1544960]\n",
      "[      0 1545216]\n",
      "[      0 1545472]\n",
      "[      0 1545728]\n",
      "[      0 1545984]\n",
      "[      0 1546240]\n",
      "[      0 1546496]\n",
      "[      0 1546752]\n",
      "[      0 1547008]\n",
      "[      0 1547264]\n",
      "[      0 1547520]\n",
      "[      0 1547776]\n",
      "[      0 1548032]\n",
      "[      0 1548288]\n",
      "[      0 1548544]\n",
      "[      0 1548800]\n",
      "[      0 1549056]\n",
      "[      0 1549312]\n",
      "[      0 1549568]\n",
      "[      0 1549824]\n",
      "[      0 1550080]\n",
      "[      0 1550336]\n",
      "[      0 1550592]\n",
      "[      0 1550848]\n",
      "[      0 1551104]\n",
      "[      0 1551360]\n",
      "[      0 1551616]\n",
      "[      0 1551872]\n",
      "[      0 1552128]\n",
      "[      0 1552384]\n",
      "[      0 1552640]\n",
      "[      0 1552896]\n",
      "[      0 1553152]\n",
      "[      0 1553408]\n",
      "[      0 1553664]\n",
      "[      0 1553920]\n",
      "[      0 1554176]\n",
      "[      0 1554432]\n",
      "[      0 1554688]\n",
      "[      0 1554944]\n",
      "[      0 1555200]\n",
      "[      0 1555456]\n",
      "[      0 1555712]\n",
      "[      0 1555968]\n",
      "[      0 1556224]\n",
      "[      0 1556480]\n",
      "[      0 1556736]\n",
      "[      0 1556992]\n",
      "[      0 1557248]\n",
      "[      0 1557504]\n",
      "[      0 1557760]\n",
      "[      0 1558016]\n",
      "[      0 1558272]\n",
      "[      0 1558528]\n",
      "[      0 1558784]\n",
      "[      0 1559040]\n",
      "[      0 1559296]\n",
      "[      0 1559552]\n",
      "[      0 1559808]\n",
      "[      0 1560064]\n",
      "[      0 1560320]\n",
      "[      0 1560576]\n",
      "[      0 1560832]\n",
      "[      0 1561088]\n",
      "[      0 1561344]\n",
      "[      0 1561600]\n",
      "[      0 1561856]\n",
      "[      0 1562112]\n",
      "[      0 1562368]\n",
      "[      0 1562624]\n",
      "[      0 1562880]\n",
      "[      0 1563136]\n",
      "[      0 1563392]\n",
      "[      0 1563648]\n",
      "[      0 1563904]\n",
      "[      0 1564160]\n",
      "[      0 1564416]\n",
      "[      0 1564672]\n",
      "[      0 1564928]\n",
      "[      0 1565184]\n",
      "[      0 1565440]\n",
      "[      0 1565696]\n",
      "[      0 1565952]\n",
      "[      0 1566208]\n",
      "[      0 1566464]\n",
      "[      0 1566720]\n",
      "[      0 1566976]\n",
      "[      0 1567232]\n",
      "[      0 1567488]\n",
      "[      0 1567744]\n",
      "[      0 1568000]\n",
      "[      0 1568256]\n",
      "[      0 1568512]\n",
      "[      0 1568768]\n",
      "[      0 1569024]\n",
      "[      0 1569280]\n",
      "[      0 1569536]\n",
      "[      0 1569792]\n",
      "[      0 1570048]\n",
      "[      0 1570304]\n",
      "[      0 1570560]\n",
      "[      0 1570816]\n",
      "[      0 1571072]\n",
      "[      0 1571328]\n",
      "[      0 1571584]\n",
      "[      0 1571840]\n",
      "[      0 1572096]\n",
      "[      0 1572352]\n",
      "[      0 1572608]\n",
      "[      0 1572864]\n",
      "[      0 1573120]\n",
      "[      0 1573376]\n",
      "[      0 1573632]\n",
      "[      0 1573888]\n",
      "[      0 1574144]\n",
      "[      0 1574400]\n",
      "[      0 1574656]\n",
      "[      0 1574912]\n",
      "[      0 1575168]\n",
      "[      0 1575424]\n",
      "[      0 1575680]\n",
      "[      0 1575936]\n",
      "[      0 1576192]\n",
      "[      0 1576448]\n",
      "[      0 1576704]\n",
      "[      0 1576960]\n",
      "[      0 1577216]\n",
      "[      0 1577472]\n",
      "[      0 1577728]\n",
      "[      0 1577984]\n",
      "[      0 1578240]\n",
      "[      0 1578496]\n",
      "[      0 1578752]\n",
      "[      0 1579008]\n",
      "[      0 1579264]\n",
      "[      0 1579520]\n",
      "[      0 1579776]\n",
      "[      0 1580032]\n",
      "[      0 1580288]\n",
      "[      0 1580544]\n",
      "[      0 1580800]\n",
      "[      0 1581056]\n",
      "[      0 1581312]\n",
      "[      0 1581568]\n",
      "[      0 1581824]\n",
      "[      0 1582080]\n",
      "[      0 1582336]\n",
      "[      0 1582592]\n",
      "[      0 1582848]\n",
      "[      0 1583104]\n",
      "[      0 1583360]\n",
      "[      0 1583616]\n",
      "[      0 1583872]\n",
      "[      0 1584128]\n",
      "[      0 1584384]\n",
      "[      0 1584640]\n",
      "[      0 1584896]\n",
      "[      0 1585152]\n",
      "[      0 1585408]\n",
      "[      0 1585664]\n",
      "[      0 1585920]\n",
      "[      0 1586176]\n",
      "[      0 1586432]\n",
      "[      0 1586688]\n",
      "[      0 1586944]\n",
      "[      0 1587200]\n",
      "[      0 1587456]\n",
      "[      0 1587712]\n",
      "[      0 1587968]\n",
      "[      0 1588224]\n",
      "[      0 1588480]\n",
      "[      0 1588736]\n",
      "[      0 1588992]\n",
      "[      0 1589248]\n",
      "[      0 1589504]\n",
      "[      0 1589760]\n",
      "[      0 1590016]\n",
      "[      0 1590272]\n",
      "[      0 1590528]\n",
      "[      0 1590784]\n",
      "[      0 1591040]\n",
      "[      0 1591296]\n",
      "[      0 1591552]\n",
      "[      0 1591808]\n",
      "[      0 1592064]\n",
      "[      0 1592320]\n",
      "[      0 1592576]\n",
      "[      0 1592832]\n",
      "[      0 1593088]\n",
      "[      0 1593344]\n",
      "[      0 1593600]\n",
      "[      0 1593856]\n",
      "[      0 1594112]\n",
      "[      0 1594368]\n",
      "[      0 1594624]\n",
      "[      0 1594880]\n",
      "[      0 1595136]\n",
      "[      0 1595392]\n",
      "[      0 1595648]\n",
      "[      0 1595904]\n",
      "[      0 1596160]\n",
      "[      0 1596416]\n",
      "[      0 1596672]\n",
      "[      0 1596928]\n",
      "[      0 1597184]\n",
      "[      0 1597440]\n",
      "[      0 1597696]\n",
      "[      0 1597952]\n",
      "[      0 1598208]\n",
      "[      0 1598464]\n",
      "[      0 1598720]\n",
      "[      0 1598976]\n",
      "[      0 1599232]\n",
      "[      0 1599488]\n",
      "[      0 1599744]\n",
      "[      0 1600000]\n",
      "[      0 1600256]\n",
      "[      0 1600512]\n",
      "[      0 1600768]\n",
      "[      0 1601024]\n",
      "[      0 1601280]\n",
      "[      0 1601536]\n",
      "[      0 1601792]\n",
      "[      0 1602048]\n",
      "[      0 1602304]\n",
      "[      0 1602560]\n",
      "[      0 1602816]\n",
      "[      0 1603072]\n",
      "[      0 1603328]\n",
      "[      0 1603584]\n",
      "[      0 1603840]\n",
      "[      0 1604096]\n",
      "[      0 1604352]\n",
      "[      0 1604608]\n",
      "[      0 1604864]\n",
      "[      0 1605120]\n",
      "[      0 1605376]\n",
      "[      0 1605632]\n",
      "[      0 1605888]\n",
      "[      0 1606144]\n",
      "[      0 1606400]\n",
      "[      0 1606656]\n",
      "[      0 1606912]\n",
      "[      0 1607168]\n",
      "[      0 1607424]\n",
      "[      0 1607680]\n",
      "[      0 1607936]\n",
      "[      0 1608192]\n",
      "[      0 1608448]\n",
      "[      0 1608704]\n",
      "[      0 1608960]\n",
      "[      0 1609216]\n",
      "[      0 1609472]\n",
      "[      0 1609728]\n",
      "[      0 1609984]\n",
      "[      0 1610240]\n",
      "[      0 1610496]\n",
      "[      0 1610752]\n",
      "[      0 1611008]\n",
      "[      0 1611264]\n",
      "[      0 1611520]\n",
      "[      0 1611776]\n",
      "[      0 1612032]\n",
      "[      0 1612288]\n",
      "[      0 1612544]\n",
      "[      0 1612800]\n",
      "[      0 1613056]\n",
      "[      0 1613312]\n",
      "[      0 1613568]\n",
      "[      0 1613824]\n",
      "[      0 1614080]\n",
      "[      0 1614336]\n",
      "[      0 1614592]\n",
      "[      0 1614848]\n",
      "[      0 1615104]\n",
      "[      0 1615360]\n",
      "[      0 1615616]\n",
      "[      0 1615872]\n",
      "[      0 1616128]\n",
      "[      0 1616384]\n",
      "[      0 1616640]\n",
      "[      0 1616896]\n",
      "[      0 1617152]\n",
      "[      0 1617408]\n",
      "[      0 1617664]\n",
      "[      0 1617920]\n",
      "[      0 1618176]\n",
      "[      0 1618432]\n",
      "[      0 1618688]\n",
      "[      0 1618944]\n",
      "[      0 1619200]\n",
      "[      0 1619456]\n",
      "[      0 1619712]\n",
      "[      0 1619968]\n",
      "[      0 1620224]\n",
      "[      0 1620480]\n",
      "[      0 1620736]\n",
      "[      0 1620992]\n",
      "[      0 1621248]\n",
      "[      0 1621504]\n",
      "[      0 1621760]\n",
      "[      0 1622016]\n",
      "[      0 1622272]\n",
      "[      0 1622528]\n",
      "[      0 1622784]\n",
      "[      0 1623040]\n",
      "[      0 1623296]\n",
      "[      0 1623552]\n",
      "[      0 1623808]\n",
      "[      0 1624064]\n",
      "[      0 1624320]\n",
      "[      0 1624576]\n",
      "[      0 1624832]\n",
      "[      0 1625088]\n",
      "[      0 1625344]\n",
      "[      0 1625600]\n",
      "[      0 1625856]\n",
      "[      0 1626112]\n",
      "[      0 1626368]\n",
      "[      0 1626624]\n",
      "[      0 1626880]\n",
      "[      0 1627136]\n",
      "[      0 1627392]\n",
      "[      0 1627648]\n",
      "[      0 1627904]\n",
      "[      0 1628160]\n",
      "[      0 1628416]\n",
      "[      0 1628672]\n",
      "[      0 1628928]\n",
      "[      0 1629184]\n",
      "[      0 1629440]\n",
      "[      0 1629696]\n",
      "[      0 1629952]\n",
      "[      0 1630208]\n",
      "[      0 1630464]\n",
      "[      0 1630720]\n",
      "[      0 1630976]\n",
      "[      0 1631232]\n",
      "[      0 1631488]\n",
      "[      0 1631744]\n",
      "[      0 1632000]\n",
      "[      0 1632256]\n",
      "[      0 1632512]\n",
      "[      0 1632768]\n",
      "[      0 1633024]\n",
      "[      0 1633280]\n",
      "[      0 1633536]\n",
      "[      0 1633792]\n",
      "[      0 1634048]\n",
      "[      0 1634304]\n",
      "[      0 1634560]\n",
      "[      0 1634816]\n",
      "[      0 1635072]\n",
      "[      0 1635328]\n",
      "[      0 1635584]\n",
      "[      0 1635840]\n",
      "[      0 1636096]\n",
      "[      0 1636352]\n",
      "[      0 1636608]\n",
      "[      0 1636864]\n",
      "[      0 1637120]\n",
      "[      0 1637376]\n",
      "[      0 1637632]\n",
      "[      0 1637888]\n",
      "[      0 1638144]\n",
      "[      0 1638400]\n",
      "[      0 1638656]\n",
      "[      0 1638912]\n",
      "[      0 1639168]\n",
      "[      0 1639424]\n",
      "[      0 1639680]\n",
      "[      0 1639936]\n",
      "[      0 1640192]\n",
      "[      0 1640448]\n",
      "[      0 1640704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1640960]\n",
      "[      0 1641216]\n",
      "[      0 1641472]\n",
      "[      0 1641728]\n",
      "[      0 1641984]\n",
      "[      0 1642240]\n",
      "[      0 1642496]\n",
      "[      0 1642752]\n",
      "[      0 1643008]\n",
      "[      0 1643264]\n",
      "[      0 1643520]\n",
      "[      0 1643776]\n",
      "[      0 1644032]\n",
      "[      0 1644288]\n",
      "[      0 1644544]\n",
      "[      0 1644800]\n",
      "[      0 1645056]\n",
      "[      0 1645312]\n",
      "[      0 1645568]\n",
      "[      0 1645824]\n",
      "[      0 1646080]\n",
      "[      0 1646336]\n",
      "[      0 1646592]\n",
      "[      0 1646848]\n",
      "[      0 1647104]\n",
      "[      0 1647360]\n",
      "[      0 1647616]\n",
      "[      0 1647872]\n",
      "[      0 1648128]\n",
      "[      0 1648384]\n",
      "[      0 1648640]\n",
      "[      0 1648896]\n",
      "[      0 1649152]\n",
      "[      0 1649408]\n",
      "[      0 1649664]\n",
      "[      0 1649920]\n",
      "[      0 1650176]\n",
      "[      0 1650432]\n",
      "[      0 1650688]\n",
      "[      0 1650944]\n",
      "[      0 1651200]\n",
      "[      0 1651456]\n",
      "[      0 1651712]\n",
      "[      0 1651968]\n",
      "[      0 1652224]\n",
      "[      0 1652480]\n",
      "[      0 1652736]\n",
      "[      0 1652992]\n",
      "[      0 1653248]\n",
      "[      0 1653504]\n",
      "[      0 1653760]\n",
      "[      0 1654016]\n",
      "[      0 1654272]\n",
      "[      0 1654528]\n",
      "[      0 1654784]\n",
      "[      0 1655040]\n",
      "[      0 1655296]\n",
      "[      0 1655552]\n",
      "[      0 1655808]\n",
      "[      0 1656064]\n",
      "[      0 1656320]\n",
      "[      0 1656576]\n",
      "[      0 1656832]\n",
      "[      0 1657088]\n",
      "[      0 1657344]\n",
      "[      0 1657600]\n",
      "[      0 1657856]\n",
      "[      0 1658112]\n",
      "[      0 1658368]\n",
      "[      0 1658624]\n",
      "[      0 1658880]\n",
      "[      0 1659136]\n",
      "[      0 1659392]\n",
      "[      0 1659648]\n",
      "[      0 1659904]\n",
      "[      0 1660160]\n",
      "[      0 1660416]\n",
      "[      0 1660672]\n",
      "[      0 1660928]\n",
      "[      0 1661184]\n",
      "[      0 1661440]\n",
      "[      0 1661696]\n",
      "[      0 1661952]\n",
      "[      0 1662208]\n",
      "[      0 1662464]\n",
      "[      0 1662720]\n",
      "[      0 1662976]\n",
      "[      0 1663232]\n",
      "[      0 1663488]\n",
      "[      0 1663744]\n",
      "[      0 1664000]\n",
      "[      0 1664256]\n",
      "[      0 1664512]\n",
      "[      0 1664768]\n",
      "[      0 1665024]\n",
      "[      0 1665280]\n",
      "[      0 1665536]\n",
      "[      0 1665792]\n",
      "[      0 1666048]\n",
      "[      0 1666304]\n",
      "[      0 1666560]\n",
      "[      0 1666816]\n",
      "[      0 1667072]\n",
      "[      0 1667328]\n",
      "[      0 1667584]\n",
      "[      0 1667840]\n",
      "[      0 1668096]\n",
      "[      0 1668352]\n",
      "[      0 1668608]\n",
      "[      0 1668864]\n",
      "[      0 1669120]\n",
      "[      0 1669376]\n",
      "[      0 1669632]\n",
      "[      0 1669888]\n",
      "[      0 1670144]\n",
      "[      0 1670400]\n",
      "[      0 1670656]\n",
      "[      0 1670912]\n",
      "[      0 1671168]\n",
      "[      0 1671424]\n",
      "[      0 1671680]\n",
      "[      0 1671936]\n",
      "[      0 1672192]\n",
      "[      0 1672448]\n",
      "[      0 1672704]\n",
      "[      0 1672960]\n",
      "[      0 1673216]\n",
      "[      0 1673472]\n",
      "[      0 1673728]\n",
      "[      0 1673984]\n",
      "[      0 1674240]\n",
      "[      0 1674496]\n",
      "[      0 1674752]\n",
      "[      0 1675008]\n",
      "[      0 1675264]\n",
      "[      0 1675520]\n",
      "[      0 1675776]\n",
      "[      0 1676032]\n",
      "[      0 1676288]\n",
      "[      0 1676544]\n",
      "[      0 1676800]\n",
      "[      0 1677056]\n",
      "[      0 1677312]\n",
      "[      0 1677568]\n",
      "[      0 1677824]\n",
      "[      0 1678080]\n",
      "[      0 1678336]\n",
      "[      0 1678592]\n",
      "[      0 1678848]\n",
      "[      0 1679104]\n",
      "[      0 1679360]\n",
      "[      0 1679616]\n",
      "[      0 1679872]\n",
      "[      0 1680128]\n",
      "[      0 1680384]\n",
      "[      0 1680640]\n",
      "[      0 1680896]\n",
      "[      0 1681152]\n",
      "[      0 1681408]\n",
      "[      0 1681664]\n",
      "[      0 1681920]\n",
      "[      0 1682176]\n",
      "[      0 1682432]\n",
      "[      0 1682688]\n",
      "[      0 1682944]\n",
      "[      0 1683200]\n",
      "[      0 1683456]\n",
      "[      0 1683712]\n",
      "[      0 1683968]\n",
      "[      0 1684224]\n",
      "[      0 1684480]\n",
      "[      0 1684736]\n",
      "[      0 1684992]\n",
      "[      0 1685248]\n",
      "[      0 1685504]\n",
      "[      0 1685760]\n",
      "[      0 1686016]\n",
      "[      0 1686272]\n",
      "[      0 1686528]\n",
      "[      0 1686784]\n",
      "[      0 1687040]\n",
      "[      0 1687296]\n",
      "[      0 1687552]\n",
      "[      0 1687808]\n",
      "[      0 1688064]\n",
      "[      0 1688320]\n",
      "[      0 1688576]\n",
      "[      0 1688832]\n",
      "[      0 1689088]\n",
      "[      0 1689344]\n",
      "[      0 1689600]\n",
      "[      0 1689856]\n",
      "[      0 1690112]\n",
      "[      0 1690368]\n",
      "[      0 1690624]\n",
      "[      0 1690880]\n",
      "[      0 1691136]\n",
      "[      0 1691392]\n",
      "[      0 1691648]\n",
      "[      0 1691904]\n",
      "[      0 1692160]\n",
      "[      0 1692416]\n",
      "[      0 1692672]\n",
      "[      0 1692928]\n",
      "[      0 1693184]\n",
      "[      0 1693440]\n",
      "[      0 1693696]\n",
      "[      0 1693952]\n",
      "[      0 1694208]\n",
      "[      0 1694464]\n",
      "[      0 1694720]\n",
      "[      0 1694976]\n",
      "[      0 1695232]\n",
      "[      0 1695488]\n",
      "[      0 1695744]\n",
      "[      0 1696000]\n",
      "[      0 1696256]\n",
      "[      0 1696512]\n",
      "[      0 1696768]\n",
      "[      0 1697024]\n",
      "[      0 1697280]\n",
      "[      0 1697536]\n",
      "[      0 1697792]\n",
      "[      0 1698048]\n",
      "[      0 1698304]\n",
      "[      0 1698560]\n",
      "[      0 1698816]\n",
      "[      0 1699072]\n",
      "[      0 1699328]\n",
      "[      0 1699584]\n",
      "[      0 1699840]\n",
      "[      0 1700096]\n",
      "[      0 1700352]\n",
      "[      0 1700608]\n",
      "[      0 1700864]\n",
      "[      0 1701120]\n",
      "[      0 1701376]\n",
      "[      0 1701632]\n",
      "[      0 1701888]\n",
      "[      0 1702144]\n",
      "[      0 1702400]\n",
      "[      0 1702656]\n",
      "[      0 1702912]\n",
      "[      0 1703168]\n",
      "[      0 1703424]\n",
      "[      0 1703680]\n",
      "[      0 1703936]\n",
      "[      0 1704192]\n",
      "[      0 1704448]\n",
      "[      0 1704704]\n",
      "[      0 1704960]\n",
      "[      0 1705216]\n",
      "[      0 1705472]\n",
      "[      0 1705728]\n",
      "[      0 1705984]\n",
      "[      0 1706240]\n",
      "[      0 1706496]\n",
      "[      0 1706752]\n",
      "[      0 1707008]\n",
      "[      0 1707264]\n",
      "[      0 1707520]\n",
      "[      0 1707776]\n",
      "[      0 1708032]\n",
      "[      0 1708288]\n",
      "[      0 1708544]\n",
      "[      0 1708800]\n",
      "[      0 1709056]\n",
      "[      0 1709312]\n",
      "[      0 1709568]\n",
      "[      0 1709824]\n",
      "[      0 1710080]\n",
      "[      0 1710336]\n",
      "[      0 1710592]\n",
      "[      0 1710848]\n",
      "[      0 1711104]\n",
      "[      0 1711360]\n",
      "[      0 1711616]\n",
      "[      0 1711872]\n",
      "[      0 1712128]\n",
      "[      0 1712384]\n",
      "[      0 1712640]\n",
      "[      0 1712896]\n",
      "[      0 1713152]\n",
      "[      0 1713408]\n",
      "[      0 1713664]\n",
      "[      0 1713920]\n",
      "[      0 1714176]\n",
      "[      0 1714432]\n",
      "[      0 1714688]\n",
      "[      0 1714944]\n",
      "[      0 1715200]\n",
      "[      0 1715456]\n",
      "[      0 1715712]\n",
      "[      0 1715968]\n",
      "[      0 1716224]\n",
      "[      0 1716480]\n",
      "[      0 1716736]\n",
      "[      0 1716992]\n",
      "[      0 1717248]\n",
      "[      0 1717504]\n",
      "[      0 1717760]\n",
      "[      0 1718016]\n",
      "[      0 1718272]\n",
      "[      0 1718528]\n",
      "[      0 1718784]\n",
      "[      0 1719040]\n",
      "[      0 1719296]\n",
      "[      0 1719552]\n",
      "[      0 1719808]\n",
      "[      0 1720064]\n",
      "[      0 1720320]\n",
      "[      0 1720576]\n",
      "[      0 1720832]\n",
      "[      0 1721088]\n",
      "[      0 1721344]\n",
      "[      0 1721600]\n",
      "[      0 1721856]\n",
      "[      0 1722112]\n",
      "[      0 1722368]\n",
      "[      0 1722624]\n",
      "[      0 1722880]\n",
      "[      0 1723136]\n",
      "[      0 1723392]\n",
      "[      0 1723648]\n",
      "[      0 1723904]\n",
      "[      0 1724160]\n",
      "[      0 1724416]\n",
      "[      0 1724672]\n",
      "[      0 1724928]\n",
      "[      0 1725184]\n",
      "[      0 1725440]\n",
      "[      0 1725696]\n",
      "[      0 1725952]\n",
      "[      0 1726208]\n",
      "[      0 1726464]\n",
      "[      0 1726720]\n",
      "[      0 1726976]\n",
      "[      0 1727232]\n",
      "[      0 1727488]\n",
      "[      0 1727744]\n",
      "[      0 1728000]\n",
      "[      0 1728256]\n",
      "[      0 1728512]\n",
      "[      0 1728768]\n",
      "[      0 1729024]\n",
      "[      0 1729280]\n",
      "[      0 1729536]\n",
      "[      0 1729792]\n",
      "[      0 1730048]\n",
      "[      0 1730304]\n",
      "[      0 1730560]\n",
      "[      0 1730816]\n",
      "[      0 1731072]\n",
      "[      0 1731328]\n",
      "[      0 1731584]\n",
      "[      0 1731840]\n",
      "[      0 1732096]\n",
      "[      0 1732352]\n",
      "[      0 1732608]\n",
      "[      0 1732864]\n",
      "[      0 1733120]\n",
      "[      0 1733376]\n",
      "[      0 1733632]\n",
      "[      0 1733888]\n",
      "[      0 1734144]\n",
      "[      0 1734400]\n",
      "[      0 1734656]\n",
      "[      0 1734912]\n",
      "[      0 1735168]\n",
      "[      0 1735424]\n",
      "[      0 1735680]\n",
      "[      0 1735936]\n",
      "[      0 1736192]\n",
      "[      0 1736448]\n",
      "[      0 1736704]\n",
      "[      0 1736960]\n",
      "[      0 1737216]\n",
      "[      0 1737472]\n",
      "[      0 1737728]\n",
      "[      0 1737984]\n",
      "[      0 1738240]\n",
      "[      0 1738496]\n",
      "[      0 1738752]\n",
      "[      0 1739008]\n",
      "[      0 1739264]\n",
      "[      0 1739520]\n",
      "[      0 1739776]\n",
      "[      0 1740032]\n",
      "[      0 1740288]\n",
      "[      0 1740544]\n",
      "[      0 1740800]\n",
      "[      0 1741056]\n",
      "[      0 1741312]\n",
      "[      0 1741568]\n",
      "[      0 1741824]\n",
      "[      0 1742080]\n",
      "[      0 1742336]\n",
      "[      0 1742592]\n",
      "[      0 1742848]\n",
      "[      0 1743104]\n",
      "[      0 1743360]\n",
      "[      0 1743616]\n",
      "[      0 1743872]\n",
      "[      0 1744128]\n",
      "[      0 1744384]\n",
      "[      0 1744640]\n",
      "[      0 1744896]\n",
      "[      0 1745152]\n",
      "[      0 1745408]\n",
      "[      0 1745664]\n",
      "[      0 1745920]\n",
      "[      0 1746176]\n",
      "[      0 1746432]\n",
      "[      0 1746688]\n",
      "[      0 1746944]\n",
      "[      0 1747200]\n",
      "[      0 1747456]\n",
      "[      0 1747712]\n",
      "[      0 1747968]\n",
      "[      0 1748224]\n",
      "[      0 1748480]\n",
      "[      0 1748736]\n",
      "[      0 1748992]\n",
      "[      0 1749248]\n",
      "[      0 1749504]\n",
      "[      0 1749760]\n",
      "[      0 1750016]\n",
      "[      0 1750272]\n",
      "[      0 1750528]\n",
      "[      0 1750784]\n",
      "[      0 1751040]\n",
      "[      0 1751296]\n",
      "[      0 1751552]\n",
      "[      0 1751808]\n",
      "[      0 1752064]\n",
      "[      0 1752320]\n",
      "[      0 1752576]\n",
      "[      0 1752832]\n",
      "[      0 1753088]\n",
      "[      0 1753344]\n",
      "[      0 1753600]\n",
      "[      0 1753856]\n",
      "[      0 1754112]\n",
      "[      0 1754368]\n",
      "[      0 1754624]\n",
      "[      0 1754880]\n",
      "[      0 1755136]\n",
      "[      0 1755392]\n",
      "[      0 1755648]\n",
      "[      0 1755904]\n",
      "[      0 1756160]\n",
      "[      0 1756416]\n",
      "[      0 1756672]\n",
      "[      0 1756928]\n",
      "[      0 1757184]\n",
      "[      0 1757440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1757696]\n",
      "[      0 1757952]\n",
      "[      0 1758208]\n",
      "[      0 1758464]\n",
      "[      0 1758720]\n",
      "[      0 1758976]\n",
      "[      0 1759232]\n",
      "[      0 1759488]\n",
      "[      0 1759744]\n",
      "[      0 1760000]\n",
      "[      0 1760256]\n",
      "[      0 1760512]\n",
      "[      0 1760768]\n",
      "[      0 1761024]\n",
      "[      0 1761280]\n",
      "[      0 1761536]\n",
      "[      0 1761792]\n",
      "[      0 1762048]\n",
      "[      0 1762304]\n",
      "[      0 1762560]\n",
      "[      0 1762816]\n",
      "[      0 1763072]\n",
      "[      0 1763328]\n",
      "[      0 1763584]\n",
      "[      0 1763840]\n",
      "[      0 1764096]\n",
      "[      0 1764352]\n",
      "[      0 1764608]\n",
      "[      0 1764864]\n",
      "[      0 1765120]\n",
      "[      0 1765376]\n",
      "[      0 1765632]\n",
      "[      0 1765888]\n",
      "[      0 1766144]\n",
      "[      0 1766400]\n",
      "[      0 1766656]\n",
      "[      0 1766912]\n",
      "[      0 1767168]\n",
      "[      0 1767424]\n",
      "[      0 1767680]\n",
      "[      0 1767936]\n",
      "[      0 1768192]\n",
      "[      0 1768448]\n",
      "[      0 1768704]\n",
      "[      0 1768960]\n",
      "[      0 1769216]\n",
      "[      0 1769472]\n",
      "[      0 1769728]\n",
      "[      0 1769984]\n",
      "[      0 1770240]\n",
      "[      0 1770496]\n",
      "[      0 1770752]\n",
      "[      0 1771008]\n",
      "[      0 1771264]\n",
      "[      0 1771520]\n",
      "[      0 1771776]\n",
      "[      0 1772032]\n",
      "[      0 1772288]\n",
      "[      0 1772544]\n",
      "[      0 1772800]\n",
      "[      0 1773056]\n",
      "[      0 1773312]\n",
      "[      0 1773568]\n",
      "[      0 1773824]\n",
      "[      0 1774080]\n",
      "[      0 1774336]\n",
      "[      0 1774592]\n",
      "[      0 1774848]\n",
      "[      0 1775104]\n",
      "[      0 1775360]\n",
      "[      0 1775616]\n",
      "[      0 1775872]\n",
      "[      0 1776128]\n",
      "[      0 1776384]\n",
      "[      0 1776640]\n",
      "[      0 1776896]\n",
      "[      0 1777152]\n",
      "[      0 1777408]\n",
      "[      0 1777664]\n",
      "[      0 1777920]\n",
      "[      0 1778176]\n",
      "[      0 1778432]\n",
      "[      0 1778688]\n",
      "[      0 1778944]\n",
      "[      0 1779200]\n",
      "[      0 1779456]\n",
      "[      0 1779712]\n",
      "[      0 1779968]\n",
      "[      0 1780224]\n",
      "[      0 1780480]\n",
      "[      0 1780736]\n",
      "[      0 1780992]\n",
      "[      0 1781248]\n",
      "[      0 1781504]\n",
      "[      0 1781760]\n",
      "[      0 1782016]\n",
      "[      0 1782272]\n",
      "[      0 1782528]\n",
      "[      0 1782784]\n",
      "[      0 1783040]\n",
      "[      0 1783296]\n",
      "[      0 1783552]\n",
      "[      0 1783808]\n",
      "[      0 1784064]\n",
      "[      0 1784320]\n",
      "[      0 1784576]\n",
      "[      0 1784832]\n",
      "[      0 1785088]\n",
      "[      0 1785344]\n",
      "[      0 1785600]\n",
      "[      0 1785856]\n",
      "[      0 1786112]\n",
      "[      0 1786368]\n",
      "[      0 1786624]\n",
      "[      0 1786880]\n",
      "[      0 1787136]\n",
      "[      0 1787392]\n",
      "[      0 1787648]\n",
      "[      0 1787904]\n",
      "[      0 1788160]\n",
      "[      0 1788416]\n",
      "[      0 1788672]\n",
      "[      0 1788928]\n",
      "[      0 1789184]\n",
      "[      0 1789440]\n",
      "[      0 1789696]\n",
      "[      0 1789952]\n",
      "[      0 1790208]\n",
      "[      0 1790464]\n",
      "[      0 1790720]\n",
      "[      0 1790976]\n",
      "[      0 1791232]\n",
      "[      0 1791488]\n",
      "[      0 1791744]\n",
      "[      0 1792000]\n",
      "[      0 1792256]\n",
      "[      0 1792512]\n",
      "[      0 1792768]\n",
      "[      0 1793024]\n",
      "[      0 1793280]\n",
      "[      0 1793536]\n",
      "[      0 1793792]\n",
      "[      0 1794048]\n",
      "[      0 1794304]\n",
      "[      0 1794560]\n",
      "[      0 1794816]\n",
      "[      0 1795072]\n",
      "[      0 1795328]\n",
      "[      0 1795584]\n",
      "[      0 1795840]\n",
      "[      0 1796096]\n",
      "[      0 1796352]\n",
      "[      0 1796608]\n",
      "[      0 1796864]\n",
      "[      0 1797120]\n",
      "[      0 1797376]\n",
      "[      0 1797632]\n",
      "[      0 1797888]\n",
      "[      0 1798144]\n",
      "[      0 1798400]\n",
      "[      0 1798656]\n",
      "[      0 1798912]\n",
      "[      0 1799168]\n",
      "[      0 1799424]\n",
      "[      0 1799680]\n",
      "[      0 1799936]\n",
      "[      0 1800192]\n",
      "[      0 1800448]\n",
      "[      0 1800704]\n",
      "[      0 1800960]\n",
      "[      0 1801216]\n",
      "[      0 1801472]\n",
      "[      0 1801728]\n",
      "[      0 1801984]\n",
      "[      0 1802240]\n",
      "[      0 1802496]\n",
      "[      0 1802752]\n",
      "[      0 1803008]\n",
      "[      0 1803264]\n",
      "[      0 1803520]\n",
      "[      0 1803776]\n",
      "[      0 1804032]\n",
      "[      0 1804288]\n",
      "[      0 1804544]\n",
      "[      0 1804800]\n",
      "[      0 1805056]\n",
      "[      0 1805312]\n",
      "[      0 1805568]\n",
      "[      0 1805824]\n",
      "[      0 1806080]\n",
      "[      0 1806336]\n",
      "[      0 1806592]\n",
      "[      0 1806848]\n",
      "[      0 1807104]\n",
      "[      0 1807360]\n",
      "[      0 1807616]\n",
      "[      0 1807872]\n",
      "[      0 1808128]\n",
      "[      0 1808384]\n",
      "[      0 1808640]\n",
      "[      0 1808896]\n",
      "[      0 1809152]\n",
      "[      0 1809408]\n",
      "[      0 1809664]\n",
      "[      0 1809920]\n",
      "[      0 1810176]\n",
      "[      0 1810432]\n",
      "[      0 1810688]\n",
      "[      0 1810944]\n",
      "[      0 1811200]\n",
      "[      0 1811456]\n",
      "[      0 1811712]\n",
      "[      0 1811968]\n",
      "[      0 1812224]\n",
      "[      0 1812480]\n",
      "[      0 1812736]\n",
      "[      0 1812992]\n",
      "[      0 1813248]\n",
      "[      0 1813504]\n",
      "[      0 1813760]\n",
      "[      0 1814016]\n",
      "[      0 1814272]\n",
      "[      0 1814528]\n",
      "[      0 1814784]\n",
      "[      0 1815040]\n",
      "[      0 1815296]\n",
      "[      0 1815552]\n",
      "[      0 1815808]\n",
      "[      0 1816064]\n",
      "[      0 1816320]\n",
      "[      0 1816576]\n",
      "[      0 1816832]\n",
      "[      0 1817088]\n",
      "[      0 1817344]\n",
      "[      0 1817600]\n",
      "[      0 1817856]\n",
      "[      0 1818112]\n",
      "[      0 1818368]\n",
      "[      0 1818624]\n",
      "[      0 1818880]\n",
      "[      0 1819136]\n",
      "[      0 1819392]\n",
      "[      0 1819648]\n",
      "[      0 1819904]\n",
      "[      0 1820160]\n",
      "[      0 1820416]\n",
      "[      0 1820672]\n",
      "[      0 1820928]\n",
      "[      0 1821184]\n",
      "[      0 1821440]\n",
      "[      0 1821696]\n",
      "[      0 1821952]\n",
      "[      0 1822208]\n",
      "[      0 1822464]\n",
      "[      0 1822720]\n",
      "[      0 1822976]\n",
      "[      0 1823232]\n",
      "[      0 1823488]\n",
      "[      0 1823744]\n",
      "[      0 1824000]\n",
      "[      0 1824256]\n",
      "[      0 1824512]\n",
      "[      0 1824768]\n",
      "[      0 1825024]\n",
      "[      0 1825280]\n",
      "[      0 1825536]\n",
      "[      0 1825792]\n",
      "[      0 1826048]\n",
      "[      0 1826304]\n",
      "[      0 1826560]\n",
      "[      0 1826816]\n",
      "[      0 1827072]\n",
      "[      0 1827328]\n",
      "[      0 1827584]\n",
      "[      0 1827840]\n",
      "[      0 1828096]\n",
      "[      0 1828352]\n",
      "[      0 1828608]\n",
      "[      0 1828864]\n",
      "[      0 1829120]\n",
      "[      0 1829376]\n",
      "[      0 1829632]\n",
      "[      0 1829888]\n",
      "[      0 1830144]\n",
      "[      0 1830400]\n",
      "[      0 1830656]\n",
      "[      0 1830912]\n",
      "[      0 1831168]\n",
      "[      0 1831424]\n",
      "[      0 1831680]\n",
      "[      0 1831936]\n",
      "[      0 1832192]\n",
      "[      0 1832448]\n",
      "[      0 1832704]\n",
      "[      0 1832960]\n",
      "[      0 1833216]\n",
      "[      0 1833472]\n",
      "[      0 1833728]\n",
      "[      0 1833984]\n",
      "[      0 1834240]\n",
      "[      0 1834496]\n",
      "[      0 1834752]\n",
      "[      0 1835008]\n",
      "[      0 1835264]\n",
      "[      0 1835520]\n",
      "[      0 1835776]\n",
      "[      0 1836032]\n",
      "[      0 1836288]\n",
      "[      0 1836544]\n",
      "[      0 1836800]\n",
      "[      0 1837056]\n",
      "[      0 1837312]\n",
      "[      0 1837568]\n",
      "[      0 1837824]\n",
      "[      0 1838080]\n",
      "[      0 1838336]\n",
      "[      0 1838592]\n",
      "[      0 1838848]\n",
      "[      0 1839104]\n",
      "[      0 1839360]\n",
      "[      0 1839616]\n",
      "[      0 1839872]\n",
      "[      0 1840128]\n",
      "[      0 1840384]\n",
      "[      0 1840640]\n",
      "[      0 1840896]\n",
      "[      0 1841152]\n",
      "[      0 1841408]\n",
      "[      0 1841664]\n",
      "[      0 1841920]\n",
      "[      0 1842176]\n",
      "[      0 1842432]\n",
      "[      0 1842688]\n",
      "[      0 1842944]\n",
      "[      0 1843200]\n",
      "[      0 1843456]\n",
      "[      0 1843712]\n",
      "[      0 1843968]\n",
      "[      0 1844224]\n",
      "[      0 1844480]\n",
      "[      0 1844736]\n",
      "[      0 1844992]\n",
      "[      0 1845248]\n",
      "[      0 1845504]\n",
      "[      0 1845760]\n",
      "[      0 1846016]\n",
      "[      0 1846272]\n",
      "[      0 1846528]\n",
      "[      0 1846784]\n",
      "[      0 1847040]\n",
      "[      0 1847296]\n",
      "[      0 1847552]\n",
      "[      0 1847808]\n",
      "[      0 1848064]\n",
      "[      0 1848320]\n",
      "[      0 1848576]\n",
      "[      0 1848832]\n",
      "[      0 1849088]\n",
      "[      0 1849344]\n",
      "[      0 1849600]\n",
      "[      0 1849856]\n",
      "[      0 1850112]\n",
      "[      0 1850368]\n",
      "[      0 1850624]\n",
      "[      0 1850880]\n",
      "[      0 1851136]\n",
      "[      0 1851392]\n",
      "[      0 1851648]\n",
      "[      0 1851904]\n",
      "[      0 1852160]\n",
      "[      0 1852416]\n",
      "[      0 1852672]\n",
      "[      0 1852928]\n",
      "[      0 1853184]\n",
      "[      0 1853440]\n",
      "[      0 1853696]\n",
      "[      0 1853952]\n",
      "[      0 1854208]\n",
      "[      0 1854464]\n",
      "[      0 1854720]\n",
      "[      0 1854976]\n",
      "[      0 1855232]\n",
      "[      0 1855488]\n",
      "[      0 1855744]\n",
      "[      0 1856000]\n",
      "[      0 1856256]\n",
      "[      0 1856512]\n",
      "[      0 1856768]\n",
      "[      0 1857024]\n",
      "[      0 1857280]\n",
      "[      0 1857536]\n",
      "[      0 1857792]\n",
      "[      0 1858048]\n",
      "[      0 1858304]\n",
      "[      0 1858560]\n",
      "[      0 1858816]\n",
      "[      0 1859072]\n",
      "[      0 1859328]\n",
      "[      0 1859584]\n",
      "[      0 1859840]\n",
      "[      0 1860096]\n",
      "[      0 1860352]\n",
      "[      0 1860608]\n",
      "[      0 1860864]\n",
      "[      0 1861120]\n",
      "[      0 1861376]\n",
      "[      0 1861632]\n",
      "[      0 1861888]\n",
      "[      0 1862144]\n",
      "[      0 1862400]\n",
      "[      0 1862656]\n",
      "[      0 1862912]\n",
      "[      0 1863168]\n",
      "[      0 1863424]\n",
      "[      0 1863680]\n",
      "[      0 1863936]\n",
      "[      0 1864192]\n",
      "[      0 1864448]\n",
      "[      0 1864704]\n",
      "[      0 1864960]\n",
      "[      0 1865216]\n",
      "[      0 1865472]\n",
      "[      0 1865728]\n",
      "[      0 1865984]\n",
      "[      0 1866240]\n",
      "[      0 1866496]\n",
      "[      0 1866752]\n",
      "[      0 1867008]\n",
      "[      0 1867264]\n",
      "[      0 1867520]\n",
      "[      0 1867776]\n",
      "[      0 1868032]\n",
      "[      0 1868288]\n",
      "[      0 1868544]\n",
      "[      0 1868800]\n",
      "[      0 1869056]\n",
      "[      0 1869312]\n",
      "[      0 1869568]\n",
      "[      0 1869824]\n",
      "[      0 1870080]\n",
      "[      0 1870336]\n",
      "[      0 1870592]\n",
      "[      0 1870848]\n",
      "[      0 1871104]\n",
      "[      0 1871360]\n",
      "[      0 1871616]\n",
      "[      0 1871872]\n",
      "[      0 1872128]\n",
      "[      0 1872384]\n",
      "[      0 1872640]\n",
      "[      0 1872896]\n",
      "[      0 1873152]\n",
      "[      0 1873408]\n",
      "[      0 1873664]\n",
      "[      0 1873920]\n",
      "[      0 1874176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1874432]\n",
      "[      0 1874688]\n",
      "[      0 1874944]\n",
      "[      0 1875200]\n",
      "[      0 1875456]\n",
      "[      0 1875712]\n",
      "[      0 1875968]\n",
      "[      0 1876224]\n",
      "[      0 1876480]\n",
      "[      0 1876736]\n",
      "[      0 1876992]\n",
      "[      0 1877248]\n",
      "[      0 1877504]\n",
      "[      0 1877760]\n",
      "[      0 1878016]\n",
      "[      0 1878272]\n",
      "[      0 1878528]\n",
      "[      0 1878784]\n",
      "[  0 256]\n",
      "(1878784, 2)\n",
      "time =  1:31:39.089432\n",
      "done!\n",
      "(7341, 2)\n",
      "[  0 256]\n",
      "[  0 512]\n",
      "[  0 768]\n",
      "[   0 1024]\n",
      "[   0 1280]\n",
      "[   0 1536]\n",
      "[   0 1792]\n",
      "[   0 2048]\n",
      "[   0 2304]\n",
      "[   0 2560]\n",
      "[   0 2816]\n",
      "[   0 3072]\n",
      "[   0 3328]\n",
      "[   0 3584]\n",
      "[   0 3840]\n",
      "[   0 4096]\n",
      "[   0 4352]\n",
      "[   0 4608]\n",
      "[   0 4864]\n",
      "[   0 5120]\n",
      "[   0 5376]\n",
      "[   0 5632]\n",
      "[   0 5888]\n",
      "[   0 6144]\n",
      "[   0 6400]\n",
      "[   0 6656]\n",
      "[   0 6912]\n",
      "[   0 7168]\n",
      "[   0 7424]\n",
      "[   0 7680]\n",
      "[   0 7936]\n",
      "[   0 8192]\n",
      "[   0 8448]\n",
      "[   0 8704]\n",
      "[   0 8960]\n",
      "[   0 9216]\n",
      "[   0 9472]\n",
      "[   0 9728]\n",
      "[   0 9984]\n",
      "[    0 10240]\n",
      "[    0 10496]\n",
      "[    0 10752]\n",
      "[    0 11008]\n",
      "[    0 11264]\n",
      "[    0 11520]\n",
      "[    0 11776]\n",
      "[    0 12032]\n",
      "[    0 12288]\n",
      "[    0 12544]\n",
      "[    0 12800]\n",
      "[    0 13056]\n",
      "[    0 13312]\n",
      "[    0 13568]\n",
      "[    0 13824]\n",
      "[    0 14080]\n",
      "[    0 14336]\n",
      "[    0 14592]\n",
      "[    0 14848]\n",
      "[    0 15104]\n",
      "[    0 15360]\n",
      "[    0 15616]\n",
      "[    0 15872]\n",
      "[    0 16128]\n",
      "[    0 16384]\n",
      "[    0 16640]\n",
      "[    0 16896]\n",
      "[    0 17152]\n",
      "[    0 17408]\n",
      "[    0 17664]\n",
      "[    0 17920]\n",
      "[    0 18176]\n",
      "[    0 18432]\n",
      "[    0 18688]\n",
      "[    0 18944]\n",
      "[    0 19200]\n",
      "[    0 19456]\n",
      "[    0 19712]\n",
      "[    0 19968]\n",
      "[    0 20224]\n",
      "[    0 20480]\n",
      "[    0 20736]\n",
      "[    0 20992]\n",
      "[    0 21248]\n",
      "[    0 21504]\n",
      "[    0 21760]\n",
      "[    0 22016]\n",
      "[    0 22272]\n",
      "[    0 22528]\n",
      "[    0 22784]\n",
      "[    0 23040]\n",
      "[    0 23296]\n",
      "[    0 23552]\n",
      "[    0 23808]\n",
      "[    0 24064]\n",
      "[    0 24320]\n",
      "[    0 24576]\n",
      "[    0 24832]\n",
      "[    0 25088]\n",
      "[    0 25344]\n",
      "[    0 25600]\n",
      "[    0 25856]\n",
      "[    0 26112]\n",
      "[    0 26368]\n",
      "[    0 26624]\n",
      "[    0 26880]\n",
      "[    0 27136]\n",
      "[    0 27392]\n",
      "[    0 27648]\n",
      "[    0 27904]\n",
      "[    0 28160]\n",
      "[    0 28416]\n",
      "[    0 28672]\n",
      "[    0 28928]\n",
      "[    0 29184]\n",
      "[    0 29440]\n",
      "[    0 29696]\n",
      "[    0 29952]\n",
      "[    0 30208]\n",
      "[    0 30464]\n",
      "[    0 30720]\n",
      "[    0 30976]\n",
      "[    0 31232]\n",
      "[    0 31488]\n",
      "[    0 31744]\n",
      "[    0 32000]\n",
      "[    0 32256]\n",
      "[    0 32512]\n",
      "[    0 32768]\n",
      "[    0 33024]\n",
      "[    0 33280]\n",
      "[    0 33536]\n",
      "[    0 33792]\n",
      "[    0 34048]\n",
      "[    0 34304]\n",
      "[    0 34560]\n",
      "[    0 34816]\n",
      "[    0 35072]\n",
      "[    0 35328]\n",
      "[    0 35584]\n",
      "[    0 35840]\n",
      "[    0 36096]\n",
      "[    0 36352]\n",
      "[    0 36608]\n",
      "[    0 36864]\n",
      "[    0 37120]\n",
      "[    0 37376]\n",
      "[    0 37632]\n",
      "[    0 37888]\n",
      "[    0 38144]\n",
      "[    0 38400]\n",
      "[    0 38656]\n",
      "[    0 38912]\n",
      "[    0 39168]\n",
      "[    0 39424]\n",
      "[    0 39680]\n",
      "[    0 39936]\n",
      "[    0 40192]\n",
      "[    0 40448]\n",
      "[    0 40704]\n",
      "[    0 40960]\n",
      "[    0 41216]\n",
      "[    0 41472]\n",
      "[    0 41728]\n",
      "[    0 41984]\n",
      "[    0 42240]\n",
      "[    0 42496]\n",
      "[    0 42752]\n",
      "[    0 43008]\n",
      "[    0 43264]\n",
      "[    0 43520]\n",
      "[    0 43776]\n",
      "[    0 44032]\n",
      "[    0 44288]\n",
      "[    0 44544]\n",
      "[    0 44800]\n",
      "[    0 45056]\n",
      "[    0 45312]\n",
      "[    0 45568]\n",
      "[    0 45824]\n",
      "[    0 46080]\n",
      "[    0 46336]\n",
      "[    0 46592]\n",
      "[    0 46848]\n",
      "[    0 47104]\n",
      "[    0 47360]\n",
      "[    0 47616]\n",
      "[    0 47872]\n",
      "[    0 48128]\n",
      "[    0 48384]\n",
      "[    0 48640]\n",
      "[    0 48896]\n",
      "[    0 49152]\n",
      "[    0 49408]\n",
      "[    0 49664]\n",
      "[    0 49920]\n",
      "[    0 50176]\n",
      "[    0 50432]\n",
      "[    0 50688]\n",
      "[    0 50944]\n",
      "[    0 51200]\n",
      "[    0 51456]\n",
      "[    0 51712]\n",
      "[    0 51968]\n",
      "[    0 52224]\n",
      "[    0 52480]\n",
      "[    0 52736]\n",
      "[    0 52992]\n",
      "[    0 53248]\n",
      "[    0 53504]\n",
      "[    0 53760]\n",
      "[    0 54016]\n",
      "[    0 54272]\n",
      "[    0 54528]\n",
      "[    0 54784]\n",
      "[    0 55040]\n",
      "[    0 55296]\n",
      "[    0 55552]\n",
      "[    0 55808]\n",
      "[    0 56064]\n",
      "[    0 56320]\n",
      "[    0 56576]\n",
      "[    0 56832]\n",
      "[    0 57088]\n",
      "[    0 57344]\n",
      "[    0 57600]\n",
      "[    0 57856]\n",
      "[    0 58112]\n",
      "[    0 58368]\n",
      "[    0 58624]\n",
      "[    0 58880]\n",
      "[    0 59136]\n",
      "[    0 59392]\n",
      "[    0 59648]\n",
      "[    0 59904]\n",
      "[    0 60160]\n",
      "[    0 60416]\n",
      "[    0 60672]\n",
      "[    0 60928]\n",
      "[    0 61184]\n",
      "[    0 61440]\n",
      "[    0 61696]\n",
      "[    0 61952]\n",
      "[    0 62208]\n",
      "[    0 62464]\n",
      "[    0 62720]\n",
      "[    0 62976]\n",
      "[    0 63232]\n",
      "[    0 63488]\n",
      "[    0 63744]\n",
      "[    0 64000]\n",
      "[    0 64256]\n",
      "[    0 64512]\n",
      "[    0 64768]\n",
      "[    0 65024]\n",
      "[    0 65280]\n",
      "[    0 65536]\n",
      "[    0 65792]\n",
      "[    0 66048]\n",
      "[    0 66304]\n",
      "[    0 66560]\n",
      "[    0 66816]\n",
      "[    0 67072]\n",
      "[    0 67328]\n",
      "[    0 67584]\n",
      "[    0 67840]\n",
      "[    0 68096]\n",
      "[    0 68352]\n",
      "[    0 68608]\n",
      "[    0 68864]\n",
      "[    0 69120]\n",
      "[    0 69376]\n",
      "[    0 69632]\n",
      "[    0 69888]\n",
      "[    0 70144]\n",
      "[    0 70400]\n",
      "[    0 70656]\n",
      "[    0 70912]\n",
      "[    0 71168]\n",
      "[    0 71424]\n",
      "[    0 71680]\n",
      "[    0 71936]\n",
      "[    0 72192]\n",
      "[    0 72448]\n",
      "[    0 72704]\n",
      "[    0 72960]\n",
      "[    0 73216]\n",
      "[    0 73472]\n",
      "[    0 73728]\n",
      "[    0 73984]\n",
      "[    0 74240]\n",
      "[    0 74496]\n",
      "[    0 74752]\n",
      "[    0 75008]\n",
      "[    0 75264]\n",
      "[    0 75520]\n",
      "[    0 75776]\n",
      "[    0 76032]\n",
      "[    0 76288]\n",
      "[    0 76544]\n",
      "[    0 76800]\n",
      "[    0 77056]\n",
      "[    0 77312]\n",
      "[    0 77568]\n",
      "[    0 77824]\n",
      "[    0 78080]\n",
      "[    0 78336]\n",
      "[    0 78592]\n",
      "[    0 78848]\n",
      "[    0 79104]\n",
      "[    0 79360]\n",
      "[    0 79616]\n",
      "[    0 79872]\n",
      "[    0 80128]\n",
      "[    0 80384]\n",
      "[    0 80640]\n",
      "[    0 80896]\n",
      "[    0 81152]\n",
      "[    0 81408]\n",
      "[    0 81664]\n",
      "[    0 81920]\n",
      "[    0 82176]\n",
      "[    0 82432]\n",
      "[    0 82688]\n",
      "[    0 82944]\n",
      "[    0 83200]\n",
      "[    0 83456]\n",
      "[    0 83712]\n",
      "[    0 83968]\n",
      "[    0 84224]\n",
      "[    0 84480]\n",
      "[    0 84736]\n",
      "[    0 84992]\n",
      "[    0 85248]\n",
      "[    0 85504]\n",
      "[    0 85760]\n",
      "[    0 86016]\n",
      "[    0 86272]\n",
      "[    0 86528]\n",
      "[    0 86784]\n",
      "[    0 87040]\n",
      "[    0 87296]\n",
      "[    0 87552]\n",
      "[    0 87808]\n",
      "[    0 88064]\n",
      "[    0 88320]\n",
      "[    0 88576]\n",
      "[    0 88832]\n",
      "[    0 89088]\n",
      "[    0 89344]\n",
      "[    0 89600]\n",
      "[    0 89856]\n",
      "[    0 90112]\n",
      "[    0 90368]\n",
      "[    0 90624]\n",
      "[    0 90880]\n",
      "[    0 91136]\n",
      "[    0 91392]\n",
      "[    0 91648]\n",
      "[    0 91904]\n",
      "[    0 92160]\n",
      "[    0 92416]\n",
      "[    0 92672]\n",
      "[    0 92928]\n",
      "[    0 93184]\n",
      "[    0 93440]\n",
      "[    0 93696]\n",
      "[    0 93952]\n",
      "[    0 94208]\n",
      "[    0 94464]\n",
      "[    0 94720]\n",
      "[    0 94976]\n",
      "[    0 95232]\n",
      "[    0 95488]\n",
      "[    0 95744]\n",
      "[    0 96000]\n",
      "[    0 96256]\n",
      "[    0 96512]\n",
      "[    0 96768]\n",
      "[    0 97024]\n",
      "[    0 97280]\n",
      "[    0 97536]\n",
      "[    0 97792]\n",
      "[    0 98048]\n",
      "[    0 98304]\n",
      "[    0 98560]\n",
      "[    0 98816]\n",
      "[    0 99072]\n",
      "[    0 99328]\n",
      "[    0 99584]\n",
      "[    0 99840]\n",
      "[     0 100096]\n",
      "[     0 100352]\n",
      "[     0 100608]\n",
      "[     0 100864]\n",
      "[     0 101120]\n",
      "[     0 101376]\n",
      "[     0 101632]\n",
      "[     0 101888]\n",
      "[     0 102144]\n",
      "[     0 102400]\n",
      "[     0 102656]\n",
      "[     0 102912]\n",
      "[     0 103168]\n",
      "[     0 103424]\n",
      "[     0 103680]\n",
      "[     0 103936]\n",
      "[     0 104192]\n",
      "[     0 104448]\n",
      "[     0 104704]\n",
      "[     0 104960]\n",
      "[     0 105216]\n",
      "[     0 105472]\n",
      "[     0 105728]\n",
      "[     0 105984]\n",
      "[     0 106240]\n",
      "[     0 106496]\n",
      "[     0 106752]\n",
      "[     0 107008]\n",
      "[     0 107264]\n",
      "[     0 107520]\n",
      "[     0 107776]\n",
      "[     0 108032]\n",
      "[     0 108288]\n",
      "[     0 108544]\n",
      "[     0 108800]\n",
      "[     0 109056]\n",
      "[     0 109312]\n",
      "[     0 109568]\n",
      "[     0 109824]\n",
      "[     0 110080]\n",
      "[     0 110336]\n",
      "[     0 110592]\n",
      "[     0 110848]\n",
      "[     0 111104]\n",
      "[     0 111360]\n",
      "[     0 111616]\n",
      "[     0 111872]\n",
      "[     0 112128]\n",
      "[     0 112384]\n",
      "[     0 112640]\n",
      "[     0 112896]\n",
      "[     0 113152]\n",
      "[     0 113408]\n",
      "[     0 113664]\n",
      "[     0 113920]\n",
      "[     0 114176]\n",
      "[     0 114432]\n",
      "[     0 114688]\n",
      "[     0 114944]\n",
      "[     0 115200]\n",
      "[     0 115456]\n",
      "[     0 115712]\n",
      "[     0 115968]\n",
      "[     0 116224]\n",
      "[     0 116480]\n",
      "[     0 116736]\n",
      "[     0 116992]\n",
      "[     0 117248]\n",
      "[     0 117504]\n",
      "[     0 117760]\n",
      "[     0 118016]\n",
      "[     0 118272]\n",
      "[     0 118528]\n",
      "[     0 118784]\n",
      "[     0 119040]\n",
      "[     0 119296]\n",
      "[     0 119552]\n",
      "[     0 119808]\n",
      "[     0 120064]\n",
      "[     0 120320]\n",
      "[     0 120576]\n",
      "[     0 120832]\n",
      "[     0 121088]\n",
      "[     0 121344]\n",
      "[     0 121600]\n",
      "[     0 121856]\n",
      "[     0 122112]\n",
      "[     0 122368]\n",
      "[     0 122624]\n",
      "[     0 122880]\n",
      "[     0 123136]\n",
      "[     0 123392]\n",
      "[     0 123648]\n",
      "[     0 123904]\n",
      "[     0 124160]\n",
      "[     0 124416]\n",
      "[     0 124672]\n",
      "[     0 124928]\n",
      "[     0 125184]\n",
      "[     0 125440]\n",
      "[     0 125696]\n",
      "[     0 125952]\n",
      "[     0 126208]\n",
      "[     0 126464]\n",
      "[     0 126720]\n",
      "[     0 126976]\n",
      "[     0 127232]\n",
      "[     0 127488]\n",
      "[     0 127744]\n",
      "[     0 128000]\n",
      "[     0 128256]\n",
      "[     0 128512]\n",
      "[     0 128768]\n",
      "[     0 129024]\n",
      "[     0 129280]\n",
      "[     0 129536]\n",
      "[     0 129792]\n",
      "[     0 130048]\n",
      "[     0 130304]\n",
      "[     0 130560]\n",
      "[     0 130816]\n",
      "[     0 131072]\n",
      "[     0 131328]\n",
      "[     0 131584]\n",
      "[     0 131840]\n",
      "[     0 132096]\n",
      "[     0 132352]\n",
      "[     0 132608]\n",
      "[     0 132864]\n",
      "[     0 133120]\n",
      "[     0 133376]\n",
      "[     0 133632]\n",
      "[     0 133888]\n",
      "[     0 134144]\n",
      "[     0 134400]\n",
      "[     0 134656]\n",
      "[     0 134912]\n",
      "[     0 135168]\n",
      "[     0 135424]\n",
      "[     0 135680]\n",
      "[     0 135936]\n",
      "[     0 136192]\n",
      "[     0 136448]\n",
      "[     0 136704]\n",
      "[     0 136960]\n",
      "[     0 137216]\n",
      "[     0 137472]\n",
      "[     0 137728]\n",
      "[     0 137984]\n",
      "[     0 138240]\n",
      "[     0 138496]\n",
      "[     0 138752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 139008]\n",
      "[     0 139264]\n",
      "[     0 139520]\n",
      "[     0 139776]\n",
      "[     0 140032]\n",
      "[     0 140288]\n",
      "[     0 140544]\n",
      "[     0 140800]\n",
      "[     0 141056]\n",
      "[     0 141312]\n",
      "[     0 141568]\n",
      "[     0 141824]\n",
      "[     0 142080]\n",
      "[     0 142336]\n",
      "[     0 142592]\n",
      "[     0 142848]\n",
      "[     0 143104]\n",
      "[     0 143360]\n",
      "[     0 143616]\n",
      "[     0 143872]\n",
      "[     0 144128]\n",
      "[     0 144384]\n",
      "[     0 144640]\n",
      "[     0 144896]\n",
      "[     0 145152]\n",
      "[     0 145408]\n",
      "[     0 145664]\n",
      "[     0 145920]\n",
      "[     0 146176]\n",
      "[     0 146432]\n",
      "[     0 146688]\n",
      "[     0 146944]\n",
      "[     0 147200]\n",
      "[     0 147456]\n",
      "[     0 147712]\n",
      "[     0 147968]\n",
      "[     0 148224]\n",
      "[     0 148480]\n",
      "[     0 148736]\n",
      "[     0 148992]\n",
      "[     0 149248]\n",
      "[     0 149504]\n",
      "[     0 149760]\n",
      "[     0 150016]\n",
      "[     0 150272]\n",
      "[     0 150528]\n",
      "[     0 150784]\n",
      "[     0 151040]\n",
      "[     0 151296]\n",
      "[     0 151552]\n",
      "[     0 151808]\n",
      "[     0 152064]\n",
      "[     0 152320]\n",
      "[     0 152576]\n",
      "[     0 152832]\n",
      "[     0 153088]\n",
      "[     0 153344]\n",
      "[     0 153600]\n",
      "[     0 153856]\n",
      "[     0 154112]\n",
      "[     0 154368]\n",
      "[     0 154624]\n",
      "[     0 154880]\n",
      "[     0 155136]\n",
      "[     0 155392]\n",
      "[     0 155648]\n",
      "[     0 155904]\n",
      "[     0 156160]\n",
      "[     0 156416]\n",
      "[     0 156672]\n",
      "[     0 156928]\n",
      "[     0 157184]\n",
      "[     0 157440]\n",
      "[     0 157696]\n",
      "[     0 157952]\n",
      "[     0 158208]\n",
      "[     0 158464]\n",
      "[     0 158720]\n",
      "[     0 158976]\n",
      "[     0 159232]\n",
      "[     0 159488]\n",
      "[     0 159744]\n",
      "[     0 160000]\n",
      "[     0 160256]\n",
      "[     0 160512]\n",
      "[     0 160768]\n",
      "[     0 161024]\n",
      "[     0 161280]\n",
      "[     0 161536]\n",
      "[     0 161792]\n",
      "[     0 162048]\n",
      "[     0 162304]\n",
      "[     0 162560]\n",
      "[     0 162816]\n",
      "[     0 163072]\n",
      "[     0 163328]\n",
      "[     0 163584]\n",
      "[     0 163840]\n",
      "[     0 164096]\n",
      "[     0 164352]\n",
      "[     0 164608]\n",
      "[     0 164864]\n",
      "[     0 165120]\n",
      "[     0 165376]\n",
      "[     0 165632]\n",
      "[     0 165888]\n",
      "[     0 166144]\n",
      "[     0 166400]\n",
      "[     0 166656]\n",
      "[     0 166912]\n",
      "[     0 167168]\n",
      "[     0 167424]\n",
      "[     0 167680]\n",
      "[     0 167936]\n",
      "[     0 168192]\n",
      "[     0 168448]\n",
      "[     0 168704]\n",
      "[     0 168960]\n",
      "[     0 169216]\n",
      "[     0 169472]\n",
      "[     0 169728]\n",
      "[     0 169984]\n",
      "[     0 170240]\n",
      "[     0 170496]\n",
      "[     0 170752]\n",
      "[     0 171008]\n",
      "[     0 171264]\n",
      "[     0 171520]\n",
      "[     0 171776]\n",
      "[     0 172032]\n",
      "[     0 172288]\n",
      "[     0 172544]\n",
      "[     0 172800]\n",
      "[     0 173056]\n",
      "[     0 173312]\n",
      "[     0 173568]\n",
      "[     0 173824]\n",
      "[     0 174080]\n",
      "[     0 174336]\n",
      "[     0 174592]\n",
      "[     0 174848]\n",
      "[     0 175104]\n",
      "[     0 175360]\n",
      "[     0 175616]\n",
      "[     0 175872]\n",
      "[     0 176128]\n",
      "[     0 176384]\n",
      "[     0 176640]\n",
      "[     0 176896]\n",
      "[     0 177152]\n",
      "[     0 177408]\n",
      "[     0 177664]\n",
      "[     0 177920]\n",
      "[     0 178176]\n",
      "[     0 178432]\n",
      "[     0 178688]\n",
      "[     0 178944]\n",
      "[     0 179200]\n",
      "[     0 179456]\n",
      "[     0 179712]\n",
      "[     0 179968]\n",
      "[     0 180224]\n",
      "[     0 180480]\n",
      "[     0 180736]\n",
      "[     0 180992]\n",
      "[     0 181248]\n",
      "[     0 181504]\n",
      "[     0 181760]\n",
      "[     0 182016]\n",
      "[     0 182272]\n",
      "[     0 182528]\n",
      "[     0 182784]\n",
      "[     0 183040]\n",
      "[     0 183296]\n",
      "[     0 183552]\n",
      "[     0 183808]\n",
      "[     0 184064]\n",
      "[     0 184320]\n",
      "[     0 184576]\n",
      "[     0 184832]\n",
      "[     0 185088]\n",
      "[     0 185344]\n",
      "[     0 185600]\n",
      "[     0 185856]\n",
      "[     0 186112]\n",
      "[     0 186368]\n",
      "[     0 186624]\n",
      "[     0 186880]\n",
      "[     0 187136]\n",
      "[     0 187392]\n",
      "[     0 187648]\n",
      "[     0 187904]\n",
      "[     0 188160]\n",
      "[     0 188416]\n",
      "[     0 188672]\n",
      "[     0 188928]\n",
      "[     0 189184]\n",
      "[     0 189440]\n",
      "[     0 189696]\n",
      "[     0 189952]\n",
      "[     0 190208]\n",
      "[     0 190464]\n",
      "[     0 190720]\n",
      "[     0 190976]\n",
      "[     0 191232]\n",
      "[     0 191488]\n",
      "[     0 191744]\n",
      "[     0 192000]\n",
      "[     0 192256]\n",
      "[     0 192512]\n",
      "[     0 192768]\n",
      "[     0 193024]\n",
      "[     0 193280]\n",
      "[     0 193536]\n",
      "[     0 193792]\n",
      "[     0 194048]\n",
      "[     0 194304]\n",
      "[     0 194560]\n",
      "[     0 194816]\n",
      "[     0 195072]\n",
      "[     0 195328]\n",
      "[     0 195584]\n",
      "[     0 195840]\n",
      "[     0 196096]\n",
      "[     0 196352]\n",
      "[     0 196608]\n",
      "[     0 196864]\n",
      "[     0 197120]\n",
      "[     0 197376]\n",
      "[     0 197632]\n",
      "[     0 197888]\n",
      "[     0 198144]\n",
      "[     0 198400]\n",
      "[     0 198656]\n",
      "[     0 198912]\n",
      "[     0 199168]\n",
      "[     0 199424]\n",
      "[     0 199680]\n",
      "[     0 199936]\n",
      "[     0 200192]\n",
      "[     0 200448]\n",
      "[     0 200704]\n",
      "[     0 200960]\n",
      "[     0 201216]\n",
      "[     0 201472]\n",
      "[     0 201728]\n",
      "[     0 201984]\n",
      "[     0 202240]\n",
      "[     0 202496]\n",
      "[     0 202752]\n",
      "[     0 203008]\n",
      "[     0 203264]\n",
      "[     0 203520]\n",
      "[     0 203776]\n",
      "[     0 204032]\n",
      "[     0 204288]\n",
      "[     0 204544]\n",
      "[     0 204800]\n",
      "[     0 205056]\n",
      "[     0 205312]\n",
      "[     0 205568]\n",
      "[     0 205824]\n",
      "[     0 206080]\n",
      "[     0 206336]\n",
      "[     0 206592]\n",
      "[     0 206848]\n",
      "[     0 207104]\n",
      "[     0 207360]\n",
      "[     0 207616]\n",
      "[     0 207872]\n",
      "[     0 208128]\n",
      "[     0 208384]\n",
      "[     0 208640]\n",
      "[     0 208896]\n",
      "[     0 209152]\n",
      "[     0 209408]\n",
      "[     0 209664]\n",
      "[     0 209920]\n",
      "[     0 210176]\n",
      "[     0 210432]\n",
      "[     0 210688]\n",
      "[     0 210944]\n",
      "[     0 211200]\n",
      "[     0 211456]\n",
      "[     0 211712]\n",
      "[     0 211968]\n",
      "[     0 212224]\n",
      "[     0 212480]\n",
      "[     0 212736]\n",
      "[     0 212992]\n",
      "[     0 213248]\n",
      "[     0 213504]\n",
      "[     0 213760]\n",
      "[     0 214016]\n",
      "[     0 214272]\n",
      "[     0 214528]\n",
      "[     0 214784]\n",
      "[     0 215040]\n",
      "[     0 215296]\n",
      "[     0 215552]\n",
      "[     0 215808]\n",
      "[     0 216064]\n",
      "[     0 216320]\n",
      "[     0 216576]\n",
      "[     0 216832]\n",
      "[     0 217088]\n",
      "[     0 217344]\n",
      "[     0 217600]\n",
      "[     0 217856]\n",
      "[     0 218112]\n",
      "[     0 218368]\n",
      "[     0 218624]\n",
      "[     0 218880]\n",
      "[     0 219136]\n",
      "[     0 219392]\n",
      "[     0 219648]\n",
      "[     0 219904]\n",
      "[     0 220160]\n",
      "[     0 220416]\n",
      "[     0 220672]\n",
      "[     0 220928]\n",
      "[     0 221184]\n",
      "[     0 221440]\n",
      "[     0 221696]\n",
      "[     0 221952]\n",
      "[     0 222208]\n",
      "[     0 222464]\n",
      "[     0 222720]\n",
      "[     0 222976]\n",
      "[     0 223232]\n",
      "[     0 223488]\n",
      "[     0 223744]\n",
      "[     0 224000]\n",
      "[     0 224256]\n",
      "[     0 224512]\n",
      "[     0 224768]\n",
      "[     0 225024]\n",
      "[     0 225280]\n",
      "[     0 225536]\n",
      "[     0 225792]\n",
      "[     0 226048]\n",
      "[     0 226304]\n",
      "[     0 226560]\n",
      "[     0 226816]\n",
      "[     0 227072]\n",
      "[     0 227328]\n",
      "[     0 227584]\n",
      "[     0 227840]\n",
      "[     0 228096]\n",
      "[     0 228352]\n",
      "[     0 228608]\n",
      "[     0 228864]\n",
      "[     0 229120]\n",
      "[     0 229376]\n",
      "[     0 229632]\n",
      "[     0 229888]\n",
      "[     0 230144]\n",
      "[     0 230400]\n",
      "[     0 230656]\n",
      "[     0 230912]\n",
      "[     0 231168]\n",
      "[     0 231424]\n",
      "[     0 231680]\n",
      "[     0 231936]\n",
      "[     0 232192]\n",
      "[     0 232448]\n",
      "[     0 232704]\n",
      "[     0 232960]\n",
      "[     0 233216]\n",
      "[     0 233472]\n",
      "[     0 233728]\n",
      "[     0 233984]\n",
      "[     0 234240]\n",
      "[     0 234496]\n",
      "[     0 234752]\n",
      "[     0 235008]\n",
      "[     0 235264]\n",
      "[     0 235520]\n",
      "[     0 235776]\n",
      "[     0 236032]\n",
      "[     0 236288]\n",
      "[     0 236544]\n",
      "[     0 236800]\n",
      "[     0 237056]\n",
      "[     0 237312]\n",
      "[     0 237568]\n",
      "[     0 237824]\n",
      "[     0 238080]\n",
      "[     0 238336]\n",
      "[     0 238592]\n",
      "[     0 238848]\n",
      "[     0 239104]\n",
      "[     0 239360]\n",
      "[     0 239616]\n",
      "[     0 239872]\n",
      "[     0 240128]\n",
      "[     0 240384]\n",
      "[     0 240640]\n",
      "[     0 240896]\n",
      "[     0 241152]\n",
      "[     0 241408]\n",
      "[     0 241664]\n",
      "[     0 241920]\n",
      "[     0 242176]\n",
      "[     0 242432]\n",
      "[     0 242688]\n",
      "[     0 242944]\n",
      "[     0 243200]\n",
      "[     0 243456]\n",
      "[     0 243712]\n",
      "[     0 243968]\n",
      "[     0 244224]\n",
      "[     0 244480]\n",
      "[     0 244736]\n",
      "[     0 244992]\n",
      "[     0 245248]\n",
      "[     0 245504]\n",
      "[     0 245760]\n",
      "[     0 246016]\n",
      "[     0 246272]\n",
      "[     0 246528]\n",
      "[     0 246784]\n",
      "[     0 247040]\n",
      "[     0 247296]\n",
      "[     0 247552]\n",
      "[     0 247808]\n",
      "[     0 248064]\n",
      "[     0 248320]\n",
      "[     0 248576]\n",
      "[     0 248832]\n",
      "[     0 249088]\n",
      "[     0 249344]\n",
      "[     0 249600]\n",
      "[     0 249856]\n",
      "[     0 250112]\n",
      "[     0 250368]\n",
      "[     0 250624]\n",
      "[     0 250880]\n",
      "[     0 251136]\n",
      "[     0 251392]\n",
      "[     0 251648]\n",
      "[     0 251904]\n",
      "[     0 252160]\n",
      "[     0 252416]\n",
      "[     0 252672]\n",
      "[     0 252928]\n",
      "[     0 253184]\n",
      "[     0 253440]\n",
      "[     0 253696]\n",
      "[     0 253952]\n",
      "[     0 254208]\n",
      "[     0 254464]\n",
      "[     0 254720]\n",
      "[     0 254976]\n",
      "[     0 255232]\n",
      "[     0 255488]\n",
      "[     0 255744]\n",
      "[     0 256000]\n",
      "[     0 256256]\n",
      "[     0 256512]\n",
      "[     0 256768]\n",
      "[     0 257024]\n",
      "[     0 257280]\n",
      "[     0 257536]\n",
      "[     0 257792]\n",
      "[     0 258048]\n",
      "[     0 258304]\n",
      "[     0 258560]\n",
      "[     0 258816]\n",
      "[     0 259072]\n",
      "[     0 259328]\n",
      "[     0 259584]\n",
      "[     0 259840]\n",
      "[     0 260096]\n",
      "[     0 260352]\n",
      "[     0 260608]\n",
      "[     0 260864]\n",
      "[     0 261120]\n",
      "[     0 261376]\n",
      "[     0 261632]\n",
      "[     0 261888]\n",
      "[     0 262144]\n",
      "[     0 262400]\n",
      "[     0 262656]\n",
      "[     0 262912]\n",
      "[     0 263168]\n",
      "[     0 263424]\n",
      "[     0 263680]\n",
      "[     0 263936]\n",
      "[     0 264192]\n",
      "[     0 264448]\n",
      "[     0 264704]\n",
      "[     0 264960]\n",
      "[     0 265216]\n",
      "[     0 265472]\n",
      "[     0 265728]\n",
      "[     0 265984]\n",
      "[     0 266240]\n",
      "[     0 266496]\n",
      "[     0 266752]\n",
      "[     0 267008]\n",
      "[     0 267264]\n",
      "[     0 267520]\n",
      "[     0 267776]\n",
      "[     0 268032]\n",
      "[     0 268288]\n",
      "[     0 268544]\n",
      "[     0 268800]\n",
      "[     0 269056]\n",
      "[     0 269312]\n",
      "[     0 269568]\n",
      "[     0 269824]\n",
      "[     0 270080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 270336]\n",
      "[     0 270592]\n",
      "[     0 270848]\n",
      "[     0 271104]\n",
      "[     0 271360]\n",
      "[     0 271616]\n",
      "[     0 271872]\n",
      "[     0 272128]\n",
      "[     0 272384]\n",
      "[     0 272640]\n",
      "[     0 272896]\n",
      "[     0 273152]\n",
      "[     0 273408]\n",
      "[     0 273664]\n",
      "[     0 273920]\n",
      "[     0 274176]\n",
      "[     0 274432]\n",
      "[     0 274688]\n",
      "[     0 274944]\n",
      "[     0 275200]\n",
      "[     0 275456]\n",
      "[     0 275712]\n",
      "[     0 275968]\n",
      "[     0 276224]\n",
      "[     0 276480]\n",
      "[     0 276736]\n",
      "[     0 276992]\n",
      "[     0 277248]\n",
      "[     0 277504]\n",
      "[     0 277760]\n",
      "[     0 278016]\n",
      "[     0 278272]\n",
      "[     0 278528]\n",
      "[     0 278784]\n",
      "[     0 279040]\n",
      "[     0 279296]\n",
      "[     0 279552]\n",
      "[     0 279808]\n",
      "[     0 280064]\n",
      "[     0 280320]\n",
      "[     0 280576]\n",
      "[     0 280832]\n",
      "[     0 281088]\n",
      "[     0 281344]\n",
      "[     0 281600]\n",
      "[     0 281856]\n",
      "[     0 282112]\n",
      "[     0 282368]\n",
      "[     0 282624]\n",
      "[     0 282880]\n",
      "[     0 283136]\n",
      "[     0 283392]\n",
      "[     0 283648]\n",
      "[     0 283904]\n",
      "[     0 284160]\n",
      "[     0 284416]\n",
      "[     0 284672]\n",
      "[     0 284928]\n",
      "[     0 285184]\n",
      "[     0 285440]\n",
      "[     0 285696]\n",
      "[     0 285952]\n",
      "[     0 286208]\n",
      "[     0 286464]\n",
      "[     0 286720]\n",
      "[     0 286976]\n",
      "[     0 287232]\n",
      "[     0 287488]\n",
      "[     0 287744]\n",
      "[     0 288000]\n",
      "[     0 288256]\n",
      "[     0 288512]\n",
      "[     0 288768]\n",
      "[     0 289024]\n",
      "[     0 289280]\n",
      "[     0 289536]\n",
      "[     0 289792]\n",
      "[     0 290048]\n",
      "[     0 290304]\n",
      "[     0 290560]\n",
      "[     0 290816]\n",
      "[     0 291072]\n",
      "[     0 291328]\n",
      "[     0 291584]\n",
      "[     0 291840]\n",
      "[     0 292096]\n",
      "[     0 292352]\n",
      "[     0 292608]\n",
      "[     0 292864]\n",
      "[     0 293120]\n",
      "[     0 293376]\n",
      "[     0 293632]\n",
      "[     0 293888]\n",
      "[     0 294144]\n",
      "[     0 294400]\n",
      "[     0 294656]\n",
      "[     0 294912]\n",
      "[     0 295168]\n",
      "[     0 295424]\n",
      "[     0 295680]\n",
      "[     0 295936]\n",
      "[     0 296192]\n",
      "[     0 296448]\n",
      "[     0 296704]\n",
      "[     0 296960]\n",
      "[     0 297216]\n",
      "[     0 297472]\n",
      "[     0 297728]\n",
      "[     0 297984]\n",
      "[     0 298240]\n",
      "[     0 298496]\n",
      "[     0 298752]\n",
      "[     0 299008]\n",
      "[     0 299264]\n",
      "[     0 299520]\n",
      "[     0 299776]\n",
      "[     0 300032]\n",
      "[     0 300288]\n",
      "[     0 300544]\n",
      "[     0 300800]\n",
      "[     0 301056]\n",
      "[     0 301312]\n",
      "[     0 301568]\n",
      "[     0 301824]\n",
      "[     0 302080]\n",
      "[     0 302336]\n",
      "[     0 302592]\n",
      "[     0 302848]\n",
      "[     0 303104]\n",
      "[     0 303360]\n",
      "[     0 303616]\n",
      "[     0 303872]\n",
      "[     0 304128]\n",
      "[     0 304384]\n",
      "[     0 304640]\n",
      "[     0 304896]\n",
      "[     0 305152]\n",
      "[     0 305408]\n",
      "[     0 305664]\n",
      "[     0 305920]\n",
      "[     0 306176]\n",
      "[     0 306432]\n",
      "[     0 306688]\n",
      "[     0 306944]\n",
      "[     0 307200]\n",
      "[     0 307456]\n",
      "[     0 307712]\n",
      "[     0 307968]\n",
      "[     0 308224]\n",
      "[     0 308480]\n",
      "[     0 308736]\n",
      "[     0 308992]\n",
      "[     0 309248]\n",
      "[     0 309504]\n",
      "[     0 309760]\n",
      "[     0 310016]\n",
      "[     0 310272]\n",
      "[     0 310528]\n",
      "[     0 310784]\n",
      "[     0 311040]\n",
      "[     0 311296]\n",
      "[     0 311552]\n",
      "[     0 311808]\n",
      "[     0 312064]\n",
      "[     0 312320]\n",
      "[     0 312576]\n",
      "[     0 312832]\n",
      "[     0 313088]\n",
      "[     0 313344]\n",
      "[     0 313600]\n",
      "[     0 313856]\n",
      "[     0 314112]\n",
      "[     0 314368]\n",
      "[     0 314624]\n",
      "[     0 314880]\n",
      "[     0 315136]\n",
      "[     0 315392]\n",
      "[     0 315648]\n",
      "[     0 315904]\n",
      "[     0 316160]\n",
      "[     0 316416]\n",
      "[     0 316672]\n",
      "[     0 316928]\n",
      "[     0 317184]\n",
      "[     0 317440]\n",
      "[     0 317696]\n",
      "[     0 317952]\n",
      "[     0 318208]\n",
      "[     0 318464]\n",
      "[     0 318720]\n",
      "[     0 318976]\n",
      "[     0 319232]\n",
      "[     0 319488]\n",
      "[     0 319744]\n",
      "[     0 320000]\n",
      "[     0 320256]\n",
      "[     0 320512]\n",
      "[     0 320768]\n",
      "[     0 321024]\n",
      "[     0 321280]\n",
      "[     0 321536]\n",
      "[     0 321792]\n",
      "[     0 322048]\n",
      "[     0 322304]\n",
      "[     0 322560]\n",
      "[     0 322816]\n",
      "[     0 323072]\n",
      "[     0 323328]\n",
      "[     0 323584]\n",
      "[     0 323840]\n",
      "[     0 324096]\n",
      "[     0 324352]\n",
      "[     0 324608]\n",
      "[     0 324864]\n",
      "[     0 325120]\n",
      "[     0 325376]\n",
      "[     0 325632]\n",
      "[     0 325888]\n",
      "[     0 326144]\n",
      "[     0 326400]\n",
      "[     0 326656]\n",
      "[     0 326912]\n",
      "[     0 327168]\n",
      "[     0 327424]\n",
      "[     0 327680]\n",
      "[     0 327936]\n",
      "[     0 328192]\n",
      "[     0 328448]\n",
      "[     0 328704]\n",
      "[     0 328960]\n",
      "[     0 329216]\n",
      "[     0 329472]\n",
      "[     0 329728]\n",
      "[     0 329984]\n",
      "[     0 330240]\n",
      "[     0 330496]\n",
      "[     0 330752]\n",
      "[     0 331008]\n",
      "[     0 331264]\n",
      "[     0 331520]\n",
      "[     0 331776]\n",
      "[     0 332032]\n",
      "[     0 332288]\n",
      "[     0 332544]\n",
      "[     0 332800]\n",
      "[     0 333056]\n",
      "[     0 333312]\n",
      "[     0 333568]\n",
      "[     0 333824]\n",
      "[     0 334080]\n",
      "[     0 334336]\n",
      "[     0 334592]\n",
      "[     0 334848]\n",
      "[     0 335104]\n",
      "[     0 335360]\n",
      "[     0 335616]\n",
      "[     0 335872]\n",
      "[     0 336128]\n",
      "[     0 336384]\n",
      "[     0 336640]\n",
      "[     0 336896]\n",
      "[     0 337152]\n",
      "[     0 337408]\n",
      "[     0 337664]\n",
      "[     0 337920]\n",
      "[     0 338176]\n",
      "[     0 338432]\n",
      "[     0 338688]\n",
      "[     0 338944]\n",
      "[     0 339200]\n",
      "[     0 339456]\n",
      "[     0 339712]\n",
      "[     0 339968]\n",
      "[     0 340224]\n",
      "[     0 340480]\n",
      "[     0 340736]\n",
      "[     0 340992]\n",
      "[     0 341248]\n",
      "[     0 341504]\n",
      "[     0 341760]\n",
      "[     0 342016]\n",
      "[     0 342272]\n",
      "[     0 342528]\n",
      "[     0 342784]\n",
      "[     0 343040]\n",
      "[     0 343296]\n",
      "[     0 343552]\n",
      "[     0 343808]\n",
      "[     0 344064]\n",
      "[     0 344320]\n",
      "[     0 344576]\n",
      "[     0 344832]\n",
      "[     0 345088]\n",
      "[     0 345344]\n",
      "[     0 345600]\n",
      "[     0 345856]\n",
      "[     0 346112]\n",
      "[     0 346368]\n",
      "[     0 346624]\n",
      "[     0 346880]\n",
      "[     0 347136]\n",
      "[     0 347392]\n",
      "[     0 347648]\n",
      "[     0 347904]\n",
      "[     0 348160]\n",
      "[     0 348416]\n",
      "[     0 348672]\n",
      "[     0 348928]\n",
      "[     0 349184]\n",
      "[     0 349440]\n",
      "[     0 349696]\n",
      "[     0 349952]\n",
      "[     0 350208]\n",
      "[     0 350464]\n",
      "[     0 350720]\n",
      "[     0 350976]\n",
      "[     0 351232]\n",
      "[     0 351488]\n",
      "[     0 351744]\n",
      "[     0 352000]\n",
      "[     0 352256]\n",
      "[     0 352512]\n",
      "[     0 352768]\n",
      "[     0 353024]\n",
      "[     0 353280]\n",
      "[     0 353536]\n",
      "[     0 353792]\n",
      "[     0 354048]\n",
      "[     0 354304]\n",
      "[     0 354560]\n",
      "[     0 354816]\n",
      "[     0 355072]\n",
      "[     0 355328]\n",
      "[     0 355584]\n",
      "[     0 355840]\n",
      "[     0 356096]\n",
      "[     0 356352]\n",
      "[     0 356608]\n",
      "[     0 356864]\n",
      "[     0 357120]\n",
      "[     0 357376]\n",
      "[     0 357632]\n",
      "[     0 357888]\n",
      "[     0 358144]\n",
      "[     0 358400]\n",
      "[     0 358656]\n",
      "[     0 358912]\n",
      "[     0 359168]\n",
      "[     0 359424]\n",
      "[     0 359680]\n",
      "[     0 359936]\n",
      "[     0 360192]\n",
      "[     0 360448]\n",
      "[     0 360704]\n",
      "[     0 360960]\n",
      "[     0 361216]\n",
      "[     0 361472]\n",
      "[     0 361728]\n",
      "[     0 361984]\n",
      "[     0 362240]\n",
      "[     0 362496]\n",
      "[     0 362752]\n",
      "[     0 363008]\n",
      "[     0 363264]\n",
      "[     0 363520]\n",
      "[     0 363776]\n",
      "[     0 364032]\n",
      "[     0 364288]\n",
      "[     0 364544]\n",
      "[     0 364800]\n",
      "[     0 365056]\n",
      "[     0 365312]\n",
      "[     0 365568]\n",
      "[     0 365824]\n",
      "[     0 366080]\n",
      "[     0 366336]\n",
      "[     0 366592]\n",
      "[     0 366848]\n",
      "[     0 367104]\n",
      "[     0 367360]\n",
      "[     0 367616]\n",
      "[     0 367872]\n",
      "[     0 368128]\n",
      "[     0 368384]\n",
      "[     0 368640]\n",
      "[     0 368896]\n",
      "[     0 369152]\n",
      "[     0 369408]\n",
      "[     0 369664]\n",
      "[     0 369920]\n",
      "[     0 370176]\n",
      "[     0 370432]\n",
      "[     0 370688]\n",
      "[     0 370944]\n",
      "[     0 371200]\n",
      "[     0 371456]\n",
      "[     0 371712]\n",
      "[     0 371968]\n",
      "[     0 372224]\n",
      "[     0 372480]\n",
      "[     0 372736]\n",
      "[     0 372992]\n",
      "[     0 373248]\n",
      "[     0 373504]\n",
      "[     0 373760]\n",
      "[     0 374016]\n",
      "[     0 374272]\n",
      "[     0 374528]\n",
      "[     0 374784]\n",
      "[     0 375040]\n",
      "[     0 375296]\n",
      "[     0 375552]\n",
      "[     0 375808]\n",
      "[     0 376064]\n",
      "[     0 376320]\n",
      "[     0 376576]\n",
      "[     0 376832]\n",
      "[     0 377088]\n",
      "[     0 377344]\n",
      "[     0 377600]\n",
      "[     0 377856]\n",
      "[     0 378112]\n",
      "[     0 378368]\n",
      "[     0 378624]\n",
      "[     0 378880]\n",
      "[     0 379136]\n",
      "[     0 379392]\n",
      "[     0 379648]\n",
      "[     0 379904]\n",
      "[     0 380160]\n",
      "[     0 380416]\n",
      "[     0 380672]\n",
      "[     0 380928]\n",
      "[     0 381184]\n",
      "[     0 381440]\n",
      "[     0 381696]\n",
      "[     0 381952]\n",
      "[     0 382208]\n",
      "[     0 382464]\n",
      "[     0 382720]\n",
      "[     0 382976]\n",
      "[     0 383232]\n",
      "[     0 383488]\n",
      "[     0 383744]\n",
      "[     0 384000]\n",
      "[     0 384256]\n",
      "[     0 384512]\n",
      "[     0 384768]\n",
      "[     0 385024]\n",
      "[     0 385280]\n",
      "[     0 385536]\n",
      "[     0 385792]\n",
      "[     0 386048]\n",
      "[     0 386304]\n",
      "[     0 386560]\n",
      "[     0 386816]\n",
      "[     0 387072]\n",
      "[     0 387328]\n",
      "[     0 387584]\n",
      "[     0 387840]\n",
      "[     0 388096]\n",
      "[     0 388352]\n",
      "[     0 388608]\n",
      "[     0 388864]\n",
      "[     0 389120]\n",
      "[     0 389376]\n",
      "[     0 389632]\n",
      "[     0 389888]\n",
      "[     0 390144]\n",
      "[     0 390400]\n",
      "[     0 390656]\n",
      "[     0 390912]\n",
      "[     0 391168]\n",
      "[     0 391424]\n",
      "[     0 391680]\n",
      "[     0 391936]\n",
      "[     0 392192]\n",
      "[     0 392448]\n",
      "[     0 392704]\n",
      "[     0 392960]\n",
      "[     0 393216]\n",
      "[     0 393472]\n",
      "[     0 393728]\n",
      "[     0 393984]\n",
      "[     0 394240]\n",
      "[     0 394496]\n",
      "[     0 394752]\n",
      "[     0 395008]\n",
      "[     0 395264]\n",
      "[     0 395520]\n",
      "[     0 395776]\n",
      "[     0 396032]\n",
      "[     0 396288]\n",
      "[     0 396544]\n",
      "[     0 396800]\n",
      "[     0 397056]\n",
      "[     0 397312]\n",
      "[     0 397568]\n",
      "[     0 397824]\n",
      "[     0 398080]\n",
      "[     0 398336]\n",
      "[     0 398592]\n",
      "[     0 398848]\n",
      "[     0 399104]\n",
      "[     0 399360]\n",
      "[     0 399616]\n",
      "[     0 399872]\n",
      "[     0 400128]\n",
      "[     0 400384]\n",
      "[     0 400640]\n",
      "[     0 400896]\n",
      "[     0 401152]\n",
      "[     0 401408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 401664]\n",
      "[     0 401920]\n",
      "[     0 402176]\n",
      "[     0 402432]\n",
      "[     0 402688]\n",
      "[     0 402944]\n",
      "[     0 403200]\n",
      "[     0 403456]\n",
      "[     0 403712]\n",
      "[     0 403968]\n",
      "[     0 404224]\n",
      "[     0 404480]\n",
      "[     0 404736]\n",
      "[     0 404992]\n",
      "[     0 405248]\n",
      "[     0 405504]\n",
      "[     0 405760]\n",
      "[     0 406016]\n",
      "[     0 406272]\n",
      "[     0 406528]\n",
      "[     0 406784]\n",
      "[     0 407040]\n",
      "[     0 407296]\n",
      "[     0 407552]\n",
      "[     0 407808]\n",
      "[     0 408064]\n",
      "[     0 408320]\n",
      "[     0 408576]\n",
      "[     0 408832]\n",
      "[     0 409088]\n",
      "[     0 409344]\n",
      "[     0 409600]\n",
      "[     0 409856]\n",
      "[     0 410112]\n",
      "[     0 410368]\n",
      "[     0 410624]\n",
      "[     0 410880]\n",
      "[     0 411136]\n",
      "[     0 411392]\n",
      "[     0 411648]\n",
      "[     0 411904]\n",
      "[     0 412160]\n",
      "[     0 412416]\n",
      "[     0 412672]\n",
      "[     0 412928]\n",
      "[     0 413184]\n",
      "[     0 413440]\n",
      "[     0 413696]\n",
      "[     0 413952]\n",
      "[     0 414208]\n",
      "[     0 414464]\n",
      "[     0 414720]\n",
      "[     0 414976]\n",
      "[     0 415232]\n",
      "[     0 415488]\n",
      "[     0 415744]\n",
      "[     0 416000]\n",
      "[     0 416256]\n",
      "[     0 416512]\n",
      "[     0 416768]\n",
      "[     0 417024]\n",
      "[     0 417280]\n",
      "[     0 417536]\n",
      "[     0 417792]\n",
      "[     0 418048]\n",
      "[     0 418304]\n",
      "[     0 418560]\n",
      "[     0 418816]\n",
      "[     0 419072]\n",
      "[     0 419328]\n",
      "[     0 419584]\n",
      "[     0 419840]\n",
      "[     0 420096]\n",
      "[     0 420352]\n",
      "[     0 420608]\n",
      "[     0 420864]\n",
      "[     0 421120]\n",
      "[     0 421376]\n",
      "[     0 421632]\n",
      "[     0 421888]\n",
      "[     0 422144]\n",
      "[     0 422400]\n",
      "[     0 422656]\n",
      "[     0 422912]\n",
      "[     0 423168]\n",
      "[     0 423424]\n",
      "[     0 423680]\n",
      "[     0 423936]\n",
      "[     0 424192]\n",
      "[     0 424448]\n",
      "[     0 424704]\n",
      "[     0 424960]\n",
      "[     0 425216]\n",
      "[     0 425472]\n",
      "[     0 425728]\n",
      "[     0 425984]\n",
      "[     0 426240]\n",
      "[     0 426496]\n",
      "[     0 426752]\n",
      "[     0 427008]\n",
      "[     0 427264]\n",
      "[     0 427520]\n",
      "[     0 427776]\n",
      "[     0 428032]\n",
      "[     0 428288]\n",
      "[     0 428544]\n",
      "[     0 428800]\n",
      "[     0 429056]\n",
      "[     0 429312]\n",
      "[     0 429568]\n",
      "[     0 429824]\n",
      "[     0 430080]\n",
      "[     0 430336]\n",
      "[     0 430592]\n",
      "[     0 430848]\n",
      "[     0 431104]\n",
      "[     0 431360]\n",
      "[     0 431616]\n",
      "[     0 431872]\n",
      "[     0 432128]\n",
      "[     0 432384]\n",
      "[     0 432640]\n",
      "[     0 432896]\n",
      "[     0 433152]\n",
      "[     0 433408]\n",
      "[     0 433664]\n",
      "[     0 433920]\n",
      "[     0 434176]\n",
      "[     0 434432]\n",
      "[     0 434688]\n",
      "[     0 434944]\n",
      "[     0 435200]\n",
      "[     0 435456]\n",
      "[     0 435712]\n",
      "[     0 435968]\n",
      "[     0 436224]\n",
      "[     0 436480]\n",
      "[     0 436736]\n",
      "[     0 436992]\n",
      "[     0 437248]\n",
      "[     0 437504]\n",
      "[     0 437760]\n",
      "[     0 438016]\n",
      "[     0 438272]\n",
      "[     0 438528]\n",
      "[     0 438784]\n",
      "[     0 439040]\n",
      "[     0 439296]\n",
      "[     0 439552]\n",
      "[     0 439808]\n",
      "[     0 440064]\n",
      "[     0 440320]\n",
      "[     0 440576]\n",
      "[     0 440832]\n",
      "[     0 441088]\n",
      "[     0 441344]\n",
      "[     0 441600]\n",
      "[     0 441856]\n",
      "[     0 442112]\n",
      "[     0 442368]\n",
      "[     0 442624]\n",
      "[     0 442880]\n",
      "[     0 443136]\n",
      "[     0 443392]\n",
      "[     0 443648]\n",
      "[     0 443904]\n",
      "[     0 444160]\n",
      "[     0 444416]\n",
      "[     0 444672]\n",
      "[     0 444928]\n",
      "[     0 445184]\n",
      "[     0 445440]\n",
      "[     0 445696]\n",
      "[     0 445952]\n",
      "[     0 446208]\n",
      "[     0 446464]\n",
      "[     0 446720]\n",
      "[     0 446976]\n",
      "[     0 447232]\n",
      "[     0 447488]\n",
      "[     0 447744]\n",
      "[     0 448000]\n",
      "[     0 448256]\n",
      "[     0 448512]\n",
      "[     0 448768]\n",
      "[     0 449024]\n",
      "[     0 449280]\n",
      "[     0 449536]\n",
      "[     0 449792]\n",
      "[     0 450048]\n",
      "[     0 450304]\n",
      "[     0 450560]\n",
      "[     0 450816]\n",
      "[     0 451072]\n",
      "[     0 451328]\n",
      "[     0 451584]\n",
      "[     0 451840]\n",
      "[     0 452096]\n",
      "[     0 452352]\n",
      "[     0 452608]\n",
      "[     0 452864]\n",
      "[     0 453120]\n",
      "[     0 453376]\n",
      "[     0 453632]\n",
      "[     0 453888]\n",
      "[     0 454144]\n",
      "[     0 454400]\n",
      "[     0 454656]\n",
      "[     0 454912]\n",
      "[     0 455168]\n",
      "[     0 455424]\n",
      "[     0 455680]\n",
      "[     0 455936]\n",
      "[     0 456192]\n",
      "[     0 456448]\n",
      "[     0 456704]\n",
      "[     0 456960]\n",
      "[     0 457216]\n",
      "[     0 457472]\n",
      "[     0 457728]\n",
      "[     0 457984]\n",
      "[     0 458240]\n",
      "[     0 458496]\n",
      "[     0 458752]\n",
      "[     0 459008]\n",
      "[     0 459264]\n",
      "[     0 459520]\n",
      "[     0 459776]\n",
      "[     0 460032]\n",
      "[     0 460288]\n",
      "[     0 460544]\n",
      "[     0 460800]\n",
      "[     0 461056]\n",
      "[     0 461312]\n",
      "[     0 461568]\n",
      "[     0 461824]\n",
      "[     0 462080]\n",
      "[     0 462336]\n",
      "[     0 462592]\n",
      "[     0 462848]\n",
      "[     0 463104]\n",
      "[     0 463360]\n",
      "[     0 463616]\n",
      "[     0 463872]\n",
      "[     0 464128]\n",
      "[     0 464384]\n",
      "[     0 464640]\n",
      "[     0 464896]\n",
      "[     0 465152]\n",
      "[     0 465408]\n",
      "[     0 465664]\n",
      "[     0 465920]\n",
      "[     0 466176]\n",
      "[     0 466432]\n",
      "[     0 466688]\n",
      "[     0 466944]\n",
      "[     0 467200]\n",
      "[     0 467456]\n",
      "[     0 467712]\n",
      "[     0 467968]\n",
      "[     0 468224]\n",
      "[     0 468480]\n",
      "[     0 468736]\n",
      "[     0 468992]\n",
      "[     0 469248]\n",
      "[     0 469504]\n",
      "[     0 469760]\n",
      "[     0 470016]\n",
      "[     0 470272]\n",
      "[     0 470528]\n",
      "[     0 470784]\n",
      "[     0 471040]\n",
      "[     0 471296]\n",
      "[     0 471552]\n",
      "[     0 471808]\n",
      "[     0 472064]\n",
      "[     0 472320]\n",
      "[     0 472576]\n",
      "[     0 472832]\n",
      "[     0 473088]\n",
      "[     0 473344]\n",
      "[     0 473600]\n",
      "[     0 473856]\n",
      "[     0 474112]\n",
      "[     0 474368]\n",
      "[     0 474624]\n",
      "[     0 474880]\n",
      "[     0 475136]\n",
      "[     0 475392]\n",
      "[     0 475648]\n",
      "[     0 475904]\n",
      "[     0 476160]\n",
      "[     0 476416]\n",
      "[     0 476672]\n",
      "[     0 476928]\n",
      "[     0 477184]\n",
      "[     0 477440]\n",
      "[     0 477696]\n",
      "[     0 477952]\n",
      "[     0 478208]\n",
      "[     0 478464]\n",
      "[     0 478720]\n",
      "[     0 478976]\n",
      "[     0 479232]\n",
      "[     0 479488]\n",
      "[     0 479744]\n",
      "[     0 480000]\n",
      "[     0 480256]\n",
      "[     0 480512]\n",
      "[     0 480768]\n",
      "[     0 481024]\n",
      "[     0 481280]\n",
      "[     0 481536]\n",
      "[     0 481792]\n",
      "[     0 482048]\n",
      "[     0 482304]\n",
      "[     0 482560]\n",
      "[     0 482816]\n",
      "[     0 483072]\n",
      "[     0 483328]\n",
      "[     0 483584]\n",
      "[     0 483840]\n",
      "[     0 484096]\n",
      "[     0 484352]\n",
      "[     0 484608]\n",
      "[     0 484864]\n",
      "[     0 485120]\n",
      "[     0 485376]\n",
      "[     0 485632]\n",
      "[     0 485888]\n",
      "[     0 486144]\n",
      "[     0 486400]\n",
      "[     0 486656]\n",
      "[     0 486912]\n",
      "[     0 487168]\n",
      "[     0 487424]\n",
      "[     0 487680]\n",
      "[     0 487936]\n",
      "[     0 488192]\n",
      "[     0 488448]\n",
      "[     0 488704]\n",
      "[     0 488960]\n",
      "[     0 489216]\n",
      "[     0 489472]\n",
      "[     0 489728]\n",
      "[     0 489984]\n",
      "[     0 490240]\n",
      "[     0 490496]\n",
      "[     0 490752]\n",
      "[     0 491008]\n",
      "[     0 491264]\n",
      "[     0 491520]\n",
      "[     0 491776]\n",
      "[     0 492032]\n",
      "[     0 492288]\n",
      "[     0 492544]\n",
      "[     0 492800]\n",
      "[     0 493056]\n",
      "[     0 493312]\n",
      "[     0 493568]\n",
      "[     0 493824]\n",
      "[     0 494080]\n",
      "[     0 494336]\n",
      "[     0 494592]\n",
      "[     0 494848]\n",
      "[     0 495104]\n",
      "[     0 495360]\n",
      "[     0 495616]\n",
      "[     0 495872]\n",
      "[     0 496128]\n",
      "[     0 496384]\n",
      "[     0 496640]\n",
      "[     0 496896]\n",
      "[     0 497152]\n",
      "[     0 497408]\n",
      "[     0 497664]\n",
      "[     0 497920]\n",
      "[     0 498176]\n",
      "[     0 498432]\n",
      "[     0 498688]\n",
      "[     0 498944]\n",
      "[     0 499200]\n",
      "[     0 499456]\n",
      "[     0 499712]\n",
      "[     0 499968]\n",
      "[     0 500224]\n",
      "[     0 500480]\n",
      "[     0 500736]\n",
      "[     0 500992]\n",
      "[     0 501248]\n",
      "[     0 501504]\n",
      "[     0 501760]\n",
      "[     0 502016]\n",
      "[     0 502272]\n",
      "[     0 502528]\n",
      "[     0 502784]\n",
      "[     0 503040]\n",
      "[     0 503296]\n",
      "[     0 503552]\n",
      "[     0 503808]\n",
      "[     0 504064]\n",
      "[     0 504320]\n",
      "[     0 504576]\n",
      "[     0 504832]\n",
      "[     0 505088]\n",
      "[     0 505344]\n",
      "[     0 505600]\n",
      "[     0 505856]\n",
      "[     0 506112]\n",
      "[     0 506368]\n",
      "[     0 506624]\n",
      "[     0 506880]\n",
      "[     0 507136]\n",
      "[     0 507392]\n",
      "[     0 507648]\n",
      "[     0 507904]\n",
      "[     0 508160]\n",
      "[     0 508416]\n",
      "[     0 508672]\n",
      "[     0 508928]\n",
      "[     0 509184]\n",
      "[     0 509440]\n",
      "[     0 509696]\n",
      "[     0 509952]\n",
      "[     0 510208]\n",
      "[     0 510464]\n",
      "[     0 510720]\n",
      "[     0 510976]\n",
      "[     0 511232]\n",
      "[     0 511488]\n",
      "[     0 511744]\n",
      "[     0 512000]\n",
      "[     0 512256]\n",
      "[     0 512512]\n",
      "[     0 512768]\n",
      "[     0 513024]\n",
      "[     0 513280]\n",
      "[     0 513536]\n",
      "[     0 513792]\n",
      "[     0 514048]\n",
      "[     0 514304]\n",
      "[     0 514560]\n",
      "[     0 514816]\n",
      "[     0 515072]\n",
      "[     0 515328]\n",
      "[     0 515584]\n",
      "[     0 515840]\n",
      "[     0 516096]\n",
      "[     0 516352]\n",
      "[     0 516608]\n",
      "[     0 516864]\n",
      "[     0 517120]\n",
      "[     0 517376]\n",
      "[     0 517632]\n",
      "[     0 517888]\n",
      "[     0 518144]\n",
      "[     0 518400]\n",
      "[     0 518656]\n",
      "[     0 518912]\n",
      "[     0 519168]\n",
      "[     0 519424]\n",
      "[     0 519680]\n",
      "[     0 519936]\n",
      "[     0 520192]\n",
      "[     0 520448]\n",
      "[     0 520704]\n",
      "[     0 520960]\n",
      "[     0 521216]\n",
      "[     0 521472]\n",
      "[     0 521728]\n",
      "[     0 521984]\n",
      "[     0 522240]\n",
      "[     0 522496]\n",
      "[     0 522752]\n",
      "[     0 523008]\n",
      "[     0 523264]\n",
      "[     0 523520]\n",
      "[     0 523776]\n",
      "[     0 524032]\n",
      "[     0 524288]\n",
      "[     0 524544]\n",
      "[     0 524800]\n",
      "[     0 525056]\n",
      "[     0 525312]\n",
      "[     0 525568]\n",
      "[     0 525824]\n",
      "[     0 526080]\n",
      "[     0 526336]\n",
      "[     0 526592]\n",
      "[     0 526848]\n",
      "[     0 527104]\n",
      "[     0 527360]\n",
      "[     0 527616]\n",
      "[     0 527872]\n",
      "[     0 528128]\n",
      "[     0 528384]\n",
      "[     0 528640]\n",
      "[     0 528896]\n",
      "[     0 529152]\n",
      "[     0 529408]\n",
      "[     0 529664]\n",
      "[     0 529920]\n",
      "[     0 530176]\n",
      "[     0 530432]\n",
      "[     0 530688]\n",
      "[     0 530944]\n",
      "[     0 531200]\n",
      "[     0 531456]\n",
      "[     0 531712]\n",
      "[     0 531968]\n",
      "[     0 532224]\n",
      "[     0 532480]\n",
      "[     0 532736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 532992]\n",
      "[     0 533248]\n",
      "[     0 533504]\n",
      "[     0 533760]\n",
      "[     0 534016]\n",
      "[     0 534272]\n",
      "[     0 534528]\n",
      "[     0 534784]\n",
      "[     0 535040]\n",
      "[     0 535296]\n",
      "[     0 535552]\n",
      "[     0 535808]\n",
      "[     0 536064]\n",
      "[     0 536320]\n",
      "[     0 536576]\n",
      "[     0 536832]\n",
      "[     0 537088]\n",
      "[     0 537344]\n",
      "[     0 537600]\n",
      "[     0 537856]\n",
      "[     0 538112]\n",
      "[     0 538368]\n",
      "[     0 538624]\n",
      "[     0 538880]\n",
      "[     0 539136]\n",
      "[     0 539392]\n",
      "[     0 539648]\n",
      "[     0 539904]\n",
      "[     0 540160]\n",
      "[     0 540416]\n",
      "[     0 540672]\n",
      "[     0 540928]\n",
      "[     0 541184]\n",
      "[     0 541440]\n",
      "[     0 541696]\n",
      "[     0 541952]\n",
      "[     0 542208]\n",
      "[     0 542464]\n",
      "[     0 542720]\n",
      "[     0 542976]\n",
      "[     0 543232]\n",
      "[     0 543488]\n",
      "[     0 543744]\n",
      "[     0 544000]\n",
      "[     0 544256]\n",
      "[     0 544512]\n",
      "[     0 544768]\n",
      "[     0 545024]\n",
      "[     0 545280]\n",
      "[     0 545536]\n",
      "[     0 545792]\n",
      "[     0 546048]\n",
      "[     0 546304]\n",
      "[     0 546560]\n",
      "[     0 546816]\n",
      "[     0 547072]\n",
      "[     0 547328]\n",
      "[     0 547584]\n",
      "[     0 547840]\n",
      "[     0 548096]\n",
      "[     0 548352]\n",
      "[     0 548608]\n",
      "[     0 548864]\n",
      "[     0 549120]\n",
      "[     0 549376]\n",
      "[     0 549632]\n",
      "[     0 549888]\n",
      "[     0 550144]\n",
      "[     0 550400]\n",
      "[     0 550656]\n",
      "[     0 550912]\n",
      "[     0 551168]\n",
      "[     0 551424]\n",
      "[     0 551680]\n",
      "[     0 551936]\n",
      "[     0 552192]\n",
      "[     0 552448]\n",
      "[     0 552704]\n",
      "[     0 552960]\n",
      "[     0 553216]\n",
      "[     0 553472]\n",
      "[     0 553728]\n",
      "[     0 553984]\n",
      "[     0 554240]\n",
      "[     0 554496]\n",
      "[     0 554752]\n",
      "[     0 555008]\n",
      "[     0 555264]\n",
      "[     0 555520]\n",
      "[     0 555776]\n",
      "[     0 556032]\n",
      "[     0 556288]\n",
      "[     0 556544]\n",
      "[     0 556800]\n",
      "[     0 557056]\n",
      "[     0 557312]\n",
      "[     0 557568]\n",
      "[     0 557824]\n",
      "[     0 558080]\n",
      "[     0 558336]\n",
      "[     0 558592]\n",
      "[     0 558848]\n",
      "[     0 559104]\n",
      "[     0 559360]\n",
      "[     0 559616]\n",
      "[     0 559872]\n",
      "[     0 560128]\n",
      "[     0 560384]\n",
      "[     0 560640]\n",
      "[     0 560896]\n",
      "[     0 561152]\n",
      "[     0 561408]\n",
      "[     0 561664]\n",
      "[     0 561920]\n",
      "[     0 562176]\n",
      "[     0 562432]\n",
      "[     0 562688]\n",
      "[     0 562944]\n",
      "[     0 563200]\n",
      "[     0 563456]\n",
      "[     0 563712]\n",
      "[     0 563968]\n",
      "[     0 564224]\n",
      "[     0 564480]\n",
      "[     0 564736]\n",
      "[     0 564992]\n",
      "[     0 565248]\n",
      "[     0 565504]\n",
      "[     0 565760]\n",
      "[     0 566016]\n",
      "[     0 566272]\n",
      "[     0 566528]\n",
      "[     0 566784]\n",
      "[     0 567040]\n",
      "[     0 567296]\n",
      "[     0 567552]\n",
      "[     0 567808]\n",
      "[     0 568064]\n",
      "[     0 568320]\n",
      "[     0 568576]\n",
      "[     0 568832]\n",
      "[     0 569088]\n",
      "[     0 569344]\n",
      "[     0 569600]\n",
      "[     0 569856]\n",
      "[     0 570112]\n",
      "[     0 570368]\n",
      "[     0 570624]\n",
      "[     0 570880]\n",
      "[     0 571136]\n",
      "[     0 571392]\n",
      "[     0 571648]\n",
      "[     0 571904]\n",
      "[     0 572160]\n",
      "[     0 572416]\n",
      "[     0 572672]\n",
      "[     0 572928]\n",
      "[     0 573184]\n",
      "[     0 573440]\n",
      "[     0 573696]\n",
      "[     0 573952]\n",
      "[     0 574208]\n",
      "[     0 574464]\n",
      "[     0 574720]\n",
      "[     0 574976]\n",
      "[     0 575232]\n",
      "[     0 575488]\n",
      "[     0 575744]\n",
      "[     0 576000]\n",
      "[     0 576256]\n",
      "[     0 576512]\n",
      "[     0 576768]\n",
      "[     0 577024]\n",
      "[     0 577280]\n",
      "[     0 577536]\n",
      "[     0 577792]\n",
      "[     0 578048]\n",
      "[     0 578304]\n",
      "[     0 578560]\n",
      "[     0 578816]\n",
      "[     0 579072]\n",
      "[     0 579328]\n",
      "[     0 579584]\n",
      "[     0 579840]\n",
      "[     0 580096]\n",
      "[     0 580352]\n",
      "[     0 580608]\n",
      "[     0 580864]\n",
      "[     0 581120]\n",
      "[     0 581376]\n",
      "[     0 581632]\n",
      "[     0 581888]\n",
      "[     0 582144]\n",
      "[     0 582400]\n",
      "[     0 582656]\n",
      "[     0 582912]\n",
      "[     0 583168]\n",
      "[     0 583424]\n",
      "[     0 583680]\n",
      "[     0 583936]\n",
      "[     0 584192]\n",
      "[     0 584448]\n",
      "[     0 584704]\n",
      "[     0 584960]\n",
      "[     0 585216]\n",
      "[     0 585472]\n",
      "[     0 585728]\n",
      "[     0 585984]\n",
      "[     0 586240]\n",
      "[     0 586496]\n",
      "[     0 586752]\n",
      "[     0 587008]\n",
      "[     0 587264]\n",
      "[     0 587520]\n",
      "[     0 587776]\n",
      "[     0 588032]\n",
      "[     0 588288]\n",
      "[     0 588544]\n",
      "[     0 588800]\n",
      "[     0 589056]\n",
      "[     0 589312]\n",
      "[     0 589568]\n",
      "[     0 589824]\n",
      "[     0 590080]\n",
      "[     0 590336]\n",
      "[     0 590592]\n",
      "[     0 590848]\n",
      "[     0 591104]\n",
      "[     0 591360]\n",
      "[     0 591616]\n",
      "[     0 591872]\n",
      "[     0 592128]\n",
      "[     0 592384]\n",
      "[     0 592640]\n",
      "[     0 592896]\n",
      "[     0 593152]\n",
      "[     0 593408]\n",
      "[     0 593664]\n",
      "[     0 593920]\n",
      "[     0 594176]\n",
      "[     0 594432]\n",
      "[     0 594688]\n",
      "[     0 594944]\n",
      "[     0 595200]\n",
      "[     0 595456]\n",
      "[     0 595712]\n",
      "[     0 595968]\n",
      "[     0 596224]\n",
      "[     0 596480]\n",
      "[     0 596736]\n",
      "[     0 596992]\n",
      "[     0 597248]\n",
      "[     0 597504]\n",
      "[     0 597760]\n",
      "[     0 598016]\n",
      "[     0 598272]\n",
      "[     0 598528]\n",
      "[     0 598784]\n",
      "[     0 599040]\n",
      "[     0 599296]\n",
      "[     0 599552]\n",
      "[     0 599808]\n",
      "[     0 600064]\n",
      "[     0 600320]\n",
      "[     0 600576]\n",
      "[     0 600832]\n",
      "[     0 601088]\n",
      "[     0 601344]\n",
      "[     0 601600]\n",
      "[     0 601856]\n",
      "[     0 602112]\n",
      "[     0 602368]\n",
      "[     0 602624]\n",
      "[     0 602880]\n",
      "[     0 603136]\n",
      "[     0 603392]\n",
      "[     0 603648]\n",
      "[     0 603904]\n",
      "[     0 604160]\n",
      "[     0 604416]\n",
      "[     0 604672]\n",
      "[     0 604928]\n",
      "[     0 605184]\n",
      "[     0 605440]\n",
      "[     0 605696]\n",
      "[     0 605952]\n",
      "[     0 606208]\n",
      "[     0 606464]\n",
      "[     0 606720]\n",
      "[     0 606976]\n",
      "[     0 607232]\n",
      "[     0 607488]\n",
      "[     0 607744]\n",
      "[     0 608000]\n",
      "[     0 608256]\n",
      "[     0 608512]\n",
      "[     0 608768]\n",
      "[     0 609024]\n",
      "[     0 609280]\n",
      "[     0 609536]\n",
      "[     0 609792]\n",
      "[     0 610048]\n",
      "[     0 610304]\n",
      "[     0 610560]\n",
      "[     0 610816]\n",
      "[     0 611072]\n",
      "[     0 611328]\n",
      "[     0 611584]\n",
      "[     0 611840]\n",
      "[     0 612096]\n",
      "[     0 612352]\n",
      "[     0 612608]\n",
      "[     0 612864]\n",
      "[     0 613120]\n",
      "[     0 613376]\n",
      "[     0 613632]\n",
      "[     0 613888]\n",
      "[     0 614144]\n",
      "[     0 614400]\n",
      "[     0 614656]\n",
      "[     0 614912]\n",
      "[     0 615168]\n",
      "[     0 615424]\n",
      "[     0 615680]\n",
      "[     0 615936]\n",
      "[     0 616192]\n",
      "[     0 616448]\n",
      "[     0 616704]\n",
      "[     0 616960]\n",
      "[     0 617216]\n",
      "[     0 617472]\n",
      "[     0 617728]\n",
      "[     0 617984]\n",
      "[     0 618240]\n",
      "[     0 618496]\n",
      "[     0 618752]\n",
      "[     0 619008]\n",
      "[     0 619264]\n",
      "[     0 619520]\n",
      "[     0 619776]\n",
      "[     0 620032]\n",
      "[     0 620288]\n",
      "[     0 620544]\n",
      "[     0 620800]\n",
      "[     0 621056]\n",
      "[     0 621312]\n",
      "[     0 621568]\n",
      "[     0 621824]\n",
      "[     0 622080]\n",
      "[     0 622336]\n",
      "[     0 622592]\n",
      "[     0 622848]\n",
      "[     0 623104]\n",
      "[     0 623360]\n",
      "[     0 623616]\n",
      "[     0 623872]\n",
      "[     0 624128]\n",
      "[     0 624384]\n",
      "[     0 624640]\n",
      "[     0 624896]\n",
      "[     0 625152]\n",
      "[     0 625408]\n",
      "[     0 625664]\n",
      "[     0 625920]\n",
      "[     0 626176]\n",
      "[     0 626432]\n",
      "[     0 626688]\n",
      "[     0 626944]\n",
      "[     0 627200]\n",
      "[     0 627456]\n",
      "[     0 627712]\n",
      "[     0 627968]\n",
      "[     0 628224]\n",
      "[     0 628480]\n",
      "[     0 628736]\n",
      "[     0 628992]\n",
      "[     0 629248]\n",
      "[     0 629504]\n",
      "[     0 629760]\n",
      "[     0 630016]\n",
      "[     0 630272]\n",
      "[     0 630528]\n",
      "[     0 630784]\n",
      "[     0 631040]\n",
      "[     0 631296]\n",
      "[     0 631552]\n",
      "[     0 631808]\n",
      "[     0 632064]\n",
      "[     0 632320]\n",
      "[     0 632576]\n",
      "[     0 632832]\n",
      "[     0 633088]\n",
      "[     0 633344]\n",
      "[     0 633600]\n",
      "[     0 633856]\n",
      "[     0 634112]\n",
      "[     0 634368]\n",
      "[     0 634624]\n",
      "[     0 634880]\n",
      "[     0 635136]\n",
      "[     0 635392]\n",
      "[     0 635648]\n",
      "[     0 635904]\n",
      "[     0 636160]\n",
      "[     0 636416]\n",
      "[     0 636672]\n",
      "[     0 636928]\n",
      "[     0 637184]\n",
      "[     0 637440]\n",
      "[     0 637696]\n",
      "[     0 637952]\n",
      "[     0 638208]\n",
      "[     0 638464]\n",
      "[     0 638720]\n",
      "[     0 638976]\n",
      "[     0 639232]\n",
      "[     0 639488]\n",
      "[     0 639744]\n",
      "[     0 640000]\n",
      "[     0 640256]\n",
      "[     0 640512]\n",
      "[     0 640768]\n",
      "[     0 641024]\n",
      "[     0 641280]\n",
      "[     0 641536]\n",
      "[     0 641792]\n",
      "[     0 642048]\n",
      "[     0 642304]\n",
      "[     0 642560]\n",
      "[     0 642816]\n",
      "[     0 643072]\n",
      "[     0 643328]\n",
      "[     0 643584]\n",
      "[     0 643840]\n",
      "[     0 644096]\n",
      "[     0 644352]\n",
      "[     0 644608]\n",
      "[     0 644864]\n",
      "[     0 645120]\n",
      "[     0 645376]\n",
      "[     0 645632]\n",
      "[     0 645888]\n",
      "[     0 646144]\n",
      "[     0 646400]\n",
      "[     0 646656]\n",
      "[     0 646912]\n",
      "[     0 647168]\n",
      "[     0 647424]\n",
      "[     0 647680]\n",
      "[     0 647936]\n",
      "[     0 648192]\n",
      "[     0 648448]\n",
      "[     0 648704]\n",
      "[     0 648960]\n",
      "[     0 649216]\n",
      "[     0 649472]\n",
      "[     0 649728]\n",
      "[     0 649984]\n",
      "[     0 650240]\n",
      "[     0 650496]\n",
      "[     0 650752]\n",
      "[     0 651008]\n",
      "[     0 651264]\n",
      "[     0 651520]\n",
      "[     0 651776]\n",
      "[     0 652032]\n",
      "[     0 652288]\n",
      "[     0 652544]\n",
      "[     0 652800]\n",
      "[     0 653056]\n",
      "[     0 653312]\n",
      "[     0 653568]\n",
      "[     0 653824]\n",
      "[     0 654080]\n",
      "[     0 654336]\n",
      "[     0 654592]\n",
      "[     0 654848]\n",
      "[     0 655104]\n",
      "[     0 655360]\n",
      "[     0 655616]\n",
      "[     0 655872]\n",
      "[     0 656128]\n",
      "[     0 656384]\n",
      "[     0 656640]\n",
      "[     0 656896]\n",
      "[     0 657152]\n",
      "[     0 657408]\n",
      "[     0 657664]\n",
      "[     0 657920]\n",
      "[     0 658176]\n",
      "[     0 658432]\n",
      "[     0 658688]\n",
      "[     0 658944]\n",
      "[     0 659200]\n",
      "[     0 659456]\n",
      "[     0 659712]\n",
      "[     0 659968]\n",
      "[     0 660224]\n",
      "[     0 660480]\n",
      "[     0 660736]\n",
      "[     0 660992]\n",
      "[     0 661248]\n",
      "[     0 661504]\n",
      "[     0 661760]\n",
      "[     0 662016]\n",
      "[     0 662272]\n",
      "[     0 662528]\n",
      "[     0 662784]\n",
      "[     0 663040]\n",
      "[     0 663296]\n",
      "[     0 663552]\n",
      "[     0 663808]\n",
      "[     0 664064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 664320]\n",
      "[     0 664576]\n",
      "[     0 664832]\n",
      "[     0 665088]\n",
      "[     0 665344]\n",
      "[     0 665600]\n",
      "[     0 665856]\n",
      "[     0 666112]\n",
      "[     0 666368]\n",
      "[     0 666624]\n",
      "[     0 666880]\n",
      "[     0 667136]\n",
      "[     0 667392]\n",
      "[     0 667648]\n",
      "[     0 667904]\n",
      "[     0 668160]\n",
      "[     0 668416]\n",
      "[     0 668672]\n",
      "[     0 668928]\n",
      "[     0 669184]\n",
      "[     0 669440]\n",
      "[     0 669696]\n",
      "[     0 669952]\n",
      "[     0 670208]\n",
      "[     0 670464]\n",
      "[     0 670720]\n",
      "[     0 670976]\n",
      "[     0 671232]\n",
      "[     0 671488]\n",
      "[     0 671744]\n",
      "[     0 672000]\n",
      "[     0 672256]\n",
      "[     0 672512]\n",
      "[     0 672768]\n",
      "[     0 673024]\n",
      "[     0 673280]\n",
      "[     0 673536]\n",
      "[     0 673792]\n",
      "[     0 674048]\n",
      "[     0 674304]\n",
      "[     0 674560]\n",
      "[     0 674816]\n",
      "[     0 675072]\n",
      "[     0 675328]\n",
      "[     0 675584]\n",
      "[     0 675840]\n",
      "[     0 676096]\n",
      "[     0 676352]\n",
      "[     0 676608]\n",
      "[     0 676864]\n",
      "[     0 677120]\n",
      "[     0 677376]\n",
      "[     0 677632]\n",
      "[     0 677888]\n",
      "[     0 678144]\n",
      "[     0 678400]\n",
      "[     0 678656]\n",
      "[     0 678912]\n",
      "[     0 679168]\n",
      "[     0 679424]\n",
      "[     0 679680]\n",
      "[     0 679936]\n",
      "[     0 680192]\n",
      "[     0 680448]\n",
      "[     0 680704]\n",
      "[     0 680960]\n",
      "[     0 681216]\n",
      "[     0 681472]\n",
      "[     0 681728]\n",
      "[     0 681984]\n",
      "[     0 682240]\n",
      "[     0 682496]\n",
      "[     0 682752]\n",
      "[     0 683008]\n",
      "[     0 683264]\n",
      "[     0 683520]\n",
      "[     0 683776]\n",
      "[     0 684032]\n",
      "[     0 684288]\n",
      "[     0 684544]\n",
      "[     0 684800]\n",
      "[     0 685056]\n",
      "[     0 685312]\n",
      "[     0 685568]\n",
      "[     0 685824]\n",
      "[     0 686080]\n",
      "[     0 686336]\n",
      "[     0 686592]\n",
      "[     0 686848]\n",
      "[     0 687104]\n",
      "[     0 687360]\n",
      "[     0 687616]\n",
      "[     0 687872]\n",
      "[     0 688128]\n",
      "[     0 688384]\n",
      "[     0 688640]\n",
      "[     0 688896]\n",
      "[     0 689152]\n",
      "[     0 689408]\n",
      "[     0 689664]\n",
      "[     0 689920]\n",
      "[     0 690176]\n",
      "[     0 690432]\n",
      "[     0 690688]\n",
      "[     0 690944]\n",
      "[     0 691200]\n",
      "[     0 691456]\n",
      "[     0 691712]\n",
      "[     0 691968]\n",
      "[     0 692224]\n",
      "[     0 692480]\n",
      "[     0 692736]\n",
      "[     0 692992]\n",
      "[     0 693248]\n",
      "[     0 693504]\n",
      "[     0 693760]\n",
      "[     0 694016]\n",
      "[     0 694272]\n",
      "[     0 694528]\n",
      "[     0 694784]\n",
      "[     0 695040]\n",
      "[     0 695296]\n",
      "[     0 695552]\n",
      "[     0 695808]\n",
      "[     0 696064]\n",
      "[     0 696320]\n",
      "[     0 696576]\n",
      "[     0 696832]\n",
      "[     0 697088]\n",
      "[     0 697344]\n",
      "[     0 697600]\n",
      "[     0 697856]\n",
      "[     0 698112]\n",
      "[     0 698368]\n",
      "[     0 698624]\n",
      "[     0 698880]\n",
      "[     0 699136]\n",
      "[     0 699392]\n",
      "[     0 699648]\n",
      "[     0 699904]\n",
      "[     0 700160]\n",
      "[     0 700416]\n",
      "[     0 700672]\n",
      "[     0 700928]\n",
      "[     0 701184]\n",
      "[     0 701440]\n",
      "[     0 701696]\n",
      "[     0 701952]\n",
      "[     0 702208]\n",
      "[     0 702464]\n",
      "[     0 702720]\n",
      "[     0 702976]\n",
      "[     0 703232]\n",
      "[     0 703488]\n",
      "[     0 703744]\n",
      "[     0 704000]\n",
      "[     0 704256]\n",
      "[     0 704512]\n",
      "[     0 704768]\n",
      "[     0 705024]\n",
      "[     0 705280]\n",
      "[     0 705536]\n",
      "[     0 705792]\n",
      "[     0 706048]\n",
      "[     0 706304]\n",
      "[     0 706560]\n",
      "[     0 706816]\n",
      "[     0 707072]\n",
      "[     0 707328]\n",
      "[     0 707584]\n",
      "[     0 707840]\n",
      "[     0 708096]\n",
      "[     0 708352]\n",
      "[     0 708608]\n",
      "[     0 708864]\n",
      "[     0 709120]\n",
      "[     0 709376]\n",
      "[     0 709632]\n",
      "[     0 709888]\n",
      "[     0 710144]\n",
      "[     0 710400]\n",
      "[     0 710656]\n",
      "[     0 710912]\n",
      "[     0 711168]\n",
      "[     0 711424]\n",
      "[     0 711680]\n",
      "[     0 711936]\n",
      "[     0 712192]\n",
      "[     0 712448]\n",
      "[     0 712704]\n",
      "[     0 712960]\n",
      "[     0 713216]\n",
      "[     0 713472]\n",
      "[     0 713728]\n",
      "[     0 713984]\n",
      "[     0 714240]\n",
      "[     0 714496]\n",
      "[     0 714752]\n",
      "[     0 715008]\n",
      "[     0 715264]\n",
      "[     0 715520]\n",
      "[     0 715776]\n",
      "[     0 716032]\n",
      "[     0 716288]\n",
      "[     0 716544]\n",
      "[     0 716800]\n",
      "[     0 717056]\n",
      "[     0 717312]\n",
      "[     0 717568]\n",
      "[     0 717824]\n",
      "[     0 718080]\n",
      "[     0 718336]\n",
      "[     0 718592]\n",
      "[     0 718848]\n",
      "[     0 719104]\n",
      "[     0 719360]\n",
      "[     0 719616]\n",
      "[     0 719872]\n",
      "[     0 720128]\n",
      "[     0 720384]\n",
      "[     0 720640]\n",
      "[     0 720896]\n",
      "[     0 721152]\n",
      "[     0 721408]\n",
      "[     0 721664]\n",
      "[     0 721920]\n",
      "[     0 722176]\n",
      "[     0 722432]\n",
      "[     0 722688]\n",
      "[     0 722944]\n",
      "[     0 723200]\n",
      "[     0 723456]\n",
      "[     0 723712]\n",
      "[     0 723968]\n",
      "[     0 724224]\n",
      "[     0 724480]\n",
      "[     0 724736]\n",
      "[     0 724992]\n",
      "[     0 725248]\n",
      "[     0 725504]\n",
      "[     0 725760]\n",
      "[     0 726016]\n",
      "[     0 726272]\n",
      "[     0 726528]\n",
      "[     0 726784]\n",
      "[     0 727040]\n",
      "[     0 727296]\n",
      "[     0 727552]\n",
      "[     0 727808]\n",
      "[     0 728064]\n",
      "[     0 728320]\n",
      "[     0 728576]\n",
      "[     0 728832]\n",
      "[     0 729088]\n",
      "[     0 729344]\n",
      "[     0 729600]\n",
      "[     0 729856]\n",
      "[     0 730112]\n",
      "[     0 730368]\n",
      "[     0 730624]\n",
      "[     0 730880]\n",
      "[     0 731136]\n",
      "[     0 731392]\n",
      "[     0 731648]\n",
      "[     0 731904]\n",
      "[     0 732160]\n",
      "[     0 732416]\n",
      "[     0 732672]\n",
      "[     0 732928]\n",
      "[     0 733184]\n",
      "[     0 733440]\n",
      "[     0 733696]\n",
      "[     0 733952]\n",
      "[     0 734208]\n",
      "[     0 734464]\n",
      "[     0 734720]\n",
      "[     0 734976]\n",
      "[     0 735232]\n",
      "[     0 735488]\n",
      "[     0 735744]\n",
      "[     0 736000]\n",
      "[     0 736256]\n",
      "[     0 736512]\n",
      "[     0 736768]\n",
      "[     0 737024]\n",
      "[     0 737280]\n",
      "[     0 737536]\n",
      "[     0 737792]\n",
      "[     0 738048]\n",
      "[     0 738304]\n",
      "[     0 738560]\n",
      "[     0 738816]\n",
      "[     0 739072]\n",
      "[     0 739328]\n",
      "[     0 739584]\n",
      "[     0 739840]\n",
      "[     0 740096]\n",
      "[     0 740352]\n",
      "[     0 740608]\n",
      "[     0 740864]\n",
      "[     0 741120]\n",
      "[     0 741376]\n",
      "[     0 741632]\n",
      "[     0 741888]\n",
      "[     0 742144]\n",
      "[     0 742400]\n",
      "[     0 742656]\n",
      "[     0 742912]\n",
      "[     0 743168]\n",
      "[     0 743424]\n",
      "[     0 743680]\n",
      "[     0 743936]\n",
      "[     0 744192]\n",
      "[     0 744448]\n",
      "[     0 744704]\n",
      "[     0 744960]\n",
      "[     0 745216]\n",
      "[     0 745472]\n",
      "[     0 745728]\n",
      "[     0 745984]\n",
      "[     0 746240]\n",
      "[     0 746496]\n",
      "[     0 746752]\n",
      "[     0 747008]\n",
      "[     0 747264]\n",
      "[     0 747520]\n",
      "[     0 747776]\n",
      "[     0 748032]\n",
      "[     0 748288]\n",
      "[     0 748544]\n",
      "[     0 748800]\n",
      "[     0 749056]\n",
      "[     0 749312]\n",
      "[     0 749568]\n",
      "[     0 749824]\n",
      "[     0 750080]\n",
      "[     0 750336]\n",
      "[     0 750592]\n",
      "[     0 750848]\n",
      "[     0 751104]\n",
      "[     0 751360]\n",
      "[     0 751616]\n",
      "[     0 751872]\n",
      "[     0 752128]\n",
      "[     0 752384]\n",
      "[     0 752640]\n",
      "[     0 752896]\n",
      "[     0 753152]\n",
      "[     0 753408]\n",
      "[     0 753664]\n",
      "[     0 753920]\n",
      "[     0 754176]\n",
      "[     0 754432]\n",
      "[     0 754688]\n",
      "[     0 754944]\n",
      "[     0 755200]\n",
      "[     0 755456]\n",
      "[     0 755712]\n",
      "[     0 755968]\n",
      "[     0 756224]\n",
      "[     0 756480]\n",
      "[     0 756736]\n",
      "[     0 756992]\n",
      "[     0 757248]\n",
      "[     0 757504]\n",
      "[     0 757760]\n",
      "[     0 758016]\n",
      "[     0 758272]\n",
      "[     0 758528]\n",
      "[     0 758784]\n",
      "[     0 759040]\n",
      "[     0 759296]\n",
      "[     0 759552]\n",
      "[     0 759808]\n",
      "[     0 760064]\n",
      "[     0 760320]\n",
      "[     0 760576]\n",
      "[     0 760832]\n",
      "[     0 761088]\n",
      "[     0 761344]\n",
      "[     0 761600]\n",
      "[     0 761856]\n",
      "[     0 762112]\n",
      "[     0 762368]\n",
      "[     0 762624]\n",
      "[     0 762880]\n",
      "[     0 763136]\n",
      "[     0 763392]\n",
      "[     0 763648]\n",
      "[     0 763904]\n",
      "[     0 764160]\n",
      "[     0 764416]\n",
      "[     0 764672]\n",
      "[     0 764928]\n",
      "[     0 765184]\n",
      "[     0 765440]\n",
      "[     0 765696]\n",
      "[     0 765952]\n",
      "[     0 766208]\n",
      "[     0 766464]\n",
      "[     0 766720]\n",
      "[     0 766976]\n",
      "[     0 767232]\n",
      "[     0 767488]\n",
      "[     0 767744]\n",
      "[     0 768000]\n",
      "[     0 768256]\n",
      "[     0 768512]\n",
      "[     0 768768]\n",
      "[     0 769024]\n",
      "[     0 769280]\n",
      "[     0 769536]\n",
      "[     0 769792]\n",
      "[     0 770048]\n",
      "[     0 770304]\n",
      "[     0 770560]\n",
      "[     0 770816]\n",
      "[     0 771072]\n",
      "[     0 771328]\n",
      "[     0 771584]\n",
      "[     0 771840]\n",
      "[     0 772096]\n",
      "[     0 772352]\n",
      "[     0 772608]\n",
      "[     0 772864]\n",
      "[     0 773120]\n",
      "[     0 773376]\n",
      "[     0 773632]\n",
      "[     0 773888]\n",
      "[     0 774144]\n",
      "[     0 774400]\n",
      "[     0 774656]\n",
      "[     0 774912]\n",
      "[     0 775168]\n",
      "[     0 775424]\n",
      "[     0 775680]\n",
      "[     0 775936]\n",
      "[     0 776192]\n",
      "[     0 776448]\n",
      "[     0 776704]\n",
      "[     0 776960]\n",
      "[     0 777216]\n",
      "[     0 777472]\n",
      "[     0 777728]\n",
      "[     0 777984]\n",
      "[     0 778240]\n",
      "[     0 778496]\n",
      "[     0 778752]\n",
      "[     0 779008]\n",
      "[     0 779264]\n",
      "[     0 779520]\n",
      "[     0 779776]\n",
      "[     0 780032]\n",
      "[     0 780288]\n",
      "[     0 780544]\n",
      "[     0 780800]\n",
      "[     0 781056]\n",
      "[     0 781312]\n",
      "[     0 781568]\n",
      "[     0 781824]\n",
      "[     0 782080]\n",
      "[     0 782336]\n",
      "[     0 782592]\n",
      "[     0 782848]\n",
      "[     0 783104]\n",
      "[     0 783360]\n",
      "[     0 783616]\n",
      "[     0 783872]\n",
      "[     0 784128]\n",
      "[     0 784384]\n",
      "[     0 784640]\n",
      "[     0 784896]\n",
      "[     0 785152]\n",
      "[     0 785408]\n",
      "[     0 785664]\n",
      "[     0 785920]\n",
      "[     0 786176]\n",
      "[     0 786432]\n",
      "[     0 786688]\n",
      "[     0 786944]\n",
      "[     0 787200]\n",
      "[     0 787456]\n",
      "[     0 787712]\n",
      "[     0 787968]\n",
      "[     0 788224]\n",
      "[     0 788480]\n",
      "[     0 788736]\n",
      "[     0 788992]\n",
      "[     0 789248]\n",
      "[     0 789504]\n",
      "[     0 789760]\n",
      "[     0 790016]\n",
      "[     0 790272]\n",
      "[     0 790528]\n",
      "[     0 790784]\n",
      "[     0 791040]\n",
      "[     0 791296]\n",
      "[     0 791552]\n",
      "[     0 791808]\n",
      "[     0 792064]\n",
      "[     0 792320]\n",
      "[     0 792576]\n",
      "[     0 792832]\n",
      "[     0 793088]\n",
      "[     0 793344]\n",
      "[     0 793600]\n",
      "[     0 793856]\n",
      "[     0 794112]\n",
      "[     0 794368]\n",
      "[     0 794624]\n",
      "[     0 794880]\n",
      "[     0 795136]\n",
      "[     0 795392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 795648]\n",
      "[     0 795904]\n",
      "[     0 796160]\n",
      "[     0 796416]\n",
      "[     0 796672]\n",
      "[     0 796928]\n",
      "[     0 797184]\n",
      "[     0 797440]\n",
      "[     0 797696]\n",
      "[     0 797952]\n",
      "[     0 798208]\n",
      "[     0 798464]\n",
      "[     0 798720]\n",
      "[     0 798976]\n",
      "[     0 799232]\n",
      "[     0 799488]\n",
      "[     0 799744]\n",
      "[     0 800000]\n",
      "[     0 800256]\n",
      "[     0 800512]\n",
      "[     0 800768]\n",
      "[     0 801024]\n",
      "[     0 801280]\n",
      "[     0 801536]\n",
      "[     0 801792]\n",
      "[     0 802048]\n",
      "[     0 802304]\n",
      "[     0 802560]\n",
      "[     0 802816]\n",
      "[     0 803072]\n",
      "[     0 803328]\n",
      "[     0 803584]\n",
      "[     0 803840]\n",
      "[     0 804096]\n",
      "[     0 804352]\n",
      "[     0 804608]\n",
      "[     0 804864]\n",
      "[     0 805120]\n",
      "[     0 805376]\n",
      "[     0 805632]\n",
      "[     0 805888]\n",
      "[     0 806144]\n",
      "[     0 806400]\n",
      "[     0 806656]\n",
      "[     0 806912]\n",
      "[     0 807168]\n",
      "[     0 807424]\n",
      "[     0 807680]\n",
      "[     0 807936]\n",
      "[     0 808192]\n",
      "[     0 808448]\n",
      "[     0 808704]\n",
      "[     0 808960]\n",
      "[     0 809216]\n",
      "[     0 809472]\n",
      "[     0 809728]\n",
      "[     0 809984]\n",
      "[     0 810240]\n",
      "[     0 810496]\n",
      "[     0 810752]\n",
      "[     0 811008]\n",
      "[     0 811264]\n",
      "[     0 811520]\n",
      "[     0 811776]\n",
      "[     0 812032]\n",
      "[     0 812288]\n",
      "[     0 812544]\n",
      "[     0 812800]\n",
      "[     0 813056]\n",
      "[     0 813312]\n",
      "[     0 813568]\n",
      "[     0 813824]\n",
      "[     0 814080]\n",
      "[     0 814336]\n",
      "[     0 814592]\n",
      "[     0 814848]\n",
      "[     0 815104]\n",
      "[     0 815360]\n",
      "[     0 815616]\n",
      "[     0 815872]\n",
      "[     0 816128]\n",
      "[     0 816384]\n",
      "[     0 816640]\n",
      "[     0 816896]\n",
      "[     0 817152]\n",
      "[     0 817408]\n",
      "[     0 817664]\n",
      "[     0 817920]\n",
      "[     0 818176]\n",
      "[     0 818432]\n",
      "[     0 818688]\n",
      "[     0 818944]\n",
      "[     0 819200]\n",
      "[     0 819456]\n",
      "[     0 819712]\n",
      "[     0 819968]\n",
      "[     0 820224]\n",
      "[     0 820480]\n",
      "[     0 820736]\n",
      "[     0 820992]\n",
      "[     0 821248]\n",
      "[     0 821504]\n",
      "[     0 821760]\n",
      "[     0 822016]\n",
      "[     0 822272]\n",
      "[     0 822528]\n",
      "[     0 822784]\n",
      "[     0 823040]\n",
      "[     0 823296]\n",
      "[     0 823552]\n",
      "[     0 823808]\n",
      "[     0 824064]\n",
      "[     0 824320]\n",
      "[     0 824576]\n",
      "[     0 824832]\n",
      "[     0 825088]\n",
      "[     0 825344]\n",
      "[     0 825600]\n",
      "[     0 825856]\n",
      "[     0 826112]\n",
      "[     0 826368]\n",
      "[     0 826624]\n",
      "[     0 826880]\n",
      "[     0 827136]\n",
      "[     0 827392]\n",
      "[     0 827648]\n",
      "[     0 827904]\n",
      "[     0 828160]\n",
      "[     0 828416]\n",
      "[     0 828672]\n",
      "[     0 828928]\n",
      "[     0 829184]\n",
      "[     0 829440]\n",
      "[     0 829696]\n",
      "[     0 829952]\n",
      "[     0 830208]\n",
      "[     0 830464]\n",
      "[     0 830720]\n",
      "[     0 830976]\n",
      "[     0 831232]\n",
      "[     0 831488]\n",
      "[     0 831744]\n",
      "[     0 832000]\n",
      "[     0 832256]\n",
      "[     0 832512]\n",
      "[     0 832768]\n",
      "[     0 833024]\n",
      "[     0 833280]\n",
      "[     0 833536]\n",
      "[     0 833792]\n",
      "[     0 834048]\n",
      "[     0 834304]\n",
      "[     0 834560]\n",
      "[     0 834816]\n",
      "[     0 835072]\n",
      "[     0 835328]\n",
      "[     0 835584]\n",
      "[     0 835840]\n",
      "[     0 836096]\n",
      "[     0 836352]\n",
      "[     0 836608]\n",
      "[     0 836864]\n",
      "[     0 837120]\n",
      "[     0 837376]\n",
      "[     0 837632]\n",
      "[     0 837888]\n",
      "[     0 838144]\n",
      "[     0 838400]\n",
      "[     0 838656]\n",
      "[     0 838912]\n",
      "[     0 839168]\n",
      "[     0 839424]\n",
      "[     0 839680]\n",
      "[     0 839936]\n",
      "[     0 840192]\n",
      "[     0 840448]\n",
      "[     0 840704]\n",
      "[     0 840960]\n",
      "[     0 841216]\n",
      "[     0 841472]\n",
      "[     0 841728]\n",
      "[     0 841984]\n",
      "[     0 842240]\n",
      "[     0 842496]\n",
      "[     0 842752]\n",
      "[     0 843008]\n",
      "[     0 843264]\n",
      "[     0 843520]\n",
      "[     0 843776]\n",
      "[     0 844032]\n",
      "[     0 844288]\n",
      "[     0 844544]\n",
      "[     0 844800]\n",
      "[     0 845056]\n",
      "[     0 845312]\n",
      "[     0 845568]\n",
      "[     0 845824]\n",
      "[     0 846080]\n",
      "[     0 846336]\n",
      "[     0 846592]\n",
      "[     0 846848]\n",
      "[     0 847104]\n",
      "[     0 847360]\n",
      "[     0 847616]\n",
      "[     0 847872]\n",
      "[     0 848128]\n",
      "[     0 848384]\n",
      "[     0 848640]\n",
      "[     0 848896]\n",
      "[     0 849152]\n",
      "[     0 849408]\n",
      "[     0 849664]\n",
      "[     0 849920]\n",
      "[     0 850176]\n",
      "[     0 850432]\n",
      "[     0 850688]\n",
      "[     0 850944]\n",
      "[     0 851200]\n",
      "[     0 851456]\n",
      "[     0 851712]\n",
      "[     0 851968]\n",
      "[     0 852224]\n",
      "[     0 852480]\n",
      "[     0 852736]\n",
      "[     0 852992]\n",
      "[     0 853248]\n",
      "[     0 853504]\n",
      "[     0 853760]\n",
      "[     0 854016]\n",
      "[     0 854272]\n",
      "[     0 854528]\n",
      "[     0 854784]\n",
      "[     0 855040]\n",
      "[     0 855296]\n",
      "[     0 855552]\n",
      "[     0 855808]\n",
      "[     0 856064]\n",
      "[     0 856320]\n",
      "[     0 856576]\n",
      "[     0 856832]\n",
      "[     0 857088]\n",
      "[     0 857344]\n",
      "[     0 857600]\n",
      "[     0 857856]\n",
      "[     0 858112]\n",
      "[     0 858368]\n",
      "[     0 858624]\n",
      "[     0 858880]\n",
      "[     0 859136]\n",
      "[     0 859392]\n",
      "[     0 859648]\n",
      "[     0 859904]\n",
      "[     0 860160]\n",
      "[     0 860416]\n",
      "[     0 860672]\n",
      "[     0 860928]\n",
      "[     0 861184]\n",
      "[     0 861440]\n",
      "[     0 861696]\n",
      "[     0 861952]\n",
      "[     0 862208]\n",
      "[     0 862464]\n",
      "[     0 862720]\n",
      "[     0 862976]\n",
      "[     0 863232]\n",
      "[     0 863488]\n",
      "[     0 863744]\n",
      "[     0 864000]\n",
      "[     0 864256]\n",
      "[     0 864512]\n",
      "[     0 864768]\n",
      "[     0 865024]\n",
      "[     0 865280]\n",
      "[     0 865536]\n",
      "[     0 865792]\n",
      "[     0 866048]\n",
      "[     0 866304]\n",
      "[     0 866560]\n",
      "[     0 866816]\n",
      "[     0 867072]\n",
      "[     0 867328]\n",
      "[     0 867584]\n",
      "[     0 867840]\n",
      "[     0 868096]\n",
      "[     0 868352]\n",
      "[     0 868608]\n",
      "[     0 868864]\n",
      "[     0 869120]\n",
      "[     0 869376]\n",
      "[     0 869632]\n",
      "[     0 869888]\n",
      "[     0 870144]\n",
      "[     0 870400]\n",
      "[     0 870656]\n",
      "[     0 870912]\n",
      "[     0 871168]\n",
      "[     0 871424]\n",
      "[     0 871680]\n",
      "[     0 871936]\n",
      "[     0 872192]\n",
      "[     0 872448]\n",
      "[     0 872704]\n",
      "[     0 872960]\n",
      "[     0 873216]\n",
      "[     0 873472]\n",
      "[     0 873728]\n",
      "[     0 873984]\n",
      "[     0 874240]\n",
      "[     0 874496]\n",
      "[     0 874752]\n",
      "[     0 875008]\n",
      "[     0 875264]\n",
      "[     0 875520]\n",
      "[     0 875776]\n",
      "[     0 876032]\n",
      "[     0 876288]\n",
      "[     0 876544]\n",
      "[     0 876800]\n",
      "[     0 877056]\n",
      "[     0 877312]\n",
      "[     0 877568]\n",
      "[     0 877824]\n",
      "[     0 878080]\n",
      "[     0 878336]\n",
      "[     0 878592]\n",
      "[     0 878848]\n",
      "[     0 879104]\n",
      "[     0 879360]\n",
      "[     0 879616]\n",
      "[     0 879872]\n",
      "[     0 880128]\n",
      "[     0 880384]\n",
      "[     0 880640]\n",
      "[     0 880896]\n",
      "[     0 881152]\n",
      "[     0 881408]\n",
      "[     0 881664]\n",
      "[     0 881920]\n",
      "[     0 882176]\n",
      "[     0 882432]\n",
      "[     0 882688]\n",
      "[     0 882944]\n",
      "[     0 883200]\n",
      "[     0 883456]\n",
      "[     0 883712]\n",
      "[     0 883968]\n",
      "[     0 884224]\n",
      "[     0 884480]\n",
      "[     0 884736]\n",
      "[     0 884992]\n",
      "[     0 885248]\n",
      "[     0 885504]\n",
      "[     0 885760]\n",
      "[     0 886016]\n",
      "[     0 886272]\n",
      "[     0 886528]\n",
      "[     0 886784]\n",
      "[     0 887040]\n",
      "[     0 887296]\n",
      "[     0 887552]\n",
      "[     0 887808]\n",
      "[     0 888064]\n",
      "[     0 888320]\n",
      "[     0 888576]\n",
      "[     0 888832]\n",
      "[     0 889088]\n",
      "[     0 889344]\n",
      "[     0 889600]\n",
      "[     0 889856]\n",
      "[     0 890112]\n",
      "[     0 890368]\n",
      "[     0 890624]\n",
      "[     0 890880]\n",
      "[     0 891136]\n",
      "[     0 891392]\n",
      "[     0 891648]\n",
      "[     0 891904]\n",
      "[     0 892160]\n",
      "[     0 892416]\n",
      "[     0 892672]\n",
      "[     0 892928]\n",
      "[     0 893184]\n",
      "[     0 893440]\n",
      "[     0 893696]\n",
      "[     0 893952]\n",
      "[     0 894208]\n",
      "[     0 894464]\n",
      "[     0 894720]\n",
      "[     0 894976]\n",
      "[     0 895232]\n",
      "[     0 895488]\n",
      "[     0 895744]\n",
      "[     0 896000]\n",
      "[     0 896256]\n",
      "[     0 896512]\n",
      "[     0 896768]\n",
      "[     0 897024]\n",
      "[     0 897280]\n",
      "[     0 897536]\n",
      "[     0 897792]\n",
      "[     0 898048]\n",
      "[     0 898304]\n",
      "[     0 898560]\n",
      "[     0 898816]\n",
      "[     0 899072]\n",
      "[     0 899328]\n",
      "[     0 899584]\n",
      "[     0 899840]\n",
      "[     0 900096]\n",
      "[     0 900352]\n",
      "[     0 900608]\n",
      "[     0 900864]\n",
      "[     0 901120]\n",
      "[     0 901376]\n",
      "[     0 901632]\n",
      "[     0 901888]\n",
      "[     0 902144]\n",
      "[     0 902400]\n",
      "[     0 902656]\n",
      "[     0 902912]\n",
      "[     0 903168]\n",
      "[     0 903424]\n",
      "[     0 903680]\n",
      "[     0 903936]\n",
      "[     0 904192]\n",
      "[     0 904448]\n",
      "[     0 904704]\n",
      "[     0 904960]\n",
      "[     0 905216]\n",
      "[     0 905472]\n",
      "[     0 905728]\n",
      "[     0 905984]\n",
      "[     0 906240]\n",
      "[     0 906496]\n",
      "[     0 906752]\n",
      "[     0 907008]\n",
      "[     0 907264]\n",
      "[     0 907520]\n",
      "[     0 907776]\n",
      "[     0 908032]\n",
      "[     0 908288]\n",
      "[     0 908544]\n",
      "[     0 908800]\n",
      "[     0 909056]\n",
      "[     0 909312]\n",
      "[     0 909568]\n",
      "[     0 909824]\n",
      "[     0 910080]\n",
      "[     0 910336]\n",
      "[     0 910592]\n",
      "[     0 910848]\n",
      "[     0 911104]\n",
      "[     0 911360]\n",
      "[     0 911616]\n",
      "[     0 911872]\n",
      "[     0 912128]\n",
      "[     0 912384]\n",
      "[     0 912640]\n",
      "[     0 912896]\n",
      "[     0 913152]\n",
      "[     0 913408]\n",
      "[     0 913664]\n",
      "[     0 913920]\n",
      "[     0 914176]\n",
      "[     0 914432]\n",
      "[     0 914688]\n",
      "[     0 914944]\n",
      "[     0 915200]\n",
      "[     0 915456]\n",
      "[     0 915712]\n",
      "[     0 915968]\n",
      "[     0 916224]\n",
      "[     0 916480]\n",
      "[     0 916736]\n",
      "[     0 916992]\n",
      "[     0 917248]\n",
      "[     0 917504]\n",
      "[     0 917760]\n",
      "[     0 918016]\n",
      "[     0 918272]\n",
      "[     0 918528]\n",
      "[     0 918784]\n",
      "[     0 919040]\n",
      "[     0 919296]\n",
      "[     0 919552]\n",
      "[     0 919808]\n",
      "[     0 920064]\n",
      "[     0 920320]\n",
      "[     0 920576]\n",
      "[     0 920832]\n",
      "[     0 921088]\n",
      "[     0 921344]\n",
      "[     0 921600]\n",
      "[     0 921856]\n",
      "[     0 922112]\n",
      "[     0 922368]\n",
      "[     0 922624]\n",
      "[     0 922880]\n",
      "[     0 923136]\n",
      "[     0 923392]\n",
      "[     0 923648]\n",
      "[     0 923904]\n",
      "[     0 924160]\n",
      "[     0 924416]\n",
      "[     0 924672]\n",
      "[     0 924928]\n",
      "[     0 925184]\n",
      "[     0 925440]\n",
      "[     0 925696]\n",
      "[     0 925952]\n",
      "[     0 926208]\n",
      "[     0 926464]\n",
      "[     0 926720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 926976]\n",
      "[     0 927232]\n",
      "[     0 927488]\n",
      "[     0 927744]\n",
      "[     0 928000]\n",
      "[     0 928256]\n",
      "[     0 928512]\n",
      "[     0 928768]\n",
      "[     0 929024]\n",
      "[     0 929280]\n",
      "[     0 929536]\n",
      "[     0 929792]\n",
      "[     0 930048]\n",
      "[     0 930304]\n",
      "[     0 930560]\n",
      "[     0 930816]\n",
      "[     0 931072]\n",
      "[     0 931328]\n",
      "[     0 931584]\n",
      "[     0 931840]\n",
      "[     0 932096]\n",
      "[     0 932352]\n",
      "[     0 932608]\n",
      "[     0 932864]\n",
      "[     0 933120]\n",
      "[     0 933376]\n",
      "[     0 933632]\n",
      "[     0 933888]\n",
      "[     0 934144]\n",
      "[     0 934400]\n",
      "[     0 934656]\n",
      "[     0 934912]\n",
      "[     0 935168]\n",
      "[     0 935424]\n",
      "[     0 935680]\n",
      "[     0 935936]\n",
      "[     0 936192]\n",
      "[     0 936448]\n",
      "[     0 936704]\n",
      "[     0 936960]\n",
      "[     0 937216]\n",
      "[     0 937472]\n",
      "[     0 937728]\n",
      "[     0 937984]\n",
      "[     0 938240]\n",
      "[     0 938496]\n",
      "[     0 938752]\n",
      "[     0 939008]\n",
      "[     0 939264]\n",
      "[     0 939520]\n",
      "[     0 939776]\n",
      "[     0 940032]\n",
      "[     0 940288]\n",
      "[     0 940544]\n",
      "[     0 940800]\n",
      "[     0 941056]\n",
      "[     0 941312]\n",
      "[     0 941568]\n",
      "[     0 941824]\n",
      "[     0 942080]\n",
      "[     0 942336]\n",
      "[     0 942592]\n",
      "[     0 942848]\n",
      "[     0 943104]\n",
      "[     0 943360]\n",
      "[     0 943616]\n",
      "[     0 943872]\n",
      "[     0 944128]\n",
      "[     0 944384]\n",
      "[     0 944640]\n",
      "[     0 944896]\n",
      "[     0 945152]\n",
      "[     0 945408]\n",
      "[     0 945664]\n",
      "[     0 945920]\n",
      "[     0 946176]\n",
      "[     0 946432]\n",
      "[     0 946688]\n",
      "[     0 946944]\n",
      "[     0 947200]\n",
      "[     0 947456]\n",
      "[     0 947712]\n",
      "[     0 947968]\n",
      "[     0 948224]\n",
      "[     0 948480]\n",
      "[     0 948736]\n",
      "[     0 948992]\n",
      "[     0 949248]\n",
      "[     0 949504]\n",
      "[     0 949760]\n",
      "[     0 950016]\n",
      "[     0 950272]\n",
      "[     0 950528]\n",
      "[     0 950784]\n",
      "[     0 951040]\n",
      "[     0 951296]\n",
      "[     0 951552]\n",
      "[     0 951808]\n",
      "[     0 952064]\n",
      "[     0 952320]\n",
      "[     0 952576]\n",
      "[     0 952832]\n",
      "[     0 953088]\n",
      "[     0 953344]\n",
      "[     0 953600]\n",
      "[     0 953856]\n",
      "[     0 954112]\n",
      "[     0 954368]\n",
      "[     0 954624]\n",
      "[     0 954880]\n",
      "[     0 955136]\n",
      "[     0 955392]\n",
      "[     0 955648]\n",
      "[     0 955904]\n",
      "[     0 956160]\n",
      "[     0 956416]\n",
      "[     0 956672]\n",
      "[     0 956928]\n",
      "[     0 957184]\n",
      "[     0 957440]\n",
      "[     0 957696]\n",
      "[     0 957952]\n",
      "[     0 958208]\n",
      "[     0 958464]\n",
      "[     0 958720]\n",
      "[     0 958976]\n",
      "[     0 959232]\n",
      "[     0 959488]\n",
      "[     0 959744]\n",
      "[     0 960000]\n",
      "[     0 960256]\n",
      "[     0 960512]\n",
      "[     0 960768]\n",
      "[     0 961024]\n",
      "[     0 961280]\n",
      "[     0 961536]\n",
      "[     0 961792]\n",
      "[     0 962048]\n",
      "[     0 962304]\n",
      "[     0 962560]\n",
      "[     0 962816]\n",
      "[     0 963072]\n",
      "[     0 963328]\n",
      "[     0 963584]\n",
      "[     0 963840]\n",
      "[     0 964096]\n",
      "[     0 964352]\n",
      "[     0 964608]\n",
      "[     0 964864]\n",
      "[     0 965120]\n",
      "[     0 965376]\n",
      "[     0 965632]\n",
      "[     0 965888]\n",
      "[     0 966144]\n",
      "[     0 966400]\n",
      "[     0 966656]\n",
      "[     0 966912]\n",
      "[     0 967168]\n",
      "[     0 967424]\n",
      "[     0 967680]\n",
      "[     0 967936]\n",
      "[     0 968192]\n",
      "[     0 968448]\n",
      "[     0 968704]\n",
      "[     0 968960]\n",
      "[     0 969216]\n",
      "[     0 969472]\n",
      "[     0 969728]\n",
      "[     0 969984]\n",
      "[     0 970240]\n",
      "[     0 970496]\n",
      "[     0 970752]\n",
      "[     0 971008]\n",
      "[     0 971264]\n",
      "[     0 971520]\n",
      "[     0 971776]\n",
      "[     0 972032]\n",
      "[     0 972288]\n",
      "[     0 972544]\n",
      "[     0 972800]\n",
      "[     0 973056]\n",
      "[     0 973312]\n",
      "[     0 973568]\n",
      "[     0 973824]\n",
      "[     0 974080]\n",
      "[     0 974336]\n",
      "[     0 974592]\n",
      "[     0 974848]\n",
      "[     0 975104]\n",
      "[     0 975360]\n",
      "[     0 975616]\n",
      "[     0 975872]\n",
      "[     0 976128]\n",
      "[     0 976384]\n",
      "[     0 976640]\n",
      "[     0 976896]\n",
      "[     0 977152]\n",
      "[     0 977408]\n",
      "[     0 977664]\n",
      "[     0 977920]\n",
      "[     0 978176]\n",
      "[     0 978432]\n",
      "[     0 978688]\n",
      "[     0 978944]\n",
      "[     0 979200]\n",
      "[     0 979456]\n",
      "[     0 979712]\n",
      "[     0 979968]\n",
      "[     0 980224]\n",
      "[     0 980480]\n",
      "[     0 980736]\n",
      "[     0 980992]\n",
      "[     0 981248]\n",
      "[     0 981504]\n",
      "[     0 981760]\n",
      "[     0 982016]\n",
      "[     0 982272]\n",
      "[     0 982528]\n",
      "[     0 982784]\n",
      "[     0 983040]\n",
      "[     0 983296]\n",
      "[     0 983552]\n",
      "[     0 983808]\n",
      "[     0 984064]\n",
      "[     0 984320]\n",
      "[     0 984576]\n",
      "[     0 984832]\n",
      "[     0 985088]\n",
      "[     0 985344]\n",
      "[     0 985600]\n",
      "[     0 985856]\n",
      "[     0 986112]\n",
      "[     0 986368]\n",
      "[     0 986624]\n",
      "[     0 986880]\n",
      "[     0 987136]\n",
      "[     0 987392]\n",
      "[     0 987648]\n",
      "[     0 987904]\n",
      "[     0 988160]\n",
      "[     0 988416]\n",
      "[     0 988672]\n",
      "[     0 988928]\n",
      "[     0 989184]\n",
      "[     0 989440]\n",
      "[     0 989696]\n",
      "[     0 989952]\n",
      "[     0 990208]\n",
      "[     0 990464]\n",
      "[     0 990720]\n",
      "[     0 990976]\n",
      "[     0 991232]\n",
      "[     0 991488]\n",
      "[     0 991744]\n",
      "[     0 992000]\n",
      "[     0 992256]\n",
      "[     0 992512]\n",
      "[     0 992768]\n",
      "[     0 993024]\n",
      "[     0 993280]\n",
      "[     0 993536]\n",
      "[     0 993792]\n",
      "[     0 994048]\n",
      "[     0 994304]\n",
      "[     0 994560]\n",
      "[     0 994816]\n",
      "[     0 995072]\n",
      "[     0 995328]\n",
      "[     0 995584]\n",
      "[     0 995840]\n",
      "[     0 996096]\n",
      "[     0 996352]\n",
      "[     0 996608]\n",
      "[     0 996864]\n",
      "[     0 997120]\n",
      "[     0 997376]\n",
      "[     0 997632]\n",
      "[     0 997888]\n",
      "[     0 998144]\n",
      "[     0 998400]\n",
      "[     0 998656]\n",
      "[     0 998912]\n",
      "[     0 999168]\n",
      "[     0 999424]\n",
      "[     0 999680]\n",
      "[     0 999936]\n",
      "[      0 1000192]\n",
      "[      0 1000448]\n",
      "[      0 1000704]\n",
      "[      0 1000960]\n",
      "[      0 1001216]\n",
      "[      0 1001472]\n",
      "[      0 1001728]\n",
      "[      0 1001984]\n",
      "[      0 1002240]\n",
      "[      0 1002496]\n",
      "[      0 1002752]\n",
      "[      0 1003008]\n",
      "[      0 1003264]\n",
      "[      0 1003520]\n",
      "[      0 1003776]\n",
      "[      0 1004032]\n",
      "[      0 1004288]\n",
      "[      0 1004544]\n",
      "[      0 1004800]\n",
      "[      0 1005056]\n",
      "[      0 1005312]\n",
      "[      0 1005568]\n",
      "[      0 1005824]\n",
      "[      0 1006080]\n",
      "[      0 1006336]\n",
      "[      0 1006592]\n",
      "[      0 1006848]\n",
      "[      0 1007104]\n",
      "[      0 1007360]\n",
      "[      0 1007616]\n",
      "[      0 1007872]\n",
      "[      0 1008128]\n",
      "[      0 1008384]\n",
      "[      0 1008640]\n",
      "[      0 1008896]\n",
      "[      0 1009152]\n",
      "[      0 1009408]\n",
      "[      0 1009664]\n",
      "[      0 1009920]\n",
      "[      0 1010176]\n",
      "[      0 1010432]\n",
      "[      0 1010688]\n",
      "[      0 1010944]\n",
      "[      0 1011200]\n",
      "[      0 1011456]\n",
      "[      0 1011712]\n",
      "[      0 1011968]\n",
      "[      0 1012224]\n",
      "[      0 1012480]\n",
      "[      0 1012736]\n",
      "[      0 1012992]\n",
      "[      0 1013248]\n",
      "[      0 1013504]\n",
      "[      0 1013760]\n",
      "[      0 1014016]\n",
      "[      0 1014272]\n",
      "[      0 1014528]\n",
      "[      0 1014784]\n",
      "[      0 1015040]\n",
      "[      0 1015296]\n",
      "[      0 1015552]\n",
      "[      0 1015808]\n",
      "[      0 1016064]\n",
      "[      0 1016320]\n",
      "[      0 1016576]\n",
      "[      0 1016832]\n",
      "[      0 1017088]\n",
      "[      0 1017344]\n",
      "[      0 1017600]\n",
      "[      0 1017856]\n",
      "[      0 1018112]\n",
      "[      0 1018368]\n",
      "[      0 1018624]\n",
      "[      0 1018880]\n",
      "[      0 1019136]\n",
      "[      0 1019392]\n",
      "[      0 1019648]\n",
      "[      0 1019904]\n",
      "[      0 1020160]\n",
      "[      0 1020416]\n",
      "[      0 1020672]\n",
      "[      0 1020928]\n",
      "[      0 1021184]\n",
      "[      0 1021440]\n",
      "[      0 1021696]\n",
      "[      0 1021952]\n",
      "[      0 1022208]\n",
      "[      0 1022464]\n",
      "[      0 1022720]\n",
      "[      0 1022976]\n",
      "[      0 1023232]\n",
      "[      0 1023488]\n",
      "[      0 1023744]\n",
      "[      0 1024000]\n",
      "[      0 1024256]\n",
      "[      0 1024512]\n",
      "[      0 1024768]\n",
      "[      0 1025024]\n",
      "[      0 1025280]\n",
      "[      0 1025536]\n",
      "[      0 1025792]\n",
      "[      0 1026048]\n",
      "[      0 1026304]\n",
      "[      0 1026560]\n",
      "[      0 1026816]\n",
      "[      0 1027072]\n",
      "[      0 1027328]\n",
      "[      0 1027584]\n",
      "[      0 1027840]\n",
      "[      0 1028096]\n",
      "[      0 1028352]\n",
      "[      0 1028608]\n",
      "[      0 1028864]\n",
      "[      0 1029120]\n",
      "[      0 1029376]\n",
      "[      0 1029632]\n",
      "[      0 1029888]\n",
      "[      0 1030144]\n",
      "[      0 1030400]\n",
      "[      0 1030656]\n",
      "[      0 1030912]\n",
      "[      0 1031168]\n",
      "[      0 1031424]\n",
      "[      0 1031680]\n",
      "[      0 1031936]\n",
      "[      0 1032192]\n",
      "[      0 1032448]\n",
      "[      0 1032704]\n",
      "[      0 1032960]\n",
      "[      0 1033216]\n",
      "[      0 1033472]\n",
      "[      0 1033728]\n",
      "[      0 1033984]\n",
      "[      0 1034240]\n",
      "[      0 1034496]\n",
      "[      0 1034752]\n",
      "[      0 1035008]\n",
      "[      0 1035264]\n",
      "[      0 1035520]\n",
      "[      0 1035776]\n",
      "[      0 1036032]\n",
      "[      0 1036288]\n",
      "[      0 1036544]\n",
      "[      0 1036800]\n",
      "[      0 1037056]\n",
      "[      0 1037312]\n",
      "[      0 1037568]\n",
      "[      0 1037824]\n",
      "[      0 1038080]\n",
      "[      0 1038336]\n",
      "[      0 1038592]\n",
      "[      0 1038848]\n",
      "[      0 1039104]\n",
      "[      0 1039360]\n",
      "[      0 1039616]\n",
      "[      0 1039872]\n",
      "[      0 1040128]\n",
      "[      0 1040384]\n",
      "[      0 1040640]\n",
      "[      0 1040896]\n",
      "[      0 1041152]\n",
      "[      0 1041408]\n",
      "[      0 1041664]\n",
      "[      0 1041920]\n",
      "[      0 1042176]\n",
      "[      0 1042432]\n",
      "[      0 1042688]\n",
      "[      0 1042944]\n",
      "[      0 1043200]\n",
      "[      0 1043456]\n",
      "[      0 1043712]\n",
      "[      0 1043968]\n",
      "[      0 1044224]\n",
      "[      0 1044480]\n",
      "[      0 1044736]\n",
      "[      0 1044992]\n",
      "[      0 1045248]\n",
      "[      0 1045504]\n",
      "[      0 1045760]\n",
      "[      0 1046016]\n",
      "[      0 1046272]\n",
      "[      0 1046528]\n",
      "[      0 1046784]\n",
      "[      0 1047040]\n",
      "[      0 1047296]\n",
      "[      0 1047552]\n",
      "[      0 1047808]\n",
      "[      0 1048064]\n",
      "[      0 1048320]\n",
      "[      0 1048576]\n",
      "[      0 1048832]\n",
      "[      0 1049088]\n",
      "[      0 1049344]\n",
      "[      0 1049600]\n",
      "[      0 1049856]\n",
      "[      0 1050112]\n",
      "[      0 1050368]\n",
      "[      0 1050624]\n",
      "[      0 1050880]\n",
      "[      0 1051136]\n",
      "[      0 1051392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1051648]\n",
      "[      0 1051904]\n",
      "[      0 1052160]\n",
      "[      0 1052416]\n",
      "[      0 1052672]\n",
      "[      0 1052928]\n",
      "[      0 1053184]\n",
      "[      0 1053440]\n",
      "[      0 1053696]\n",
      "[      0 1053952]\n",
      "[      0 1054208]\n",
      "[      0 1054464]\n",
      "[      0 1054720]\n",
      "[      0 1054976]\n",
      "[      0 1055232]\n",
      "[      0 1055488]\n",
      "[      0 1055744]\n",
      "[      0 1056000]\n",
      "[      0 1056256]\n",
      "[      0 1056512]\n",
      "[      0 1056768]\n",
      "[      0 1057024]\n",
      "[      0 1057280]\n",
      "[      0 1057536]\n",
      "[      0 1057792]\n",
      "[      0 1058048]\n",
      "[      0 1058304]\n",
      "[      0 1058560]\n",
      "[      0 1058816]\n",
      "[      0 1059072]\n",
      "[      0 1059328]\n",
      "[      0 1059584]\n",
      "[      0 1059840]\n",
      "[      0 1060096]\n",
      "[      0 1060352]\n",
      "[      0 1060608]\n",
      "[      0 1060864]\n",
      "[      0 1061120]\n",
      "[      0 1061376]\n",
      "[      0 1061632]\n",
      "[      0 1061888]\n",
      "[      0 1062144]\n",
      "[      0 1062400]\n",
      "[      0 1062656]\n",
      "[      0 1062912]\n",
      "[      0 1063168]\n",
      "[      0 1063424]\n",
      "[      0 1063680]\n",
      "[      0 1063936]\n",
      "[      0 1064192]\n",
      "[      0 1064448]\n",
      "[      0 1064704]\n",
      "[      0 1064960]\n",
      "[      0 1065216]\n",
      "[      0 1065472]\n",
      "[      0 1065728]\n",
      "[      0 1065984]\n",
      "[      0 1066240]\n",
      "[      0 1066496]\n",
      "[      0 1066752]\n",
      "[      0 1067008]\n",
      "[      0 1067264]\n",
      "[      0 1067520]\n",
      "[      0 1067776]\n",
      "[      0 1068032]\n",
      "[      0 1068288]\n",
      "[      0 1068544]\n",
      "[      0 1068800]\n",
      "[      0 1069056]\n",
      "[      0 1069312]\n",
      "[      0 1069568]\n",
      "[      0 1069824]\n",
      "[      0 1070080]\n",
      "[      0 1070336]\n",
      "[      0 1070592]\n",
      "[      0 1070848]\n",
      "[      0 1071104]\n",
      "[      0 1071360]\n",
      "[      0 1071616]\n",
      "[      0 1071872]\n",
      "[      0 1072128]\n",
      "[      0 1072384]\n",
      "[      0 1072640]\n",
      "[      0 1072896]\n",
      "[      0 1073152]\n",
      "[      0 1073408]\n",
      "[      0 1073664]\n",
      "[      0 1073920]\n",
      "[      0 1074176]\n",
      "[      0 1074432]\n",
      "[      0 1074688]\n",
      "[      0 1074944]\n",
      "[      0 1075200]\n",
      "[      0 1075456]\n",
      "[      0 1075712]\n",
      "[      0 1075968]\n",
      "[      0 1076224]\n",
      "[      0 1076480]\n",
      "[      0 1076736]\n",
      "[      0 1076992]\n",
      "[      0 1077248]\n",
      "[      0 1077504]\n",
      "[      0 1077760]\n",
      "[      0 1078016]\n",
      "[      0 1078272]\n",
      "[      0 1078528]\n",
      "[      0 1078784]\n",
      "[      0 1079040]\n",
      "[      0 1079296]\n",
      "[      0 1079552]\n",
      "[      0 1079808]\n",
      "[      0 1080064]\n",
      "[      0 1080320]\n",
      "[      0 1080576]\n",
      "[      0 1080832]\n",
      "[      0 1081088]\n",
      "[      0 1081344]\n",
      "[      0 1081600]\n",
      "[      0 1081856]\n",
      "[      0 1082112]\n",
      "[      0 1082368]\n",
      "[      0 1082624]\n",
      "[      0 1082880]\n",
      "[      0 1083136]\n",
      "[      0 1083392]\n",
      "[      0 1083648]\n",
      "[      0 1083904]\n",
      "[      0 1084160]\n",
      "[      0 1084416]\n",
      "[      0 1084672]\n",
      "[      0 1084928]\n",
      "[      0 1085184]\n",
      "[      0 1085440]\n",
      "[      0 1085696]\n",
      "[      0 1085952]\n",
      "[      0 1086208]\n",
      "[      0 1086464]\n",
      "[      0 1086720]\n",
      "[      0 1086976]\n",
      "[      0 1087232]\n",
      "[      0 1087488]\n",
      "[      0 1087744]\n",
      "[      0 1088000]\n",
      "[      0 1088256]\n",
      "[      0 1088512]\n",
      "[      0 1088768]\n",
      "[      0 1089024]\n",
      "[      0 1089280]\n",
      "[      0 1089536]\n",
      "[      0 1089792]\n",
      "[      0 1090048]\n",
      "[      0 1090304]\n",
      "[      0 1090560]\n",
      "[      0 1090816]\n",
      "[      0 1091072]\n",
      "[      0 1091328]\n",
      "[      0 1091584]\n",
      "[      0 1091840]\n",
      "[      0 1092096]\n",
      "[      0 1092352]\n",
      "[      0 1092608]\n",
      "[      0 1092864]\n",
      "[      0 1093120]\n",
      "[      0 1093376]\n",
      "[      0 1093632]\n",
      "[      0 1093888]\n",
      "[      0 1094144]\n",
      "[      0 1094400]\n",
      "[      0 1094656]\n",
      "[      0 1094912]\n",
      "[      0 1095168]\n",
      "[      0 1095424]\n",
      "[      0 1095680]\n",
      "[      0 1095936]\n",
      "[      0 1096192]\n",
      "[      0 1096448]\n",
      "[      0 1096704]\n",
      "[      0 1096960]\n",
      "[      0 1097216]\n",
      "[      0 1097472]\n",
      "[      0 1097728]\n",
      "[      0 1097984]\n",
      "[      0 1098240]\n",
      "[      0 1098496]\n",
      "[      0 1098752]\n",
      "[      0 1099008]\n",
      "[      0 1099264]\n",
      "[      0 1099520]\n",
      "[      0 1099776]\n",
      "[      0 1100032]\n",
      "[      0 1100288]\n",
      "[      0 1100544]\n",
      "[      0 1100800]\n",
      "[      0 1101056]\n",
      "[      0 1101312]\n",
      "[      0 1101568]\n",
      "[      0 1101824]\n",
      "[      0 1102080]\n",
      "[      0 1102336]\n",
      "[      0 1102592]\n",
      "[      0 1102848]\n",
      "[      0 1103104]\n",
      "[      0 1103360]\n",
      "[      0 1103616]\n",
      "[      0 1103872]\n",
      "[      0 1104128]\n",
      "[      0 1104384]\n",
      "[      0 1104640]\n",
      "[      0 1104896]\n",
      "[      0 1105152]\n",
      "[      0 1105408]\n",
      "[      0 1105664]\n",
      "[      0 1105920]\n",
      "[      0 1106176]\n",
      "[      0 1106432]\n",
      "[      0 1106688]\n",
      "[      0 1106944]\n",
      "[      0 1107200]\n",
      "[      0 1107456]\n",
      "[      0 1107712]\n",
      "[      0 1107968]\n",
      "[      0 1108224]\n",
      "[      0 1108480]\n",
      "[      0 1108736]\n",
      "[      0 1108992]\n",
      "[      0 1109248]\n",
      "[      0 1109504]\n",
      "[      0 1109760]\n",
      "[      0 1110016]\n",
      "[      0 1110272]\n",
      "[      0 1110528]\n",
      "[      0 1110784]\n",
      "[      0 1111040]\n",
      "[      0 1111296]\n",
      "[      0 1111552]\n",
      "[      0 1111808]\n",
      "[      0 1112064]\n",
      "[      0 1112320]\n",
      "[      0 1112576]\n",
      "[      0 1112832]\n",
      "[      0 1113088]\n",
      "[      0 1113344]\n",
      "[      0 1113600]\n",
      "[      0 1113856]\n",
      "[      0 1114112]\n",
      "[      0 1114368]\n",
      "[      0 1114624]\n",
      "[      0 1114880]\n",
      "[      0 1115136]\n",
      "[      0 1115392]\n",
      "[      0 1115648]\n",
      "[      0 1115904]\n",
      "[      0 1116160]\n",
      "[      0 1116416]\n",
      "[      0 1116672]\n",
      "[      0 1116928]\n",
      "[      0 1117184]\n",
      "[      0 1117440]\n",
      "[      0 1117696]\n",
      "[      0 1117952]\n",
      "[      0 1118208]\n",
      "[      0 1118464]\n",
      "[      0 1118720]\n",
      "[      0 1118976]\n",
      "[      0 1119232]\n",
      "[      0 1119488]\n",
      "[      0 1119744]\n",
      "[      0 1120000]\n",
      "[      0 1120256]\n",
      "[      0 1120512]\n",
      "[      0 1120768]\n",
      "[      0 1121024]\n",
      "[      0 1121280]\n",
      "[      0 1121536]\n",
      "[      0 1121792]\n",
      "[      0 1122048]\n",
      "[      0 1122304]\n",
      "[      0 1122560]\n",
      "[      0 1122816]\n",
      "[      0 1123072]\n",
      "[      0 1123328]\n",
      "[      0 1123584]\n",
      "[      0 1123840]\n",
      "[      0 1124096]\n",
      "[      0 1124352]\n",
      "[      0 1124608]\n",
      "[      0 1124864]\n",
      "[      0 1125120]\n",
      "[      0 1125376]\n",
      "[      0 1125632]\n",
      "[      0 1125888]\n",
      "[      0 1126144]\n",
      "[      0 1126400]\n",
      "[      0 1126656]\n",
      "[      0 1126912]\n",
      "[      0 1127168]\n",
      "[      0 1127424]\n",
      "[      0 1127680]\n",
      "[      0 1127936]\n",
      "[      0 1128192]\n",
      "[      0 1128448]\n",
      "[      0 1128704]\n",
      "[      0 1128960]\n",
      "[      0 1129216]\n",
      "[      0 1129472]\n",
      "[      0 1129728]\n",
      "[      0 1129984]\n",
      "[      0 1130240]\n",
      "[      0 1130496]\n",
      "[      0 1130752]\n",
      "[      0 1131008]\n",
      "[      0 1131264]\n",
      "[      0 1131520]\n",
      "[      0 1131776]\n",
      "[      0 1132032]\n",
      "[      0 1132288]\n",
      "[      0 1132544]\n",
      "[      0 1132800]\n",
      "[      0 1133056]\n",
      "[      0 1133312]\n",
      "[      0 1133568]\n",
      "[      0 1133824]\n",
      "[      0 1134080]\n",
      "[      0 1134336]\n",
      "[      0 1134592]\n",
      "[      0 1134848]\n",
      "[      0 1135104]\n",
      "[      0 1135360]\n",
      "[      0 1135616]\n",
      "[      0 1135872]\n",
      "[      0 1136128]\n",
      "[      0 1136384]\n",
      "[      0 1136640]\n",
      "[      0 1136896]\n",
      "[      0 1137152]\n",
      "[      0 1137408]\n",
      "[      0 1137664]\n",
      "[      0 1137920]\n",
      "[      0 1138176]\n",
      "[      0 1138432]\n",
      "[      0 1138688]\n",
      "[      0 1138944]\n",
      "[      0 1139200]\n",
      "[      0 1139456]\n",
      "[      0 1139712]\n",
      "[      0 1139968]\n",
      "[      0 1140224]\n",
      "[      0 1140480]\n",
      "[      0 1140736]\n",
      "[      0 1140992]\n",
      "[      0 1141248]\n",
      "[      0 1141504]\n",
      "[      0 1141760]\n",
      "[      0 1142016]\n",
      "[      0 1142272]\n",
      "[      0 1142528]\n",
      "[      0 1142784]\n",
      "[      0 1143040]\n",
      "[      0 1143296]\n",
      "[      0 1143552]\n",
      "[      0 1143808]\n",
      "[      0 1144064]\n",
      "[      0 1144320]\n",
      "[      0 1144576]\n",
      "[      0 1144832]\n",
      "[      0 1145088]\n",
      "[      0 1145344]\n",
      "[      0 1145600]\n",
      "[      0 1145856]\n",
      "[      0 1146112]\n",
      "[      0 1146368]\n",
      "[      0 1146624]\n",
      "[      0 1146880]\n",
      "[      0 1147136]\n",
      "[      0 1147392]\n",
      "[      0 1147648]\n",
      "[      0 1147904]\n",
      "[      0 1148160]\n",
      "[      0 1148416]\n",
      "[      0 1148672]\n",
      "[      0 1148928]\n",
      "[      0 1149184]\n",
      "[      0 1149440]\n",
      "[      0 1149696]\n",
      "[      0 1149952]\n",
      "[      0 1150208]\n",
      "[      0 1150464]\n",
      "[      0 1150720]\n",
      "[      0 1150976]\n",
      "[      0 1151232]\n",
      "[      0 1151488]\n",
      "[      0 1151744]\n",
      "[      0 1152000]\n",
      "[      0 1152256]\n",
      "[      0 1152512]\n",
      "[      0 1152768]\n",
      "[      0 1153024]\n",
      "[      0 1153280]\n",
      "[      0 1153536]\n",
      "[      0 1153792]\n",
      "[      0 1154048]\n",
      "[      0 1154304]\n",
      "[      0 1154560]\n",
      "[      0 1154816]\n",
      "[      0 1155072]\n",
      "[      0 1155328]\n",
      "[      0 1155584]\n",
      "[      0 1155840]\n",
      "[      0 1156096]\n",
      "[      0 1156352]\n",
      "[      0 1156608]\n",
      "[      0 1156864]\n",
      "[      0 1157120]\n",
      "[      0 1157376]\n",
      "[      0 1157632]\n",
      "[      0 1157888]\n",
      "[      0 1158144]\n",
      "[      0 1158400]\n",
      "[      0 1158656]\n",
      "[      0 1158912]\n",
      "[      0 1159168]\n",
      "[      0 1159424]\n",
      "[      0 1159680]\n",
      "[      0 1159936]\n",
      "[      0 1160192]\n",
      "[      0 1160448]\n",
      "[      0 1160704]\n",
      "[      0 1160960]\n",
      "[      0 1161216]\n",
      "[      0 1161472]\n",
      "[      0 1161728]\n",
      "[      0 1161984]\n",
      "[      0 1162240]\n",
      "[      0 1162496]\n",
      "[      0 1162752]\n",
      "[      0 1163008]\n",
      "[      0 1163264]\n",
      "[      0 1163520]\n",
      "[      0 1163776]\n",
      "[      0 1164032]\n",
      "[      0 1164288]\n",
      "[      0 1164544]\n",
      "[      0 1164800]\n",
      "[      0 1165056]\n",
      "[      0 1165312]\n",
      "[      0 1165568]\n",
      "[      0 1165824]\n",
      "[      0 1166080]\n",
      "[      0 1166336]\n",
      "[      0 1166592]\n",
      "[      0 1166848]\n",
      "[      0 1167104]\n",
      "[      0 1167360]\n",
      "[      0 1167616]\n",
      "[      0 1167872]\n",
      "[      0 1168128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1168384]\n",
      "[      0 1168640]\n",
      "[      0 1168896]\n",
      "[      0 1169152]\n",
      "[      0 1169408]\n",
      "[      0 1169664]\n",
      "[      0 1169920]\n",
      "[      0 1170176]\n",
      "[      0 1170432]\n",
      "[      0 1170688]\n",
      "[      0 1170944]\n",
      "[      0 1171200]\n",
      "[      0 1171456]\n",
      "[      0 1171712]\n",
      "[      0 1171968]\n",
      "[      0 1172224]\n",
      "[      0 1172480]\n",
      "[      0 1172736]\n",
      "[      0 1172992]\n",
      "[      0 1173248]\n",
      "[      0 1173504]\n",
      "[      0 1173760]\n",
      "[      0 1174016]\n",
      "[      0 1174272]\n",
      "[      0 1174528]\n",
      "[      0 1174784]\n",
      "[      0 1175040]\n",
      "[      0 1175296]\n",
      "[      0 1175552]\n",
      "[      0 1175808]\n",
      "[      0 1176064]\n",
      "[      0 1176320]\n",
      "[      0 1176576]\n",
      "[      0 1176832]\n",
      "[      0 1177088]\n",
      "[      0 1177344]\n",
      "[      0 1177600]\n",
      "[      0 1177856]\n",
      "[      0 1178112]\n",
      "[      0 1178368]\n",
      "[      0 1178624]\n",
      "[      0 1178880]\n",
      "[      0 1179136]\n",
      "[      0 1179392]\n",
      "[      0 1179648]\n",
      "[      0 1179904]\n",
      "[      0 1180160]\n",
      "[      0 1180416]\n",
      "[      0 1180672]\n",
      "[      0 1180928]\n",
      "[      0 1181184]\n",
      "[      0 1181440]\n",
      "[      0 1181696]\n",
      "[      0 1181952]\n",
      "[      0 1182208]\n",
      "[      0 1182464]\n",
      "[      0 1182720]\n",
      "[      0 1182976]\n",
      "[      0 1183232]\n",
      "[      0 1183488]\n",
      "[      0 1183744]\n",
      "[      0 1184000]\n",
      "[      0 1184256]\n",
      "[      0 1184512]\n",
      "[      0 1184768]\n",
      "[      0 1185024]\n",
      "[      0 1185280]\n",
      "[      0 1185536]\n",
      "[      0 1185792]\n",
      "[      0 1186048]\n",
      "[      0 1186304]\n",
      "[      0 1186560]\n",
      "[      0 1186816]\n",
      "[      0 1187072]\n",
      "[      0 1187328]\n",
      "[      0 1187584]\n",
      "[      0 1187840]\n",
      "[      0 1188096]\n",
      "[      0 1188352]\n",
      "[      0 1188608]\n",
      "[      0 1188864]\n",
      "[      0 1189120]\n",
      "[      0 1189376]\n",
      "[      0 1189632]\n",
      "[      0 1189888]\n",
      "[      0 1190144]\n",
      "[      0 1190400]\n",
      "[      0 1190656]\n",
      "[      0 1190912]\n",
      "[      0 1191168]\n",
      "[      0 1191424]\n",
      "[      0 1191680]\n",
      "[      0 1191936]\n",
      "[      0 1192192]\n",
      "[      0 1192448]\n",
      "[      0 1192704]\n",
      "[      0 1192960]\n",
      "[      0 1193216]\n",
      "[      0 1193472]\n",
      "[      0 1193728]\n",
      "[      0 1193984]\n",
      "[      0 1194240]\n",
      "[      0 1194496]\n",
      "[      0 1194752]\n",
      "[      0 1195008]\n",
      "[      0 1195264]\n",
      "[      0 1195520]\n",
      "[      0 1195776]\n",
      "[      0 1196032]\n",
      "[      0 1196288]\n",
      "[      0 1196544]\n",
      "[      0 1196800]\n",
      "[      0 1197056]\n",
      "[      0 1197312]\n",
      "[      0 1197568]\n",
      "[      0 1197824]\n",
      "[      0 1198080]\n",
      "[      0 1198336]\n",
      "[      0 1198592]\n",
      "[      0 1198848]\n",
      "[      0 1199104]\n",
      "[      0 1199360]\n",
      "[      0 1199616]\n",
      "[      0 1199872]\n",
      "[      0 1200128]\n",
      "[      0 1200384]\n",
      "[      0 1200640]\n",
      "[      0 1200896]\n",
      "[      0 1201152]\n",
      "[      0 1201408]\n",
      "[      0 1201664]\n",
      "[      0 1201920]\n",
      "[      0 1202176]\n",
      "[      0 1202432]\n",
      "[      0 1202688]\n",
      "[      0 1202944]\n",
      "[      0 1203200]\n",
      "[      0 1203456]\n",
      "[      0 1203712]\n",
      "[      0 1203968]\n",
      "[      0 1204224]\n",
      "[      0 1204480]\n",
      "[      0 1204736]\n",
      "[      0 1204992]\n",
      "[      0 1205248]\n",
      "[      0 1205504]\n",
      "[      0 1205760]\n",
      "[      0 1206016]\n",
      "[      0 1206272]\n",
      "[      0 1206528]\n",
      "[      0 1206784]\n",
      "[      0 1207040]\n",
      "[      0 1207296]\n",
      "[      0 1207552]\n",
      "[      0 1207808]\n",
      "[      0 1208064]\n",
      "[      0 1208320]\n",
      "[      0 1208576]\n",
      "[      0 1208832]\n",
      "[      0 1209088]\n",
      "[      0 1209344]\n",
      "[      0 1209600]\n",
      "[      0 1209856]\n",
      "[      0 1210112]\n",
      "[      0 1210368]\n",
      "[      0 1210624]\n",
      "[      0 1210880]\n",
      "[      0 1211136]\n",
      "[      0 1211392]\n",
      "[      0 1211648]\n",
      "[      0 1211904]\n",
      "[      0 1212160]\n",
      "[      0 1212416]\n",
      "[      0 1212672]\n",
      "[      0 1212928]\n",
      "[      0 1213184]\n",
      "[      0 1213440]\n",
      "[      0 1213696]\n",
      "[      0 1213952]\n",
      "[      0 1214208]\n",
      "[      0 1214464]\n",
      "[      0 1214720]\n",
      "[      0 1214976]\n",
      "[      0 1215232]\n",
      "[      0 1215488]\n",
      "[      0 1215744]\n",
      "[      0 1216000]\n",
      "[      0 1216256]\n",
      "[      0 1216512]\n",
      "[      0 1216768]\n",
      "[      0 1217024]\n",
      "[      0 1217280]\n",
      "[      0 1217536]\n",
      "[      0 1217792]\n",
      "[      0 1218048]\n",
      "[      0 1218304]\n",
      "[      0 1218560]\n",
      "[      0 1218816]\n",
      "[      0 1219072]\n",
      "[      0 1219328]\n",
      "[      0 1219584]\n",
      "[      0 1219840]\n",
      "[      0 1220096]\n",
      "[      0 1220352]\n",
      "[      0 1220608]\n",
      "[      0 1220864]\n",
      "[      0 1221120]\n",
      "[      0 1221376]\n",
      "[      0 1221632]\n",
      "[      0 1221888]\n",
      "[      0 1222144]\n",
      "[      0 1222400]\n",
      "[      0 1222656]\n",
      "[      0 1222912]\n",
      "[      0 1223168]\n",
      "[      0 1223424]\n",
      "[      0 1223680]\n",
      "[      0 1223936]\n",
      "[      0 1224192]\n",
      "[      0 1224448]\n",
      "[      0 1224704]\n",
      "[      0 1224960]\n",
      "[      0 1225216]\n",
      "[      0 1225472]\n",
      "[      0 1225728]\n",
      "[      0 1225984]\n",
      "[      0 1226240]\n",
      "[      0 1226496]\n",
      "[      0 1226752]\n",
      "[      0 1227008]\n",
      "[      0 1227264]\n",
      "[      0 1227520]\n",
      "[      0 1227776]\n",
      "[      0 1228032]\n",
      "[      0 1228288]\n",
      "[      0 1228544]\n",
      "[      0 1228800]\n",
      "[      0 1229056]\n",
      "[      0 1229312]\n",
      "[      0 1229568]\n",
      "[      0 1229824]\n",
      "[      0 1230080]\n",
      "[      0 1230336]\n",
      "[      0 1230592]\n",
      "[      0 1230848]\n",
      "[      0 1231104]\n",
      "[      0 1231360]\n",
      "[      0 1231616]\n",
      "[      0 1231872]\n",
      "[      0 1232128]\n",
      "[      0 1232384]\n",
      "[      0 1232640]\n",
      "[      0 1232896]\n",
      "[      0 1233152]\n",
      "[      0 1233408]\n",
      "[      0 1233664]\n",
      "[      0 1233920]\n",
      "[      0 1234176]\n",
      "[      0 1234432]\n",
      "[      0 1234688]\n",
      "[      0 1234944]\n",
      "[      0 1235200]\n",
      "[      0 1235456]\n",
      "[      0 1235712]\n",
      "[      0 1235968]\n",
      "[      0 1236224]\n",
      "[      0 1236480]\n",
      "[      0 1236736]\n",
      "[      0 1236992]\n",
      "[      0 1237248]\n",
      "[      0 1237504]\n",
      "[      0 1237760]\n",
      "[      0 1238016]\n",
      "[      0 1238272]\n",
      "[      0 1238528]\n",
      "[      0 1238784]\n",
      "[      0 1239040]\n",
      "[      0 1239296]\n",
      "[      0 1239552]\n",
      "[      0 1239808]\n",
      "[      0 1240064]\n",
      "[      0 1240320]\n",
      "[      0 1240576]\n",
      "[      0 1240832]\n",
      "[      0 1241088]\n",
      "[      0 1241344]\n",
      "[      0 1241600]\n",
      "[      0 1241856]\n",
      "[      0 1242112]\n",
      "[      0 1242368]\n",
      "[      0 1242624]\n",
      "[      0 1242880]\n",
      "[      0 1243136]\n",
      "[      0 1243392]\n",
      "[      0 1243648]\n",
      "[      0 1243904]\n",
      "[      0 1244160]\n",
      "[      0 1244416]\n",
      "[      0 1244672]\n",
      "[      0 1244928]\n",
      "[      0 1245184]\n",
      "[      0 1245440]\n",
      "[      0 1245696]\n",
      "[      0 1245952]\n",
      "[      0 1246208]\n",
      "[      0 1246464]\n",
      "[      0 1246720]\n",
      "[      0 1246976]\n",
      "[      0 1247232]\n",
      "[      0 1247488]\n",
      "[      0 1247744]\n",
      "[      0 1248000]\n",
      "[      0 1248256]\n",
      "[      0 1248512]\n",
      "[      0 1248768]\n",
      "[      0 1249024]\n",
      "[      0 1249280]\n",
      "[      0 1249536]\n",
      "[      0 1249792]\n",
      "[      0 1250048]\n",
      "[      0 1250304]\n",
      "[      0 1250560]\n",
      "[      0 1250816]\n",
      "[      0 1251072]\n",
      "[      0 1251328]\n",
      "[      0 1251584]\n",
      "[      0 1251840]\n",
      "[      0 1252096]\n",
      "[      0 1252352]\n",
      "[      0 1252608]\n",
      "[      0 1252864]\n",
      "[      0 1253120]\n",
      "[      0 1253376]\n",
      "[      0 1253632]\n",
      "[      0 1253888]\n",
      "[      0 1254144]\n",
      "[      0 1254400]\n",
      "[      0 1254656]\n",
      "[      0 1254912]\n",
      "[      0 1255168]\n",
      "[      0 1255424]\n",
      "[      0 1255680]\n",
      "[      0 1255936]\n",
      "[      0 1256192]\n",
      "[      0 1256448]\n",
      "[      0 1256704]\n",
      "[      0 1256960]\n",
      "[      0 1257216]\n",
      "[      0 1257472]\n",
      "[      0 1257728]\n",
      "[      0 1257984]\n",
      "[      0 1258240]\n",
      "[      0 1258496]\n",
      "[      0 1258752]\n",
      "[      0 1259008]\n",
      "[      0 1259264]\n",
      "[      0 1259520]\n",
      "[      0 1259776]\n",
      "[      0 1260032]\n",
      "[      0 1260288]\n",
      "[      0 1260544]\n",
      "[      0 1260800]\n",
      "[      0 1261056]\n",
      "[      0 1261312]\n",
      "[      0 1261568]\n",
      "[      0 1261824]\n",
      "[      0 1262080]\n",
      "[      0 1262336]\n",
      "[      0 1262592]\n",
      "[      0 1262848]\n",
      "[      0 1263104]\n",
      "[      0 1263360]\n",
      "[      0 1263616]\n",
      "[      0 1263872]\n",
      "[      0 1264128]\n",
      "[      0 1264384]\n",
      "[      0 1264640]\n",
      "[      0 1264896]\n",
      "[      0 1265152]\n",
      "[      0 1265408]\n",
      "[      0 1265664]\n",
      "[      0 1265920]\n",
      "[      0 1266176]\n",
      "[      0 1266432]\n",
      "[      0 1266688]\n",
      "[      0 1266944]\n",
      "[      0 1267200]\n",
      "[      0 1267456]\n",
      "[      0 1267712]\n",
      "[      0 1267968]\n",
      "[      0 1268224]\n",
      "[      0 1268480]\n",
      "[      0 1268736]\n",
      "[      0 1268992]\n",
      "[      0 1269248]\n",
      "[      0 1269504]\n",
      "[      0 1269760]\n",
      "[      0 1270016]\n",
      "[      0 1270272]\n",
      "[      0 1270528]\n",
      "[      0 1270784]\n",
      "[      0 1271040]\n",
      "[      0 1271296]\n",
      "[      0 1271552]\n",
      "[      0 1271808]\n",
      "[      0 1272064]\n",
      "[      0 1272320]\n",
      "[      0 1272576]\n",
      "[      0 1272832]\n",
      "[      0 1273088]\n",
      "[      0 1273344]\n",
      "[      0 1273600]\n",
      "[      0 1273856]\n",
      "[      0 1274112]\n",
      "[      0 1274368]\n",
      "[      0 1274624]\n",
      "[      0 1274880]\n",
      "[      0 1275136]\n",
      "[      0 1275392]\n",
      "[      0 1275648]\n",
      "[      0 1275904]\n",
      "[      0 1276160]\n",
      "[      0 1276416]\n",
      "[      0 1276672]\n",
      "[      0 1276928]\n",
      "[      0 1277184]\n",
      "[      0 1277440]\n",
      "[      0 1277696]\n",
      "[      0 1277952]\n",
      "[      0 1278208]\n",
      "[      0 1278464]\n",
      "[      0 1278720]\n",
      "[      0 1278976]\n",
      "[      0 1279232]\n",
      "[      0 1279488]\n",
      "[      0 1279744]\n",
      "[      0 1280000]\n",
      "[      0 1280256]\n",
      "[      0 1280512]\n",
      "[      0 1280768]\n",
      "[      0 1281024]\n",
      "[      0 1281280]\n",
      "[      0 1281536]\n",
      "[      0 1281792]\n",
      "[      0 1282048]\n",
      "[      0 1282304]\n",
      "[      0 1282560]\n",
      "[      0 1282816]\n",
      "[      0 1283072]\n",
      "[      0 1283328]\n",
      "[      0 1283584]\n",
      "[      0 1283840]\n",
      "[      0 1284096]\n",
      "[      0 1284352]\n",
      "[      0 1284608]\n",
      "[      0 1284864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1285120]\n",
      "[      0 1285376]\n",
      "[      0 1285632]\n",
      "[      0 1285888]\n",
      "[      0 1286144]\n",
      "[      0 1286400]\n",
      "[      0 1286656]\n",
      "[      0 1286912]\n",
      "[      0 1287168]\n",
      "[      0 1287424]\n",
      "[      0 1287680]\n",
      "[      0 1287936]\n",
      "[      0 1288192]\n",
      "[      0 1288448]\n",
      "[      0 1288704]\n",
      "[      0 1288960]\n",
      "[      0 1289216]\n",
      "[      0 1289472]\n",
      "[      0 1289728]\n",
      "[      0 1289984]\n",
      "[      0 1290240]\n",
      "[      0 1290496]\n",
      "[      0 1290752]\n",
      "[      0 1291008]\n",
      "[      0 1291264]\n",
      "[      0 1291520]\n",
      "[      0 1291776]\n",
      "[      0 1292032]\n",
      "[      0 1292288]\n",
      "[      0 1292544]\n",
      "[      0 1292800]\n",
      "[      0 1293056]\n",
      "[      0 1293312]\n",
      "[      0 1293568]\n",
      "[      0 1293824]\n",
      "[      0 1294080]\n",
      "[      0 1294336]\n",
      "[      0 1294592]\n",
      "[      0 1294848]\n",
      "[      0 1295104]\n",
      "[      0 1295360]\n",
      "[      0 1295616]\n",
      "[      0 1295872]\n",
      "[      0 1296128]\n",
      "[      0 1296384]\n",
      "[      0 1296640]\n",
      "[      0 1296896]\n",
      "[      0 1297152]\n",
      "[      0 1297408]\n",
      "[      0 1297664]\n",
      "[      0 1297920]\n",
      "[      0 1298176]\n",
      "[      0 1298432]\n",
      "[      0 1298688]\n",
      "[      0 1298944]\n",
      "[      0 1299200]\n",
      "[      0 1299456]\n",
      "[      0 1299712]\n",
      "[      0 1299968]\n",
      "[      0 1300224]\n",
      "[      0 1300480]\n",
      "[      0 1300736]\n",
      "[      0 1300992]\n",
      "[      0 1301248]\n",
      "[      0 1301504]\n",
      "[      0 1301760]\n",
      "[      0 1302016]\n",
      "[      0 1302272]\n",
      "[      0 1302528]\n",
      "[      0 1302784]\n",
      "[      0 1303040]\n",
      "[      0 1303296]\n",
      "[      0 1303552]\n",
      "[      0 1303808]\n",
      "[      0 1304064]\n",
      "[      0 1304320]\n",
      "[      0 1304576]\n",
      "[      0 1304832]\n",
      "[      0 1305088]\n",
      "[      0 1305344]\n",
      "[      0 1305600]\n",
      "[      0 1305856]\n",
      "[      0 1306112]\n",
      "[      0 1306368]\n",
      "[      0 1306624]\n",
      "[      0 1306880]\n",
      "[      0 1307136]\n",
      "[      0 1307392]\n",
      "[      0 1307648]\n",
      "[      0 1307904]\n",
      "[      0 1308160]\n",
      "[      0 1308416]\n",
      "[      0 1308672]\n",
      "[      0 1308928]\n",
      "[      0 1309184]\n",
      "[      0 1309440]\n",
      "[      0 1309696]\n",
      "[      0 1309952]\n",
      "[      0 1310208]\n",
      "[      0 1310464]\n",
      "[      0 1310720]\n",
      "[      0 1310976]\n",
      "[      0 1311232]\n",
      "[      0 1311488]\n",
      "[      0 1311744]\n",
      "[      0 1312000]\n",
      "[      0 1312256]\n",
      "[      0 1312512]\n",
      "[      0 1312768]\n",
      "[      0 1313024]\n",
      "[      0 1313280]\n",
      "[      0 1313536]\n",
      "[      0 1313792]\n",
      "[      0 1314048]\n",
      "[      0 1314304]\n",
      "[      0 1314560]\n",
      "[      0 1314816]\n",
      "[      0 1315072]\n",
      "[      0 1315328]\n",
      "[      0 1315584]\n",
      "[      0 1315840]\n",
      "[      0 1316096]\n",
      "[      0 1316352]\n",
      "[      0 1316608]\n",
      "[      0 1316864]\n",
      "[      0 1317120]\n",
      "[      0 1317376]\n",
      "[      0 1317632]\n",
      "[      0 1317888]\n",
      "[      0 1318144]\n",
      "[      0 1318400]\n",
      "[      0 1318656]\n",
      "[      0 1318912]\n",
      "[      0 1319168]\n",
      "[      0 1319424]\n",
      "[      0 1319680]\n",
      "[      0 1319936]\n",
      "[      0 1320192]\n",
      "[      0 1320448]\n",
      "[      0 1320704]\n",
      "[      0 1320960]\n",
      "[      0 1321216]\n",
      "[      0 1321472]\n",
      "[      0 1321728]\n",
      "[      0 1321984]\n",
      "[      0 1322240]\n",
      "[      0 1322496]\n",
      "[      0 1322752]\n",
      "[      0 1323008]\n",
      "[      0 1323264]\n",
      "[      0 1323520]\n",
      "[      0 1323776]\n",
      "[      0 1324032]\n",
      "[      0 1324288]\n",
      "[      0 1324544]\n",
      "[      0 1324800]\n",
      "[      0 1325056]\n",
      "[      0 1325312]\n",
      "[      0 1325568]\n",
      "[      0 1325824]\n",
      "[      0 1326080]\n",
      "[      0 1326336]\n",
      "[      0 1326592]\n",
      "[      0 1326848]\n",
      "[      0 1327104]\n",
      "[      0 1327360]\n",
      "[      0 1327616]\n",
      "[      0 1327872]\n",
      "[      0 1328128]\n",
      "[      0 1328384]\n",
      "[      0 1328640]\n",
      "[      0 1328896]\n",
      "[      0 1329152]\n",
      "[      0 1329408]\n",
      "[      0 1329664]\n",
      "[      0 1329920]\n",
      "[      0 1330176]\n",
      "[      0 1330432]\n",
      "[      0 1330688]\n",
      "[      0 1330944]\n",
      "[      0 1331200]\n",
      "[      0 1331456]\n",
      "[      0 1331712]\n",
      "[      0 1331968]\n",
      "[      0 1332224]\n",
      "[      0 1332480]\n",
      "[      0 1332736]\n",
      "[      0 1332992]\n",
      "[      0 1333248]\n",
      "[      0 1333504]\n",
      "[      0 1333760]\n",
      "[      0 1334016]\n",
      "[      0 1334272]\n",
      "[      0 1334528]\n",
      "[      0 1334784]\n",
      "[      0 1335040]\n",
      "[      0 1335296]\n",
      "[      0 1335552]\n",
      "[      0 1335808]\n",
      "[      0 1336064]\n",
      "[      0 1336320]\n",
      "[      0 1336576]\n",
      "[      0 1336832]\n",
      "[      0 1337088]\n",
      "[      0 1337344]\n",
      "[      0 1337600]\n",
      "[      0 1337856]\n",
      "[      0 1338112]\n",
      "[      0 1338368]\n",
      "[      0 1338624]\n",
      "[      0 1338880]\n",
      "[      0 1339136]\n",
      "[      0 1339392]\n",
      "[      0 1339648]\n",
      "[      0 1339904]\n",
      "[      0 1340160]\n",
      "[      0 1340416]\n",
      "[      0 1340672]\n",
      "[      0 1340928]\n",
      "[      0 1341184]\n",
      "[      0 1341440]\n",
      "[      0 1341696]\n",
      "[      0 1341952]\n",
      "[      0 1342208]\n",
      "[      0 1342464]\n",
      "[      0 1342720]\n",
      "[      0 1342976]\n",
      "[      0 1343232]\n",
      "[      0 1343488]\n",
      "[      0 1343744]\n",
      "[      0 1344000]\n",
      "[      0 1344256]\n",
      "[      0 1344512]\n",
      "[      0 1344768]\n",
      "[      0 1345024]\n",
      "[      0 1345280]\n",
      "[      0 1345536]\n",
      "[      0 1345792]\n",
      "[      0 1346048]\n",
      "[      0 1346304]\n",
      "[      0 1346560]\n",
      "[      0 1346816]\n",
      "[      0 1347072]\n",
      "[      0 1347328]\n",
      "[      0 1347584]\n",
      "[      0 1347840]\n",
      "[      0 1348096]\n",
      "[      0 1348352]\n",
      "[      0 1348608]\n",
      "[      0 1348864]\n",
      "[      0 1349120]\n",
      "[      0 1349376]\n",
      "[      0 1349632]\n",
      "[      0 1349888]\n",
      "[      0 1350144]\n",
      "[      0 1350400]\n",
      "[      0 1350656]\n",
      "[      0 1350912]\n",
      "[      0 1351168]\n",
      "[      0 1351424]\n",
      "[      0 1351680]\n",
      "[      0 1351936]\n",
      "[      0 1352192]\n",
      "[      0 1352448]\n",
      "[      0 1352704]\n",
      "[      0 1352960]\n",
      "[      0 1353216]\n",
      "[      0 1353472]\n",
      "[      0 1353728]\n",
      "[      0 1353984]\n",
      "[      0 1354240]\n",
      "[      0 1354496]\n",
      "[      0 1354752]\n",
      "[      0 1355008]\n",
      "[      0 1355264]\n",
      "[      0 1355520]\n",
      "[      0 1355776]\n",
      "[      0 1356032]\n",
      "[      0 1356288]\n",
      "[      0 1356544]\n",
      "[      0 1356800]\n",
      "[      0 1357056]\n",
      "[      0 1357312]\n",
      "[      0 1357568]\n",
      "[      0 1357824]\n",
      "[      0 1358080]\n",
      "[      0 1358336]\n",
      "[      0 1358592]\n",
      "[      0 1358848]\n",
      "[      0 1359104]\n",
      "[      0 1359360]\n",
      "[      0 1359616]\n",
      "[      0 1359872]\n",
      "[      0 1360128]\n",
      "[      0 1360384]\n",
      "[      0 1360640]\n",
      "[      0 1360896]\n",
      "[      0 1361152]\n",
      "[      0 1361408]\n",
      "[      0 1361664]\n",
      "[      0 1361920]\n",
      "[      0 1362176]\n",
      "[      0 1362432]\n",
      "[      0 1362688]\n",
      "[      0 1362944]\n",
      "[      0 1363200]\n",
      "[      0 1363456]\n",
      "[      0 1363712]\n",
      "[      0 1363968]\n",
      "[      0 1364224]\n",
      "[      0 1364480]\n",
      "[      0 1364736]\n",
      "[      0 1364992]\n",
      "[      0 1365248]\n",
      "[      0 1365504]\n",
      "[      0 1365760]\n",
      "[      0 1366016]\n",
      "[      0 1366272]\n",
      "[      0 1366528]\n",
      "[      0 1366784]\n",
      "[      0 1367040]\n",
      "[      0 1367296]\n",
      "[      0 1367552]\n",
      "[      0 1367808]\n",
      "[      0 1368064]\n",
      "[      0 1368320]\n",
      "[      0 1368576]\n",
      "[      0 1368832]\n",
      "[      0 1369088]\n",
      "[      0 1369344]\n",
      "[      0 1369600]\n",
      "[      0 1369856]\n",
      "[      0 1370112]\n",
      "[      0 1370368]\n",
      "[      0 1370624]\n",
      "[      0 1370880]\n",
      "[      0 1371136]\n",
      "[      0 1371392]\n",
      "[      0 1371648]\n",
      "[      0 1371904]\n",
      "[      0 1372160]\n",
      "[      0 1372416]\n",
      "[      0 1372672]\n",
      "[      0 1372928]\n",
      "[      0 1373184]\n",
      "[      0 1373440]\n",
      "[      0 1373696]\n",
      "[      0 1373952]\n",
      "[      0 1374208]\n",
      "[      0 1374464]\n",
      "[      0 1374720]\n",
      "[      0 1374976]\n",
      "[      0 1375232]\n",
      "[      0 1375488]\n",
      "[      0 1375744]\n",
      "[      0 1376000]\n",
      "[      0 1376256]\n",
      "[      0 1376512]\n",
      "[      0 1376768]\n",
      "[      0 1377024]\n",
      "[      0 1377280]\n",
      "[      0 1377536]\n",
      "[      0 1377792]\n",
      "[      0 1378048]\n",
      "[      0 1378304]\n",
      "[      0 1378560]\n",
      "[      0 1378816]\n",
      "[      0 1379072]\n",
      "[      0 1379328]\n",
      "[      0 1379584]\n",
      "[      0 1379840]\n",
      "[      0 1380096]\n",
      "[      0 1380352]\n",
      "[      0 1380608]\n",
      "[      0 1380864]\n",
      "[      0 1381120]\n",
      "[      0 1381376]\n",
      "[      0 1381632]\n",
      "[      0 1381888]\n",
      "[      0 1382144]\n",
      "[      0 1382400]\n",
      "[      0 1382656]\n",
      "[      0 1382912]\n",
      "[      0 1383168]\n",
      "[      0 1383424]\n",
      "[      0 1383680]\n",
      "[      0 1383936]\n",
      "[      0 1384192]\n",
      "[      0 1384448]\n",
      "[      0 1384704]\n",
      "[      0 1384960]\n",
      "[      0 1385216]\n",
      "[      0 1385472]\n",
      "[      0 1385728]\n",
      "[      0 1385984]\n",
      "[      0 1386240]\n",
      "[      0 1386496]\n",
      "[      0 1386752]\n",
      "[      0 1387008]\n",
      "[      0 1387264]\n",
      "[      0 1387520]\n",
      "[      0 1387776]\n",
      "[      0 1388032]\n",
      "[      0 1388288]\n",
      "[      0 1388544]\n",
      "[      0 1388800]\n",
      "[      0 1389056]\n",
      "[      0 1389312]\n",
      "[      0 1389568]\n",
      "[      0 1389824]\n",
      "[      0 1390080]\n",
      "[      0 1390336]\n",
      "[      0 1390592]\n",
      "[      0 1390848]\n",
      "[      0 1391104]\n",
      "[      0 1391360]\n",
      "[      0 1391616]\n",
      "[      0 1391872]\n",
      "[      0 1392128]\n",
      "[      0 1392384]\n",
      "[      0 1392640]\n",
      "[      0 1392896]\n",
      "[      0 1393152]\n",
      "[      0 1393408]\n",
      "[      0 1393664]\n",
      "[      0 1393920]\n",
      "[      0 1394176]\n",
      "[      0 1394432]\n",
      "[      0 1394688]\n",
      "[      0 1394944]\n",
      "[      0 1395200]\n",
      "[      0 1395456]\n",
      "[      0 1395712]\n",
      "[      0 1395968]\n",
      "[      0 1396224]\n",
      "[      0 1396480]\n",
      "[      0 1396736]\n",
      "[      0 1396992]\n",
      "[      0 1397248]\n",
      "[      0 1397504]\n",
      "[      0 1397760]\n",
      "[      0 1398016]\n",
      "[      0 1398272]\n",
      "[      0 1398528]\n",
      "[      0 1398784]\n",
      "[      0 1399040]\n",
      "[      0 1399296]\n",
      "[      0 1399552]\n",
      "[      0 1399808]\n",
      "[      0 1400064]\n",
      "[      0 1400320]\n",
      "[      0 1400576]\n",
      "[      0 1400832]\n",
      "[      0 1401088]\n",
      "[      0 1401344]\n",
      "[      0 1401600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1401856]\n",
      "[      0 1402112]\n",
      "[      0 1402368]\n",
      "[      0 1402624]\n",
      "[      0 1402880]\n",
      "[      0 1403136]\n",
      "[      0 1403392]\n",
      "[      0 1403648]\n",
      "[      0 1403904]\n",
      "[      0 1404160]\n",
      "[      0 1404416]\n",
      "[      0 1404672]\n",
      "[      0 1404928]\n",
      "[      0 1405184]\n",
      "[      0 1405440]\n",
      "[      0 1405696]\n",
      "[      0 1405952]\n",
      "[      0 1406208]\n",
      "[      0 1406464]\n",
      "[      0 1406720]\n",
      "[      0 1406976]\n",
      "[      0 1407232]\n",
      "[      0 1407488]\n",
      "[      0 1407744]\n",
      "[      0 1408000]\n",
      "[      0 1408256]\n",
      "[      0 1408512]\n",
      "[      0 1408768]\n",
      "[      0 1409024]\n",
      "[      0 1409280]\n",
      "[      0 1409536]\n",
      "[      0 1409792]\n",
      "[      0 1410048]\n",
      "[      0 1410304]\n",
      "[      0 1410560]\n",
      "[      0 1410816]\n",
      "[      0 1411072]\n",
      "[      0 1411328]\n",
      "[      0 1411584]\n",
      "[      0 1411840]\n",
      "[      0 1412096]\n",
      "[      0 1412352]\n",
      "[      0 1412608]\n",
      "[      0 1412864]\n",
      "[      0 1413120]\n",
      "[      0 1413376]\n",
      "[      0 1413632]\n",
      "[      0 1413888]\n",
      "[      0 1414144]\n",
      "[      0 1414400]\n",
      "[      0 1414656]\n",
      "[      0 1414912]\n",
      "[      0 1415168]\n",
      "[      0 1415424]\n",
      "[      0 1415680]\n",
      "[      0 1415936]\n",
      "[      0 1416192]\n",
      "[      0 1416448]\n",
      "[      0 1416704]\n",
      "[      0 1416960]\n",
      "[      0 1417216]\n",
      "[      0 1417472]\n",
      "[      0 1417728]\n",
      "[      0 1417984]\n",
      "[      0 1418240]\n",
      "[      0 1418496]\n",
      "[      0 1418752]\n",
      "[      0 1419008]\n",
      "[      0 1419264]\n",
      "[      0 1419520]\n",
      "[      0 1419776]\n",
      "[      0 1420032]\n",
      "[      0 1420288]\n",
      "[      0 1420544]\n",
      "[      0 1420800]\n",
      "[      0 1421056]\n",
      "[      0 1421312]\n",
      "[      0 1421568]\n",
      "[      0 1421824]\n",
      "[      0 1422080]\n",
      "[      0 1422336]\n",
      "[      0 1422592]\n",
      "[      0 1422848]\n",
      "[      0 1423104]\n",
      "[      0 1423360]\n",
      "[      0 1423616]\n",
      "[      0 1423872]\n",
      "[      0 1424128]\n",
      "[      0 1424384]\n",
      "[      0 1424640]\n",
      "[      0 1424896]\n",
      "[      0 1425152]\n",
      "[      0 1425408]\n",
      "[      0 1425664]\n",
      "[      0 1425920]\n",
      "[      0 1426176]\n",
      "[      0 1426432]\n",
      "[      0 1426688]\n",
      "[      0 1426944]\n",
      "[      0 1427200]\n",
      "[      0 1427456]\n",
      "[      0 1427712]\n",
      "[      0 1427968]\n",
      "[      0 1428224]\n",
      "[      0 1428480]\n",
      "[      0 1428736]\n",
      "[      0 1428992]\n",
      "[      0 1429248]\n",
      "[      0 1429504]\n",
      "[      0 1429760]\n",
      "[      0 1430016]\n",
      "[      0 1430272]\n",
      "[      0 1430528]\n",
      "[      0 1430784]\n",
      "[      0 1431040]\n",
      "[      0 1431296]\n",
      "[      0 1431552]\n",
      "[      0 1431808]\n",
      "[      0 1432064]\n",
      "[      0 1432320]\n",
      "[      0 1432576]\n",
      "[      0 1432832]\n",
      "[      0 1433088]\n",
      "[      0 1433344]\n",
      "[      0 1433600]\n",
      "[      0 1433856]\n",
      "[      0 1434112]\n",
      "[      0 1434368]\n",
      "[      0 1434624]\n",
      "[      0 1434880]\n",
      "[      0 1435136]\n",
      "[      0 1435392]\n",
      "[      0 1435648]\n",
      "[      0 1435904]\n",
      "[      0 1436160]\n",
      "[      0 1436416]\n",
      "[      0 1436672]\n",
      "[      0 1436928]\n",
      "[      0 1437184]\n",
      "[      0 1437440]\n",
      "[      0 1437696]\n",
      "[      0 1437952]\n",
      "[      0 1438208]\n",
      "[      0 1438464]\n",
      "[      0 1438720]\n",
      "[      0 1438976]\n",
      "[      0 1439232]\n",
      "[      0 1439488]\n",
      "[      0 1439744]\n",
      "[      0 1440000]\n",
      "[      0 1440256]\n",
      "[      0 1440512]\n",
      "[      0 1440768]\n",
      "[      0 1441024]\n",
      "[      0 1441280]\n",
      "[      0 1441536]\n",
      "[      0 1441792]\n",
      "[      0 1442048]\n",
      "[      0 1442304]\n",
      "[      0 1442560]\n",
      "[      0 1442816]\n",
      "[      0 1443072]\n",
      "[      0 1443328]\n",
      "[      0 1443584]\n",
      "[      0 1443840]\n",
      "[      0 1444096]\n",
      "[      0 1444352]\n",
      "[      0 1444608]\n",
      "[      0 1444864]\n",
      "[      0 1445120]\n",
      "[      0 1445376]\n",
      "[      0 1445632]\n",
      "[      0 1445888]\n",
      "[      0 1446144]\n",
      "[      0 1446400]\n",
      "[      0 1446656]\n",
      "[      0 1446912]\n",
      "[      0 1447168]\n",
      "[      0 1447424]\n",
      "[      0 1447680]\n",
      "[      0 1447936]\n",
      "[      0 1448192]\n",
      "[      0 1448448]\n",
      "[      0 1448704]\n",
      "[      0 1448960]\n",
      "[      0 1449216]\n",
      "[      0 1449472]\n",
      "[      0 1449728]\n",
      "[      0 1449984]\n",
      "[      0 1450240]\n",
      "[      0 1450496]\n",
      "[      0 1450752]\n",
      "[      0 1451008]\n",
      "[      0 1451264]\n",
      "[      0 1451520]\n",
      "[      0 1451776]\n",
      "[      0 1452032]\n",
      "[      0 1452288]\n",
      "[      0 1452544]\n",
      "[      0 1452800]\n",
      "[      0 1453056]\n",
      "[      0 1453312]\n",
      "[      0 1453568]\n",
      "[      0 1453824]\n",
      "[      0 1454080]\n",
      "[      0 1454336]\n",
      "[      0 1454592]\n",
      "[      0 1454848]\n",
      "[      0 1455104]\n",
      "[      0 1455360]\n",
      "[      0 1455616]\n",
      "[      0 1455872]\n",
      "[      0 1456128]\n",
      "[      0 1456384]\n",
      "[      0 1456640]\n",
      "[      0 1456896]\n",
      "[      0 1457152]\n",
      "[      0 1457408]\n",
      "[      0 1457664]\n",
      "[      0 1457920]\n",
      "[      0 1458176]\n",
      "[      0 1458432]\n",
      "[      0 1458688]\n",
      "[      0 1458944]\n",
      "[      0 1459200]\n",
      "[      0 1459456]\n",
      "[      0 1459712]\n",
      "[      0 1459968]\n",
      "[      0 1460224]\n",
      "[      0 1460480]\n",
      "[      0 1460736]\n",
      "[      0 1460992]\n",
      "[      0 1461248]\n",
      "[      0 1461504]\n",
      "[      0 1461760]\n",
      "[      0 1462016]\n",
      "[      0 1462272]\n",
      "[      0 1462528]\n",
      "[      0 1462784]\n",
      "[      0 1463040]\n",
      "[      0 1463296]\n",
      "[      0 1463552]\n",
      "[      0 1463808]\n",
      "[      0 1464064]\n",
      "[      0 1464320]\n",
      "[      0 1464576]\n",
      "[      0 1464832]\n",
      "[      0 1465088]\n",
      "[      0 1465344]\n",
      "[      0 1465600]\n",
      "[      0 1465856]\n",
      "[      0 1466112]\n",
      "[      0 1466368]\n",
      "[      0 1466624]\n",
      "[      0 1466880]\n",
      "[      0 1467136]\n",
      "[      0 1467392]\n",
      "[      0 1467648]\n",
      "[      0 1467904]\n",
      "[      0 1468160]\n",
      "[      0 1468416]\n",
      "[      0 1468672]\n",
      "[      0 1468928]\n",
      "[      0 1469184]\n",
      "[      0 1469440]\n",
      "[      0 1469696]\n",
      "[      0 1469952]\n",
      "[      0 1470208]\n",
      "[      0 1470464]\n",
      "[      0 1470720]\n",
      "[      0 1470976]\n",
      "[      0 1471232]\n",
      "[      0 1471488]\n",
      "[      0 1471744]\n",
      "[      0 1472000]\n",
      "[      0 1472256]\n",
      "[      0 1472512]\n",
      "[      0 1472768]\n",
      "[      0 1473024]\n",
      "[      0 1473280]\n",
      "[      0 1473536]\n",
      "[      0 1473792]\n",
      "[      0 1474048]\n",
      "[      0 1474304]\n",
      "[      0 1474560]\n",
      "[      0 1474816]\n",
      "[      0 1475072]\n",
      "[      0 1475328]\n",
      "[      0 1475584]\n",
      "[      0 1475840]\n",
      "[      0 1476096]\n",
      "[      0 1476352]\n",
      "[      0 1476608]\n",
      "[      0 1476864]\n",
      "[      0 1477120]\n",
      "[      0 1477376]\n",
      "[      0 1477632]\n",
      "[      0 1477888]\n",
      "[      0 1478144]\n",
      "[      0 1478400]\n",
      "[      0 1478656]\n",
      "[      0 1478912]\n",
      "[      0 1479168]\n",
      "[      0 1479424]\n",
      "[      0 1479680]\n",
      "[      0 1479936]\n",
      "[      0 1480192]\n",
      "[      0 1480448]\n",
      "[      0 1480704]\n",
      "[      0 1480960]\n",
      "[      0 1481216]\n",
      "[      0 1481472]\n",
      "[      0 1481728]\n",
      "[      0 1481984]\n",
      "[      0 1482240]\n",
      "[      0 1482496]\n",
      "[      0 1482752]\n",
      "[      0 1483008]\n",
      "[      0 1483264]\n",
      "[      0 1483520]\n",
      "[      0 1483776]\n",
      "[      0 1484032]\n",
      "[      0 1484288]\n",
      "[      0 1484544]\n",
      "[      0 1484800]\n",
      "[      0 1485056]\n",
      "[      0 1485312]\n",
      "[      0 1485568]\n",
      "[      0 1485824]\n",
      "[      0 1486080]\n",
      "[      0 1486336]\n",
      "[      0 1486592]\n",
      "[      0 1486848]\n",
      "[      0 1487104]\n",
      "[      0 1487360]\n",
      "[      0 1487616]\n",
      "[      0 1487872]\n",
      "[      0 1488128]\n",
      "[      0 1488384]\n",
      "[      0 1488640]\n",
      "[      0 1488896]\n",
      "[      0 1489152]\n",
      "[      0 1489408]\n",
      "[      0 1489664]\n",
      "[      0 1489920]\n",
      "[      0 1490176]\n",
      "[      0 1490432]\n",
      "[      0 1490688]\n",
      "[      0 1490944]\n",
      "[      0 1491200]\n",
      "[      0 1491456]\n",
      "[      0 1491712]\n",
      "[      0 1491968]\n",
      "[      0 1492224]\n",
      "[      0 1492480]\n",
      "[      0 1492736]\n",
      "[      0 1492992]\n",
      "[      0 1493248]\n",
      "[      0 1493504]\n",
      "[      0 1493760]\n",
      "[      0 1494016]\n",
      "[      0 1494272]\n",
      "[      0 1494528]\n",
      "[      0 1494784]\n",
      "[      0 1495040]\n",
      "[      0 1495296]\n",
      "[      0 1495552]\n",
      "[      0 1495808]\n",
      "[      0 1496064]\n",
      "[      0 1496320]\n",
      "[      0 1496576]\n",
      "[      0 1496832]\n",
      "[      0 1497088]\n",
      "[      0 1497344]\n",
      "[      0 1497600]\n",
      "[      0 1497856]\n",
      "[      0 1498112]\n",
      "[      0 1498368]\n",
      "[      0 1498624]\n",
      "[      0 1498880]\n",
      "[      0 1499136]\n",
      "[      0 1499392]\n",
      "[      0 1499648]\n",
      "[      0 1499904]\n",
      "[      0 1500160]\n",
      "[      0 1500416]\n",
      "[      0 1500672]\n",
      "[      0 1500928]\n",
      "[      0 1501184]\n",
      "[      0 1501440]\n",
      "[      0 1501696]\n",
      "[      0 1501952]\n",
      "[      0 1502208]\n",
      "[      0 1502464]\n",
      "[      0 1502720]\n",
      "[      0 1502976]\n",
      "[      0 1503232]\n",
      "[      0 1503488]\n",
      "[      0 1503744]\n",
      "[      0 1504000]\n",
      "[      0 1504256]\n",
      "[      0 1504512]\n",
      "[      0 1504768]\n",
      "[      0 1505024]\n",
      "[      0 1505280]\n",
      "[      0 1505536]\n",
      "[      0 1505792]\n",
      "[      0 1506048]\n",
      "[      0 1506304]\n",
      "[      0 1506560]\n",
      "[      0 1506816]\n",
      "[      0 1507072]\n",
      "[      0 1507328]\n",
      "[      0 1507584]\n",
      "[      0 1507840]\n",
      "[      0 1508096]\n",
      "[      0 1508352]\n",
      "[      0 1508608]\n",
      "[      0 1508864]\n",
      "[      0 1509120]\n",
      "[      0 1509376]\n",
      "[      0 1509632]\n",
      "[      0 1509888]\n",
      "[      0 1510144]\n",
      "[      0 1510400]\n",
      "[      0 1510656]\n",
      "[      0 1510912]\n",
      "[      0 1511168]\n",
      "[      0 1511424]\n",
      "[      0 1511680]\n",
      "[      0 1511936]\n",
      "[      0 1512192]\n",
      "[      0 1512448]\n",
      "[      0 1512704]\n",
      "[      0 1512960]\n",
      "[      0 1513216]\n",
      "[      0 1513472]\n",
      "[      0 1513728]\n",
      "[      0 1513984]\n",
      "[      0 1514240]\n",
      "[      0 1514496]\n",
      "[      0 1514752]\n",
      "[      0 1515008]\n",
      "[      0 1515264]\n",
      "[      0 1515520]\n",
      "[      0 1515776]\n",
      "[      0 1516032]\n",
      "[      0 1516288]\n",
      "[      0 1516544]\n",
      "[      0 1516800]\n",
      "[      0 1517056]\n",
      "[      0 1517312]\n",
      "[      0 1517568]\n",
      "[      0 1517824]\n",
      "[      0 1518080]\n",
      "[      0 1518336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1518592]\n",
      "[      0 1518848]\n",
      "[      0 1519104]\n",
      "[      0 1519360]\n",
      "[      0 1519616]\n",
      "[      0 1519872]\n",
      "[      0 1520128]\n",
      "[      0 1520384]\n",
      "[      0 1520640]\n",
      "[      0 1520896]\n",
      "[      0 1521152]\n",
      "[      0 1521408]\n",
      "[      0 1521664]\n",
      "[      0 1521920]\n",
      "[      0 1522176]\n",
      "[      0 1522432]\n",
      "[      0 1522688]\n",
      "[      0 1522944]\n",
      "[      0 1523200]\n",
      "[      0 1523456]\n",
      "[      0 1523712]\n",
      "[      0 1523968]\n",
      "[      0 1524224]\n",
      "[      0 1524480]\n",
      "[      0 1524736]\n",
      "[      0 1524992]\n",
      "[      0 1525248]\n",
      "[      0 1525504]\n",
      "[      0 1525760]\n",
      "[      0 1526016]\n",
      "[      0 1526272]\n",
      "[      0 1526528]\n",
      "[      0 1526784]\n",
      "[      0 1527040]\n",
      "[      0 1527296]\n",
      "[      0 1527552]\n",
      "[      0 1527808]\n",
      "[      0 1528064]\n",
      "[      0 1528320]\n",
      "[      0 1528576]\n",
      "[      0 1528832]\n",
      "[      0 1529088]\n",
      "[      0 1529344]\n",
      "[      0 1529600]\n",
      "[      0 1529856]\n",
      "[      0 1530112]\n",
      "[      0 1530368]\n",
      "[      0 1530624]\n",
      "[      0 1530880]\n",
      "[      0 1531136]\n",
      "[      0 1531392]\n",
      "[      0 1531648]\n",
      "[      0 1531904]\n",
      "[      0 1532160]\n",
      "[      0 1532416]\n",
      "[      0 1532672]\n",
      "[      0 1532928]\n",
      "[      0 1533184]\n",
      "[      0 1533440]\n",
      "[      0 1533696]\n",
      "[      0 1533952]\n",
      "[      0 1534208]\n",
      "[      0 1534464]\n",
      "[      0 1534720]\n",
      "[      0 1534976]\n",
      "[      0 1535232]\n",
      "[      0 1535488]\n",
      "[      0 1535744]\n",
      "[      0 1536000]\n",
      "[      0 1536256]\n",
      "[      0 1536512]\n",
      "[      0 1536768]\n",
      "[      0 1537024]\n",
      "[      0 1537280]\n",
      "[      0 1537536]\n",
      "[      0 1537792]\n",
      "[      0 1538048]\n",
      "[      0 1538304]\n",
      "[      0 1538560]\n",
      "[      0 1538816]\n",
      "[      0 1539072]\n",
      "[      0 1539328]\n",
      "[      0 1539584]\n",
      "[      0 1539840]\n",
      "[      0 1540096]\n",
      "[      0 1540352]\n",
      "[      0 1540608]\n",
      "[      0 1540864]\n",
      "[      0 1541120]\n",
      "[      0 1541376]\n",
      "[      0 1541632]\n",
      "[      0 1541888]\n",
      "[      0 1542144]\n",
      "[      0 1542400]\n",
      "[      0 1542656]\n",
      "[      0 1542912]\n",
      "[      0 1543168]\n",
      "[      0 1543424]\n",
      "[      0 1543680]\n",
      "[      0 1543936]\n",
      "[      0 1544192]\n",
      "[      0 1544448]\n",
      "[      0 1544704]\n",
      "[      0 1544960]\n",
      "[      0 1545216]\n",
      "[      0 1545472]\n",
      "[      0 1545728]\n",
      "[      0 1545984]\n",
      "[      0 1546240]\n",
      "[      0 1546496]\n",
      "[      0 1546752]\n",
      "[      0 1547008]\n",
      "[      0 1547264]\n",
      "[      0 1547520]\n",
      "[      0 1547776]\n",
      "[      0 1548032]\n",
      "[      0 1548288]\n",
      "[      0 1548544]\n",
      "[      0 1548800]\n",
      "[      0 1549056]\n",
      "[      0 1549312]\n",
      "[      0 1549568]\n",
      "[      0 1549824]\n",
      "[      0 1550080]\n",
      "[      0 1550336]\n",
      "[      0 1550592]\n",
      "[      0 1550848]\n",
      "[      0 1551104]\n",
      "[      0 1551360]\n",
      "[      0 1551616]\n",
      "[      0 1551872]\n",
      "[      0 1552128]\n",
      "[      0 1552384]\n",
      "[      0 1552640]\n",
      "[      0 1552896]\n",
      "[      0 1553152]\n",
      "[      0 1553408]\n",
      "[      0 1553664]\n",
      "[      0 1553920]\n",
      "[      0 1554176]\n",
      "[      0 1554432]\n",
      "[      0 1554688]\n",
      "[      0 1554944]\n",
      "[      0 1555200]\n",
      "[      0 1555456]\n",
      "[      0 1555712]\n",
      "[      0 1555968]\n",
      "[      0 1556224]\n",
      "[      0 1556480]\n",
      "[      0 1556736]\n",
      "[      0 1556992]\n",
      "[      0 1557248]\n",
      "[      0 1557504]\n",
      "[      0 1557760]\n",
      "[      0 1558016]\n",
      "[      0 1558272]\n",
      "[      0 1558528]\n",
      "[      0 1558784]\n",
      "[      0 1559040]\n",
      "[      0 1559296]\n",
      "[      0 1559552]\n",
      "[      0 1559808]\n",
      "[      0 1560064]\n",
      "[      0 1560320]\n",
      "[      0 1560576]\n",
      "[      0 1560832]\n",
      "[      0 1561088]\n",
      "[      0 1561344]\n",
      "[      0 1561600]\n",
      "[      0 1561856]\n",
      "[      0 1562112]\n",
      "[      0 1562368]\n",
      "[      0 1562624]\n",
      "[      0 1562880]\n",
      "[      0 1563136]\n",
      "[      0 1563392]\n",
      "[      0 1563648]\n",
      "[      0 1563904]\n",
      "[      0 1564160]\n",
      "[      0 1564416]\n",
      "[      0 1564672]\n",
      "[      0 1564928]\n",
      "[      0 1565184]\n",
      "[      0 1565440]\n",
      "[      0 1565696]\n",
      "[      0 1565952]\n",
      "[      0 1566208]\n",
      "[      0 1566464]\n",
      "[      0 1566720]\n",
      "[      0 1566976]\n",
      "[      0 1567232]\n",
      "[      0 1567488]\n",
      "[      0 1567744]\n",
      "[      0 1568000]\n",
      "[      0 1568256]\n",
      "[      0 1568512]\n",
      "[      0 1568768]\n",
      "[      0 1569024]\n",
      "[      0 1569280]\n",
      "[      0 1569536]\n",
      "[      0 1569792]\n",
      "[      0 1570048]\n",
      "[      0 1570304]\n",
      "[      0 1570560]\n",
      "[      0 1570816]\n",
      "[      0 1571072]\n",
      "[      0 1571328]\n",
      "[      0 1571584]\n",
      "[      0 1571840]\n",
      "[      0 1572096]\n",
      "[      0 1572352]\n",
      "[      0 1572608]\n",
      "[      0 1572864]\n",
      "[      0 1573120]\n",
      "[      0 1573376]\n",
      "[      0 1573632]\n",
      "[      0 1573888]\n",
      "[      0 1574144]\n",
      "[      0 1574400]\n",
      "[      0 1574656]\n",
      "[      0 1574912]\n",
      "[      0 1575168]\n",
      "[      0 1575424]\n",
      "[      0 1575680]\n",
      "[      0 1575936]\n",
      "[      0 1576192]\n",
      "[      0 1576448]\n",
      "[      0 1576704]\n",
      "[      0 1576960]\n",
      "[      0 1577216]\n",
      "[      0 1577472]\n",
      "[      0 1577728]\n",
      "[      0 1577984]\n",
      "[      0 1578240]\n",
      "[      0 1578496]\n",
      "[      0 1578752]\n",
      "[      0 1579008]\n",
      "[      0 1579264]\n",
      "[      0 1579520]\n",
      "[      0 1579776]\n",
      "[      0 1580032]\n",
      "[      0 1580288]\n",
      "[      0 1580544]\n",
      "[      0 1580800]\n",
      "[      0 1581056]\n",
      "[      0 1581312]\n",
      "[      0 1581568]\n",
      "[      0 1581824]\n",
      "[      0 1582080]\n",
      "[      0 1582336]\n",
      "[      0 1582592]\n",
      "[      0 1582848]\n",
      "[      0 1583104]\n",
      "[      0 1583360]\n",
      "[      0 1583616]\n",
      "[      0 1583872]\n",
      "[      0 1584128]\n",
      "[      0 1584384]\n",
      "[      0 1584640]\n",
      "[      0 1584896]\n",
      "[      0 1585152]\n",
      "[      0 1585408]\n",
      "[      0 1585664]\n",
      "[      0 1585920]\n",
      "[      0 1586176]\n",
      "[      0 1586432]\n",
      "[      0 1586688]\n",
      "[      0 1586944]\n",
      "[      0 1587200]\n",
      "[      0 1587456]\n",
      "[      0 1587712]\n",
      "[      0 1587968]\n",
      "[      0 1588224]\n",
      "[      0 1588480]\n",
      "[      0 1588736]\n",
      "[      0 1588992]\n",
      "[      0 1589248]\n",
      "[      0 1589504]\n",
      "[      0 1589760]\n",
      "[      0 1590016]\n",
      "[      0 1590272]\n",
      "[      0 1590528]\n",
      "[      0 1590784]\n",
      "[      0 1591040]\n",
      "[      0 1591296]\n",
      "[      0 1591552]\n",
      "[      0 1591808]\n",
      "[      0 1592064]\n",
      "[      0 1592320]\n",
      "[      0 1592576]\n",
      "[      0 1592832]\n",
      "[      0 1593088]\n",
      "[      0 1593344]\n",
      "[      0 1593600]\n",
      "[      0 1593856]\n",
      "[      0 1594112]\n",
      "[      0 1594368]\n",
      "[      0 1594624]\n",
      "[      0 1594880]\n",
      "[      0 1595136]\n",
      "[      0 1595392]\n",
      "[      0 1595648]\n",
      "[      0 1595904]\n",
      "[      0 1596160]\n",
      "[      0 1596416]\n",
      "[      0 1596672]\n",
      "[      0 1596928]\n",
      "[      0 1597184]\n",
      "[      0 1597440]\n",
      "[      0 1597696]\n",
      "[      0 1597952]\n",
      "[      0 1598208]\n",
      "[      0 1598464]\n",
      "[      0 1598720]\n",
      "[      0 1598976]\n",
      "[      0 1599232]\n",
      "[      0 1599488]\n",
      "[      0 1599744]\n",
      "[      0 1600000]\n",
      "[      0 1600256]\n",
      "[      0 1600512]\n",
      "[      0 1600768]\n",
      "[      0 1601024]\n",
      "[      0 1601280]\n",
      "[      0 1601536]\n",
      "[      0 1601792]\n",
      "[      0 1602048]\n",
      "[      0 1602304]\n",
      "[      0 1602560]\n",
      "[      0 1602816]\n",
      "[      0 1603072]\n",
      "[      0 1603328]\n",
      "[      0 1603584]\n",
      "[      0 1603840]\n",
      "[      0 1604096]\n",
      "[      0 1604352]\n",
      "[      0 1604608]\n",
      "[      0 1604864]\n",
      "[      0 1605120]\n",
      "[      0 1605376]\n",
      "[      0 1605632]\n",
      "[      0 1605888]\n",
      "[      0 1606144]\n",
      "[      0 1606400]\n",
      "[      0 1606656]\n",
      "[      0 1606912]\n",
      "[      0 1607168]\n",
      "[      0 1607424]\n",
      "[      0 1607680]\n",
      "[      0 1607936]\n",
      "[      0 1608192]\n",
      "[      0 1608448]\n",
      "[      0 1608704]\n",
      "[      0 1608960]\n",
      "[      0 1609216]\n",
      "[      0 1609472]\n",
      "[      0 1609728]\n",
      "[      0 1609984]\n",
      "[      0 1610240]\n",
      "[      0 1610496]\n",
      "[      0 1610752]\n",
      "[      0 1611008]\n",
      "[      0 1611264]\n",
      "[      0 1611520]\n",
      "[      0 1611776]\n",
      "[      0 1612032]\n",
      "[      0 1612288]\n",
      "[      0 1612544]\n",
      "[      0 1612800]\n",
      "[      0 1613056]\n",
      "[      0 1613312]\n",
      "[      0 1613568]\n",
      "[      0 1613824]\n",
      "[      0 1614080]\n",
      "[      0 1614336]\n",
      "[      0 1614592]\n",
      "[      0 1614848]\n",
      "[      0 1615104]\n",
      "[      0 1615360]\n",
      "[      0 1615616]\n",
      "[      0 1615872]\n",
      "[      0 1616128]\n",
      "[      0 1616384]\n",
      "[      0 1616640]\n",
      "[      0 1616896]\n",
      "[      0 1617152]\n",
      "[      0 1617408]\n",
      "[      0 1617664]\n",
      "[      0 1617920]\n",
      "[      0 1618176]\n",
      "[      0 1618432]\n",
      "[      0 1618688]\n",
      "[      0 1618944]\n",
      "[      0 1619200]\n",
      "[      0 1619456]\n",
      "[      0 1619712]\n",
      "[      0 1619968]\n",
      "[      0 1620224]\n",
      "[      0 1620480]\n",
      "[      0 1620736]\n",
      "[      0 1620992]\n",
      "[      0 1621248]\n",
      "[      0 1621504]\n",
      "[      0 1621760]\n",
      "[      0 1622016]\n",
      "[      0 1622272]\n",
      "[      0 1622528]\n",
      "[      0 1622784]\n",
      "[      0 1623040]\n",
      "[      0 1623296]\n",
      "[      0 1623552]\n",
      "[      0 1623808]\n",
      "[      0 1624064]\n",
      "[      0 1624320]\n",
      "[      0 1624576]\n",
      "[      0 1624832]\n",
      "[      0 1625088]\n",
      "[      0 1625344]\n",
      "[      0 1625600]\n",
      "[      0 1625856]\n",
      "[      0 1626112]\n",
      "[      0 1626368]\n",
      "[      0 1626624]\n",
      "[      0 1626880]\n",
      "[      0 1627136]\n",
      "[      0 1627392]\n",
      "[      0 1627648]\n",
      "[      0 1627904]\n",
      "[      0 1628160]\n",
      "[      0 1628416]\n",
      "[      0 1628672]\n",
      "[      0 1628928]\n",
      "[      0 1629184]\n",
      "[      0 1629440]\n",
      "[      0 1629696]\n",
      "[      0 1629952]\n",
      "[      0 1630208]\n",
      "[      0 1630464]\n",
      "[      0 1630720]\n",
      "[      0 1630976]\n",
      "[      0 1631232]\n",
      "[      0 1631488]\n",
      "[      0 1631744]\n",
      "[      0 1632000]\n",
      "[      0 1632256]\n",
      "[      0 1632512]\n",
      "[      0 1632768]\n",
      "[      0 1633024]\n",
      "[      0 1633280]\n",
      "[      0 1633536]\n",
      "[      0 1633792]\n",
      "[      0 1634048]\n",
      "[      0 1634304]\n",
      "[      0 1634560]\n",
      "[      0 1634816]\n",
      "[      0 1635072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1635328]\n",
      "[      0 1635584]\n",
      "[      0 1635840]\n",
      "[      0 1636096]\n",
      "[      0 1636352]\n",
      "[      0 1636608]\n",
      "[      0 1636864]\n",
      "[      0 1637120]\n",
      "[      0 1637376]\n",
      "[      0 1637632]\n",
      "[      0 1637888]\n",
      "[      0 1638144]\n",
      "[      0 1638400]\n",
      "[      0 1638656]\n",
      "[      0 1638912]\n",
      "[      0 1639168]\n",
      "[      0 1639424]\n",
      "[      0 1639680]\n",
      "[      0 1639936]\n",
      "[      0 1640192]\n",
      "[      0 1640448]\n",
      "[      0 1640704]\n",
      "[      0 1640960]\n",
      "[      0 1641216]\n",
      "[      0 1641472]\n",
      "[      0 1641728]\n",
      "[      0 1641984]\n",
      "[      0 1642240]\n",
      "[      0 1642496]\n",
      "[      0 1642752]\n",
      "[      0 1643008]\n",
      "[      0 1643264]\n",
      "[      0 1643520]\n",
      "[      0 1643776]\n",
      "[      0 1644032]\n",
      "[      0 1644288]\n",
      "[      0 1644544]\n",
      "[      0 1644800]\n",
      "[      0 1645056]\n",
      "[      0 1645312]\n",
      "[      0 1645568]\n",
      "[      0 1645824]\n",
      "[      0 1646080]\n",
      "[      0 1646336]\n",
      "[      0 1646592]\n",
      "[      0 1646848]\n",
      "[      0 1647104]\n",
      "[      0 1647360]\n",
      "[      0 1647616]\n",
      "[      0 1647872]\n",
      "[      0 1648128]\n",
      "[      0 1648384]\n",
      "[      0 1648640]\n",
      "[      0 1648896]\n",
      "[      0 1649152]\n",
      "[      0 1649408]\n",
      "[      0 1649664]\n",
      "[      0 1649920]\n",
      "[      0 1650176]\n",
      "[      0 1650432]\n",
      "[      0 1650688]\n",
      "[      0 1650944]\n",
      "[      0 1651200]\n",
      "[      0 1651456]\n",
      "[      0 1651712]\n",
      "[      0 1651968]\n",
      "[      0 1652224]\n",
      "[      0 1652480]\n",
      "[      0 1652736]\n",
      "[      0 1652992]\n",
      "[      0 1653248]\n",
      "[      0 1653504]\n",
      "[      0 1653760]\n",
      "[      0 1654016]\n",
      "[      0 1654272]\n",
      "[      0 1654528]\n",
      "[      0 1654784]\n",
      "[      0 1655040]\n",
      "[      0 1655296]\n",
      "[      0 1655552]\n",
      "[      0 1655808]\n",
      "[      0 1656064]\n",
      "[      0 1656320]\n",
      "[      0 1656576]\n",
      "[      0 1656832]\n",
      "[      0 1657088]\n",
      "[      0 1657344]\n",
      "[      0 1657600]\n",
      "[      0 1657856]\n",
      "[      0 1658112]\n",
      "[      0 1658368]\n",
      "[      0 1658624]\n",
      "[      0 1658880]\n",
      "[      0 1659136]\n",
      "[      0 1659392]\n",
      "[      0 1659648]\n",
      "[      0 1659904]\n",
      "[      0 1660160]\n",
      "[      0 1660416]\n",
      "[      0 1660672]\n",
      "[      0 1660928]\n",
      "[      0 1661184]\n",
      "[      0 1661440]\n",
      "[      0 1661696]\n",
      "[      0 1661952]\n",
      "[      0 1662208]\n",
      "[      0 1662464]\n",
      "[      0 1662720]\n",
      "[      0 1662976]\n",
      "[      0 1663232]\n",
      "[      0 1663488]\n",
      "[      0 1663744]\n",
      "[      0 1664000]\n",
      "[      0 1664256]\n",
      "[      0 1664512]\n",
      "[      0 1664768]\n",
      "[      0 1665024]\n",
      "[      0 1665280]\n",
      "[      0 1665536]\n",
      "[      0 1665792]\n",
      "[      0 1666048]\n",
      "[      0 1666304]\n",
      "[      0 1666560]\n",
      "[      0 1666816]\n",
      "[      0 1667072]\n",
      "[      0 1667328]\n",
      "[      0 1667584]\n",
      "[      0 1667840]\n",
      "[      0 1668096]\n",
      "[      0 1668352]\n",
      "[      0 1668608]\n",
      "[      0 1668864]\n",
      "[      0 1669120]\n",
      "[      0 1669376]\n",
      "[      0 1669632]\n",
      "[      0 1669888]\n",
      "[      0 1670144]\n",
      "[      0 1670400]\n",
      "[      0 1670656]\n",
      "[      0 1670912]\n",
      "[      0 1671168]\n",
      "[      0 1671424]\n",
      "[      0 1671680]\n",
      "[      0 1671936]\n",
      "[      0 1672192]\n",
      "[      0 1672448]\n",
      "[      0 1672704]\n",
      "[      0 1672960]\n",
      "[      0 1673216]\n",
      "[      0 1673472]\n",
      "[      0 1673728]\n",
      "[      0 1673984]\n",
      "[      0 1674240]\n",
      "[      0 1674496]\n",
      "[      0 1674752]\n",
      "[      0 1675008]\n",
      "[      0 1675264]\n",
      "[      0 1675520]\n",
      "[      0 1675776]\n",
      "[      0 1676032]\n",
      "[      0 1676288]\n",
      "[      0 1676544]\n",
      "[      0 1676800]\n",
      "[      0 1677056]\n",
      "[      0 1677312]\n",
      "[      0 1677568]\n",
      "[      0 1677824]\n",
      "[      0 1678080]\n",
      "[      0 1678336]\n",
      "[      0 1678592]\n",
      "[      0 1678848]\n",
      "[      0 1679104]\n",
      "[      0 1679360]\n",
      "[      0 1679616]\n",
      "[      0 1679872]\n",
      "[      0 1680128]\n",
      "[      0 1680384]\n",
      "[      0 1680640]\n",
      "[      0 1680896]\n",
      "[      0 1681152]\n",
      "[      0 1681408]\n",
      "[      0 1681664]\n",
      "[      0 1681920]\n",
      "[      0 1682176]\n",
      "[      0 1682432]\n",
      "[      0 1682688]\n",
      "[      0 1682944]\n",
      "[      0 1683200]\n",
      "[      0 1683456]\n",
      "[      0 1683712]\n",
      "[      0 1683968]\n",
      "[      0 1684224]\n",
      "[      0 1684480]\n",
      "[      0 1684736]\n",
      "[      0 1684992]\n",
      "[      0 1685248]\n",
      "[      0 1685504]\n",
      "[      0 1685760]\n",
      "[      0 1686016]\n",
      "[      0 1686272]\n",
      "[      0 1686528]\n",
      "[      0 1686784]\n",
      "[      0 1687040]\n",
      "[      0 1687296]\n",
      "[      0 1687552]\n",
      "[      0 1687808]\n",
      "[      0 1688064]\n",
      "[      0 1688320]\n",
      "[      0 1688576]\n",
      "[      0 1688832]\n",
      "[      0 1689088]\n",
      "[      0 1689344]\n",
      "[      0 1689600]\n",
      "[      0 1689856]\n",
      "[      0 1690112]\n",
      "[      0 1690368]\n",
      "[      0 1690624]\n",
      "[      0 1690880]\n",
      "[      0 1691136]\n",
      "[      0 1691392]\n",
      "[      0 1691648]\n",
      "[      0 1691904]\n",
      "[      0 1692160]\n",
      "[      0 1692416]\n",
      "[      0 1692672]\n",
      "[      0 1692928]\n",
      "[      0 1693184]\n",
      "[      0 1693440]\n",
      "[      0 1693696]\n",
      "[      0 1693952]\n",
      "[      0 1694208]\n",
      "[      0 1694464]\n",
      "[      0 1694720]\n",
      "[      0 1694976]\n",
      "[      0 1695232]\n",
      "[      0 1695488]\n",
      "[      0 1695744]\n",
      "[      0 1696000]\n",
      "[      0 1696256]\n",
      "[      0 1696512]\n",
      "[      0 1696768]\n",
      "[      0 1697024]\n",
      "[      0 1697280]\n",
      "[      0 1697536]\n",
      "[      0 1697792]\n",
      "[      0 1698048]\n",
      "[      0 1698304]\n",
      "[      0 1698560]\n",
      "[      0 1698816]\n",
      "[      0 1699072]\n",
      "[      0 1699328]\n",
      "[      0 1699584]\n",
      "[      0 1699840]\n",
      "[      0 1700096]\n",
      "[      0 1700352]\n",
      "[      0 1700608]\n",
      "[      0 1700864]\n",
      "[      0 1701120]\n",
      "[      0 1701376]\n",
      "[      0 1701632]\n",
      "[      0 1701888]\n",
      "[      0 1702144]\n",
      "[      0 1702400]\n",
      "[      0 1702656]\n",
      "[      0 1702912]\n",
      "[      0 1703168]\n",
      "[      0 1703424]\n",
      "[      0 1703680]\n",
      "[      0 1703936]\n",
      "[      0 1704192]\n",
      "[      0 1704448]\n",
      "[      0 1704704]\n",
      "[      0 1704960]\n",
      "[      0 1705216]\n",
      "[      0 1705472]\n",
      "[      0 1705728]\n",
      "[      0 1705984]\n",
      "[      0 1706240]\n",
      "[      0 1706496]\n",
      "[      0 1706752]\n",
      "[      0 1707008]\n",
      "[      0 1707264]\n",
      "[      0 1707520]\n",
      "[      0 1707776]\n",
      "[      0 1708032]\n",
      "[      0 1708288]\n",
      "[      0 1708544]\n",
      "[      0 1708800]\n",
      "[      0 1709056]\n",
      "[      0 1709312]\n",
      "[      0 1709568]\n",
      "[      0 1709824]\n",
      "[      0 1710080]\n",
      "[      0 1710336]\n",
      "[      0 1710592]\n",
      "[      0 1710848]\n",
      "[      0 1711104]\n",
      "[      0 1711360]\n",
      "[      0 1711616]\n",
      "[      0 1711872]\n",
      "[      0 1712128]\n",
      "[      0 1712384]\n",
      "[      0 1712640]\n",
      "[      0 1712896]\n",
      "[      0 1713152]\n",
      "[      0 1713408]\n",
      "[      0 1713664]\n",
      "[      0 1713920]\n",
      "[      0 1714176]\n",
      "[      0 1714432]\n",
      "[      0 1714688]\n",
      "[      0 1714944]\n",
      "[      0 1715200]\n",
      "[      0 1715456]\n",
      "[      0 1715712]\n",
      "[      0 1715968]\n",
      "[      0 1716224]\n",
      "[      0 1716480]\n",
      "[      0 1716736]\n",
      "[      0 1716992]\n",
      "[      0 1717248]\n",
      "[      0 1717504]\n",
      "[      0 1717760]\n",
      "[      0 1718016]\n",
      "[      0 1718272]\n",
      "[      0 1718528]\n",
      "[      0 1718784]\n",
      "[      0 1719040]\n",
      "[      0 1719296]\n",
      "[      0 1719552]\n",
      "[      0 1719808]\n",
      "[      0 1720064]\n",
      "[      0 1720320]\n",
      "[      0 1720576]\n",
      "[      0 1720832]\n",
      "[      0 1721088]\n",
      "[      0 1721344]\n",
      "[      0 1721600]\n",
      "[      0 1721856]\n",
      "[      0 1722112]\n",
      "[      0 1722368]\n",
      "[      0 1722624]\n",
      "[      0 1722880]\n",
      "[      0 1723136]\n",
      "[      0 1723392]\n",
      "[      0 1723648]\n",
      "[      0 1723904]\n",
      "[      0 1724160]\n",
      "[      0 1724416]\n",
      "[      0 1724672]\n",
      "[      0 1724928]\n",
      "[      0 1725184]\n",
      "[      0 1725440]\n",
      "[      0 1725696]\n",
      "[      0 1725952]\n",
      "[      0 1726208]\n",
      "[      0 1726464]\n",
      "[      0 1726720]\n",
      "[      0 1726976]\n",
      "[      0 1727232]\n",
      "[      0 1727488]\n",
      "[      0 1727744]\n",
      "[      0 1728000]\n",
      "[      0 1728256]\n",
      "[      0 1728512]\n",
      "[      0 1728768]\n",
      "[      0 1729024]\n",
      "[      0 1729280]\n",
      "[      0 1729536]\n",
      "[      0 1729792]\n",
      "[      0 1730048]\n",
      "[      0 1730304]\n",
      "[      0 1730560]\n",
      "[      0 1730816]\n",
      "[      0 1731072]\n",
      "[      0 1731328]\n",
      "[      0 1731584]\n",
      "[      0 1731840]\n",
      "[      0 1732096]\n",
      "[      0 1732352]\n",
      "[      0 1732608]\n",
      "[      0 1732864]\n",
      "[      0 1733120]\n",
      "[      0 1733376]\n",
      "[      0 1733632]\n",
      "[      0 1733888]\n",
      "[      0 1734144]\n",
      "[      0 1734400]\n",
      "[      0 1734656]\n",
      "[      0 1734912]\n",
      "[      0 1735168]\n",
      "[      0 1735424]\n",
      "[      0 1735680]\n",
      "[      0 1735936]\n",
      "[      0 1736192]\n",
      "[      0 1736448]\n",
      "[      0 1736704]\n",
      "[      0 1736960]\n",
      "[      0 1737216]\n",
      "[      0 1737472]\n",
      "[      0 1737728]\n",
      "[      0 1737984]\n",
      "[      0 1738240]\n",
      "[      0 1738496]\n",
      "[      0 1738752]\n",
      "[      0 1739008]\n",
      "[      0 1739264]\n",
      "[      0 1739520]\n",
      "[      0 1739776]\n",
      "[      0 1740032]\n",
      "[      0 1740288]\n",
      "[      0 1740544]\n",
      "[      0 1740800]\n",
      "[      0 1741056]\n",
      "[      0 1741312]\n",
      "[      0 1741568]\n",
      "[      0 1741824]\n",
      "[      0 1742080]\n",
      "[      0 1742336]\n",
      "[      0 1742592]\n",
      "[      0 1742848]\n",
      "[      0 1743104]\n",
      "[      0 1743360]\n",
      "[      0 1743616]\n",
      "[      0 1743872]\n",
      "[      0 1744128]\n",
      "[      0 1744384]\n",
      "[      0 1744640]\n",
      "[      0 1744896]\n",
      "[      0 1745152]\n",
      "[      0 1745408]\n",
      "[      0 1745664]\n",
      "[      0 1745920]\n",
      "[      0 1746176]\n",
      "[      0 1746432]\n",
      "[      0 1746688]\n",
      "[      0 1746944]\n",
      "[      0 1747200]\n",
      "[      0 1747456]\n",
      "[      0 1747712]\n",
      "[      0 1747968]\n",
      "[      0 1748224]\n",
      "[      0 1748480]\n",
      "[      0 1748736]\n",
      "[      0 1748992]\n",
      "[      0 1749248]\n",
      "[      0 1749504]\n",
      "[      0 1749760]\n",
      "[      0 1750016]\n",
      "[      0 1750272]\n",
      "[      0 1750528]\n",
      "[      0 1750784]\n",
      "[      0 1751040]\n",
      "[      0 1751296]\n",
      "[      0 1751552]\n",
      "[      0 1751808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1752064]\n",
      "[      0 1752320]\n",
      "[      0 1752576]\n",
      "[      0 1752832]\n",
      "[      0 1753088]\n",
      "[      0 1753344]\n",
      "[      0 1753600]\n",
      "[      0 1753856]\n",
      "[      0 1754112]\n",
      "[      0 1754368]\n",
      "[      0 1754624]\n",
      "[      0 1754880]\n",
      "[      0 1755136]\n",
      "[      0 1755392]\n",
      "[      0 1755648]\n",
      "[      0 1755904]\n",
      "[      0 1756160]\n",
      "[      0 1756416]\n",
      "[      0 1756672]\n",
      "[      0 1756928]\n",
      "[      0 1757184]\n",
      "[      0 1757440]\n",
      "[      0 1757696]\n",
      "[      0 1757952]\n",
      "[      0 1758208]\n",
      "[      0 1758464]\n",
      "[      0 1758720]\n",
      "[      0 1758976]\n",
      "[      0 1759232]\n",
      "[      0 1759488]\n",
      "[      0 1759744]\n",
      "[      0 1760000]\n",
      "[      0 1760256]\n",
      "[      0 1760512]\n",
      "[      0 1760768]\n",
      "[      0 1761024]\n",
      "[      0 1761280]\n",
      "[      0 1761536]\n",
      "[      0 1761792]\n",
      "[      0 1762048]\n",
      "[      0 1762304]\n",
      "[      0 1762560]\n",
      "[      0 1762816]\n",
      "[      0 1763072]\n",
      "[      0 1763328]\n",
      "[      0 1763584]\n",
      "[      0 1763840]\n",
      "[      0 1764096]\n",
      "[      0 1764352]\n",
      "[      0 1764608]\n",
      "[      0 1764864]\n",
      "[      0 1765120]\n",
      "[      0 1765376]\n",
      "[      0 1765632]\n",
      "[      0 1765888]\n",
      "[      0 1766144]\n",
      "[      0 1766400]\n",
      "[      0 1766656]\n",
      "[      0 1766912]\n",
      "[      0 1767168]\n",
      "[      0 1767424]\n",
      "[      0 1767680]\n",
      "[      0 1767936]\n",
      "[      0 1768192]\n",
      "[      0 1768448]\n",
      "[      0 1768704]\n",
      "[      0 1768960]\n",
      "[      0 1769216]\n",
      "[      0 1769472]\n",
      "[      0 1769728]\n",
      "[      0 1769984]\n",
      "[      0 1770240]\n",
      "[      0 1770496]\n",
      "[      0 1770752]\n",
      "[      0 1771008]\n",
      "[      0 1771264]\n",
      "[      0 1771520]\n",
      "[      0 1771776]\n",
      "[      0 1772032]\n",
      "[      0 1772288]\n",
      "[      0 1772544]\n",
      "[      0 1772800]\n",
      "[      0 1773056]\n",
      "[      0 1773312]\n",
      "[      0 1773568]\n",
      "[      0 1773824]\n",
      "[      0 1774080]\n",
      "[      0 1774336]\n",
      "[      0 1774592]\n",
      "[      0 1774848]\n",
      "[      0 1775104]\n",
      "[      0 1775360]\n",
      "[      0 1775616]\n",
      "[      0 1775872]\n",
      "[      0 1776128]\n",
      "[      0 1776384]\n",
      "[      0 1776640]\n",
      "[      0 1776896]\n",
      "[      0 1777152]\n",
      "[      0 1777408]\n",
      "[      0 1777664]\n",
      "[      0 1777920]\n",
      "[      0 1778176]\n",
      "[      0 1778432]\n",
      "[      0 1778688]\n",
      "[      0 1778944]\n",
      "[      0 1779200]\n",
      "[      0 1779456]\n",
      "[      0 1779712]\n",
      "[      0 1779968]\n",
      "[      0 1780224]\n",
      "[      0 1780480]\n",
      "[      0 1780736]\n",
      "[      0 1780992]\n",
      "[      0 1781248]\n",
      "[      0 1781504]\n",
      "[      0 1781760]\n",
      "[      0 1782016]\n",
      "[      0 1782272]\n",
      "[      0 1782528]\n",
      "[      0 1782784]\n",
      "[      0 1783040]\n",
      "[      0 1783296]\n",
      "[      0 1783552]\n",
      "[      0 1783808]\n",
      "[      0 1784064]\n",
      "[      0 1784320]\n",
      "[      0 1784576]\n",
      "[      0 1784832]\n",
      "[      0 1785088]\n",
      "[      0 1785344]\n",
      "[      0 1785600]\n",
      "[      0 1785856]\n",
      "[      0 1786112]\n",
      "[      0 1786368]\n",
      "[      0 1786624]\n",
      "[      0 1786880]\n",
      "[      0 1787136]\n",
      "[      0 1787392]\n",
      "[      0 1787648]\n",
      "[      0 1787904]\n",
      "[      0 1788160]\n",
      "[      0 1788416]\n",
      "[      0 1788672]\n",
      "[      0 1788928]\n",
      "[      0 1789184]\n",
      "[      0 1789440]\n",
      "[      0 1789696]\n",
      "[      0 1789952]\n",
      "[      0 1790208]\n",
      "[      0 1790464]\n",
      "[      0 1790720]\n",
      "[      0 1790976]\n",
      "[      0 1791232]\n",
      "[      0 1791488]\n",
      "[      0 1791744]\n",
      "[      0 1792000]\n",
      "[      0 1792256]\n",
      "[      0 1792512]\n",
      "[      0 1792768]\n",
      "[      0 1793024]\n",
      "[      0 1793280]\n",
      "[      0 1793536]\n",
      "[      0 1793792]\n",
      "[      0 1794048]\n",
      "[      0 1794304]\n",
      "[      0 1794560]\n",
      "[      0 1794816]\n",
      "[      0 1795072]\n",
      "[      0 1795328]\n",
      "[      0 1795584]\n",
      "[      0 1795840]\n",
      "[      0 1796096]\n",
      "[      0 1796352]\n",
      "[      0 1796608]\n",
      "[      0 1796864]\n",
      "[      0 1797120]\n",
      "[      0 1797376]\n",
      "[      0 1797632]\n",
      "[      0 1797888]\n",
      "[      0 1798144]\n",
      "[      0 1798400]\n",
      "[      0 1798656]\n",
      "[      0 1798912]\n",
      "[      0 1799168]\n",
      "[      0 1799424]\n",
      "[      0 1799680]\n",
      "[      0 1799936]\n",
      "[      0 1800192]\n",
      "[      0 1800448]\n",
      "[      0 1800704]\n",
      "[      0 1800960]\n",
      "[      0 1801216]\n",
      "[      0 1801472]\n",
      "[      0 1801728]\n",
      "[      0 1801984]\n",
      "[      0 1802240]\n",
      "[      0 1802496]\n",
      "[      0 1802752]\n",
      "[      0 1803008]\n",
      "[      0 1803264]\n",
      "[      0 1803520]\n",
      "[      0 1803776]\n",
      "[      0 1804032]\n",
      "[      0 1804288]\n",
      "[      0 1804544]\n",
      "[      0 1804800]\n",
      "[      0 1805056]\n",
      "[      0 1805312]\n",
      "[      0 1805568]\n",
      "[      0 1805824]\n",
      "[      0 1806080]\n",
      "[      0 1806336]\n",
      "[      0 1806592]\n",
      "[      0 1806848]\n",
      "[      0 1807104]\n",
      "[      0 1807360]\n",
      "[      0 1807616]\n",
      "[      0 1807872]\n",
      "[      0 1808128]\n",
      "[      0 1808384]\n",
      "[      0 1808640]\n",
      "[      0 1808896]\n",
      "[      0 1809152]\n",
      "[      0 1809408]\n",
      "[      0 1809664]\n",
      "[      0 1809920]\n",
      "[      0 1810176]\n",
      "[      0 1810432]\n",
      "[      0 1810688]\n",
      "[      0 1810944]\n",
      "[      0 1811200]\n",
      "[      0 1811456]\n",
      "[      0 1811712]\n",
      "[      0 1811968]\n",
      "[      0 1812224]\n",
      "[      0 1812480]\n",
      "[      0 1812736]\n",
      "[      0 1812992]\n",
      "[      0 1813248]\n",
      "[      0 1813504]\n",
      "[      0 1813760]\n",
      "[      0 1814016]\n",
      "[      0 1814272]\n",
      "[      0 1814528]\n",
      "[      0 1814784]\n",
      "[      0 1815040]\n",
      "[      0 1815296]\n",
      "[      0 1815552]\n",
      "[      0 1815808]\n",
      "[      0 1816064]\n",
      "[      0 1816320]\n",
      "[      0 1816576]\n",
      "[      0 1816832]\n",
      "[      0 1817088]\n",
      "[      0 1817344]\n",
      "[      0 1817600]\n",
      "[      0 1817856]\n",
      "[      0 1818112]\n",
      "[      0 1818368]\n",
      "[      0 1818624]\n",
      "[      0 1818880]\n",
      "[      0 1819136]\n",
      "[      0 1819392]\n",
      "[      0 1819648]\n",
      "[      0 1819904]\n",
      "[      0 1820160]\n",
      "[      0 1820416]\n",
      "[      0 1820672]\n",
      "[      0 1820928]\n",
      "[      0 1821184]\n",
      "[      0 1821440]\n",
      "[      0 1821696]\n",
      "[      0 1821952]\n",
      "[      0 1822208]\n",
      "[      0 1822464]\n",
      "[      0 1822720]\n",
      "[      0 1822976]\n",
      "[      0 1823232]\n",
      "[      0 1823488]\n",
      "[      0 1823744]\n",
      "[      0 1824000]\n",
      "[      0 1824256]\n",
      "[      0 1824512]\n",
      "[      0 1824768]\n",
      "[      0 1825024]\n",
      "[      0 1825280]\n",
      "[      0 1825536]\n",
      "[      0 1825792]\n",
      "[      0 1826048]\n",
      "[      0 1826304]\n",
      "[      0 1826560]\n",
      "[      0 1826816]\n",
      "[      0 1827072]\n",
      "[      0 1827328]\n",
      "[      0 1827584]\n",
      "[      0 1827840]\n",
      "[      0 1828096]\n",
      "[      0 1828352]\n",
      "[      0 1828608]\n",
      "[      0 1828864]\n",
      "[      0 1829120]\n",
      "[      0 1829376]\n",
      "[      0 1829632]\n",
      "[      0 1829888]\n",
      "[      0 1830144]\n",
      "[      0 1830400]\n",
      "[      0 1830656]\n",
      "[      0 1830912]\n",
      "[      0 1831168]\n",
      "[      0 1831424]\n",
      "[      0 1831680]\n",
      "[      0 1831936]\n",
      "[      0 1832192]\n",
      "[      0 1832448]\n",
      "[      0 1832704]\n",
      "[      0 1832960]\n",
      "[      0 1833216]\n",
      "[      0 1833472]\n",
      "[      0 1833728]\n",
      "[      0 1833984]\n",
      "[      0 1834240]\n",
      "[      0 1834496]\n",
      "[      0 1834752]\n",
      "[      0 1835008]\n",
      "[      0 1835264]\n",
      "[      0 1835520]\n",
      "[      0 1835776]\n",
      "[      0 1836032]\n",
      "[      0 1836288]\n",
      "[      0 1836544]\n",
      "[      0 1836800]\n",
      "[      0 1837056]\n",
      "[      0 1837312]\n",
      "[      0 1837568]\n",
      "[      0 1837824]\n",
      "[      0 1838080]\n",
      "[      0 1838336]\n",
      "[      0 1838592]\n",
      "[      0 1838848]\n",
      "[      0 1839104]\n",
      "[      0 1839360]\n",
      "[      0 1839616]\n",
      "[      0 1839872]\n",
      "[      0 1840128]\n",
      "[      0 1840384]\n",
      "[      0 1840640]\n",
      "[      0 1840896]\n",
      "[      0 1841152]\n",
      "[      0 1841408]\n",
      "[      0 1841664]\n",
      "[      0 1841920]\n",
      "[      0 1842176]\n",
      "[      0 1842432]\n",
      "[      0 1842688]\n",
      "[      0 1842944]\n",
      "[      0 1843200]\n",
      "[      0 1843456]\n",
      "[      0 1843712]\n",
      "[      0 1843968]\n",
      "[      0 1844224]\n",
      "[      0 1844480]\n",
      "[      0 1844736]\n",
      "[      0 1844992]\n",
      "[      0 1845248]\n",
      "[      0 1845504]\n",
      "[      0 1845760]\n",
      "[      0 1846016]\n",
      "[      0 1846272]\n",
      "[      0 1846528]\n",
      "[      0 1846784]\n",
      "[      0 1847040]\n",
      "[      0 1847296]\n",
      "[      0 1847552]\n",
      "[      0 1847808]\n",
      "[      0 1848064]\n",
      "[      0 1848320]\n",
      "[      0 1848576]\n",
      "[      0 1848832]\n",
      "[      0 1849088]\n",
      "[      0 1849344]\n",
      "[      0 1849600]\n",
      "[      0 1849856]\n",
      "[      0 1850112]\n",
      "[      0 1850368]\n",
      "[      0 1850624]\n",
      "[      0 1850880]\n",
      "[      0 1851136]\n",
      "[      0 1851392]\n",
      "[      0 1851648]\n",
      "[      0 1851904]\n",
      "[      0 1852160]\n",
      "[      0 1852416]\n",
      "[      0 1852672]\n",
      "[      0 1852928]\n",
      "[      0 1853184]\n",
      "[      0 1853440]\n",
      "[      0 1853696]\n",
      "[      0 1853952]\n",
      "[      0 1854208]\n",
      "[      0 1854464]\n",
      "[      0 1854720]\n",
      "[      0 1854976]\n",
      "[      0 1855232]\n",
      "[      0 1855488]\n",
      "[      0 1855744]\n",
      "[      0 1856000]\n",
      "[      0 1856256]\n",
      "[      0 1856512]\n",
      "[      0 1856768]\n",
      "[      0 1857024]\n",
      "[      0 1857280]\n",
      "[      0 1857536]\n",
      "[      0 1857792]\n",
      "[      0 1858048]\n",
      "[      0 1858304]\n",
      "[      0 1858560]\n",
      "[      0 1858816]\n",
      "[      0 1859072]\n",
      "[      0 1859328]\n",
      "[      0 1859584]\n",
      "[      0 1859840]\n",
      "[      0 1860096]\n",
      "[      0 1860352]\n",
      "[      0 1860608]\n",
      "[      0 1860864]\n",
      "[      0 1861120]\n",
      "[      0 1861376]\n",
      "[      0 1861632]\n",
      "[      0 1861888]\n",
      "[      0 1862144]\n",
      "[      0 1862400]\n",
      "[      0 1862656]\n",
      "[      0 1862912]\n",
      "[      0 1863168]\n",
      "[      0 1863424]\n",
      "[      0 1863680]\n",
      "[      0 1863936]\n",
      "[      0 1864192]\n",
      "[      0 1864448]\n",
      "[      0 1864704]\n",
      "[      0 1864960]\n",
      "[      0 1865216]\n",
      "[      0 1865472]\n",
      "[      0 1865728]\n",
      "[      0 1865984]\n",
      "[      0 1866240]\n",
      "[      0 1866496]\n",
      "[      0 1866752]\n",
      "[      0 1867008]\n",
      "[      0 1867264]\n",
      "[      0 1867520]\n",
      "[      0 1867776]\n",
      "[      0 1868032]\n",
      "[      0 1868288]\n",
      "[      0 1868544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1868800]\n",
      "[      0 1869056]\n",
      "[      0 1869312]\n",
      "[      0 1869568]\n",
      "[      0 1869824]\n",
      "[      0 1870080]\n",
      "[      0 1870336]\n",
      "[      0 1870592]\n",
      "[      0 1870848]\n",
      "[      0 1871104]\n",
      "[      0 1871360]\n",
      "[      0 1871616]\n",
      "[      0 1871872]\n",
      "[      0 1872128]\n",
      "[      0 1872384]\n",
      "[      0 1872640]\n",
      "[      0 1872896]\n",
      "[      0 1873152]\n",
      "[      0 1873408]\n",
      "[      0 1873664]\n",
      "[      0 1873920]\n",
      "[      0 1874176]\n",
      "[      0 1874432]\n",
      "[      0 1874688]\n",
      "[      0 1874944]\n",
      "[      0 1875200]\n",
      "[      0 1875456]\n",
      "[      0 1875712]\n",
      "[      0 1875968]\n",
      "[      0 1876224]\n",
      "[      0 1876480]\n",
      "[      0 1876736]\n",
      "[      0 1876992]\n",
      "[      0 1877248]\n",
      "[      0 1877504]\n",
      "[      0 1877760]\n",
      "[      0 1878016]\n",
      "[      0 1878272]\n",
      "[      0 1878528]\n",
      "[      0 1878784]\n",
      "[  0 256]\n",
      "(1878784, 2)\n",
      "time =  1:32:00.616677\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "all_test_data_addr = [train_data_addr] # NEED TO REWRITE\n",
    "# test_data_len = 685845 * len(all_test_data_addr)\n",
    "test_data_len = 1879087 * len(all_test_data_addr)\n",
    "# test_data_len = ???\n",
    "batch_size = 256\n",
    "steps = int((test_data_len/batch_size)-1)\n",
    "window_size = 10\n",
    "\n",
    "model = load_model(model_save_addr + '4_MRCG96_win10_batch256_mini128_e200_512sigmoid_512sigmoid.h5')\n",
    "\n",
    "tic = time.clock()\n",
    "ynew = model.predict_generator(dg_test(all_test_data_addr, batch_size, test_data_len, window_size),steps)\n",
    "print(ynew.shape)\n",
    "\n",
    "toc = time.clock()\n",
    "print('time = ', str(datetime.timedelta(seconds=toc-tic)))\n",
    "\n",
    "with h5py.File(predict_label_addr +'4_batch'+str(batch_size)+'_'+all_test_data_addr[0].split('\\\\')[-1].split('.')[0]+'.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('label', data=ynew)\n",
    "    h5f.close()\n",
    "\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03115811 0.9688419 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2067ce496d8>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot labels\n",
    "test_label_file = h5py.File(test_label_addr,'r')\n",
    "true_label = test_label_file['test_label']\n",
    "# print(true_label[0][100])\n",
    "test_predict_label_file = h5py.File(test_predict_label_addr,'r')\n",
    "pred_label = test_predict_label_file['label']\n",
    "print(pred_label[100])\n",
    "flag = 0\n",
    "rangee = 500\n",
    "plt.plot(true_label[0][0+256+flag:rangee+flag+256])\n",
    "plt.plot(np.transpose(pred_label)[1][0+flag:rangee+flag])\n",
    "# for i in range(len(pred_label)):\n",
    "#     print(\"X=%s, Predicted=%s\" % (real_label[0][i+256], pred_label[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "class result:\n",
    "    def __init__(self, pred_label_addr, true_label_addr, threshold_step):\n",
    "        # read True and predicted label by the model________________________________________________________part one\n",
    "        h5f = h5py.File(pred_label_addr, 'r')\n",
    "        variables = h5f.items()\n",
    "        for var in variables:\n",
    "            name = var[0]\n",
    "            data = var[1]\n",
    "#             print (\"Name \", name)  # Name\n",
    "            if type(data) is h5py.Dataset:\n",
    "                 self.pred_label = data.value\n",
    "        h5f.close()\n",
    "        if self.pred_label.shape[0]<self.pred_label.shape[1]:\n",
    "            self.pred_label = np.transpose(self.pred_label)\n",
    "        \n",
    "        h5f = h5py.File(true_label_addr, 'r')\n",
    "        variables = h5f.items()\n",
    "        for var in variables:\n",
    "            name = var[0]\n",
    "            data = var[1]\n",
    "#             print (\"Name \", name)  # Name\n",
    "            if type(data) is h5py.Dataset:\n",
    "                 self.true_label = data.value\n",
    "        h5f.close()\n",
    "        if self.true_label.shape[0]<self.true_label.shape[1]:\n",
    "            self.true_label = np.transpose(self.true_label)\n",
    "            \n",
    "        # make thresholds for AUC part_______________________________________________________________________part two       \n",
    "        self.threshold_step = np.arange(0, 1.00000000001, threshold_step)\n",
    "        \n",
    "        # extract batch_size for the number of first frame ignore in windowing in prediction_________________part three\n",
    "        self.batch_size = int(pred_label_addr.split('\\\\')[-1].split('.')[0].split('batch')[1].split('_')[0])\n",
    "        \n",
    "        # count number true and false label in prediction and all of the label that contain speech or not____part four\n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.ap = 0\n",
    "        self.an = 0\n",
    "        pred_label_mask = np.ma.masked_greater_equal(np.transpose(self.pred_label)[1], 0.1).mask\n",
    "        for i in range(len(pred_label_mask)):\n",
    "            if pred_label_mask[i] == True and self.true_label[i+self.batch_size] == 1:\n",
    "                self.tp += 1 \n",
    "            elif pred_label_mask[i] == False and self.true_label[i+self.batch_size] == 0:\n",
    "                self.tn += 1\n",
    "            elif pred_label_mask[i] == True and self.true_label[i+self.batch_size] == 0:\n",
    "                self.fp += 1\n",
    "            elif pred_label_mask[i] == False and self.true_label[i+self.batch_size] == 1:\n",
    "                self.fn += 1\n",
    "        self.ap = int(np.sum(self.true_label[self.batch_size:self.batch_size+len(pred_label_mask)]))\n",
    "        self.an = int(len(pred_label_mask) - self.ap)\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        fig, axs =plt.subplots(1,1)\n",
    "        clust_data = [[self.tp,             self.fp,           self.tp+self.fp        ],\n",
    "                      [self.fn,             self.tn,           self.fn+self.tn        ],\n",
    "                      [self.tp+self.fn,     self.fp+self.tn,   self.tp+self.fp+self.fn+self.tn]]\n",
    "        colLabel = (\"ture_Speech\", \"true_Non-speech\", \"Total\")\n",
    "        rowLabel = (\"pred_Speech\", \"pred_Non-speech\", \"Total\")\n",
    "        axs.axis('tight')\n",
    "        axs.axis('off')\n",
    "        the_table = axs.table(cellText=clust_data,colLabels=colLabel,rowLabels=rowLabel,loc='center')\n",
    "\n",
    "#         axs[1].plot(clust_data[:,0],clust_data[:,1])\n",
    "        plt.title('confusion_matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        fig, axs =plt.subplots(1,1)\n",
    "        clust_data = [[self.tp/(self.tp+self.fp),  self.fp/(self.tp+self.fp),   (self.tp+self.fp)/(self.ap+self.an)],\n",
    "                      [self.fn/(self.fn+self.tn),  self.tn/(self.fn+self.tn),   (self.fn+self.tn)/(self.ap+self.an)],\n",
    "                      [(self.tp+self.fn)/(self.ap+self.an),     (self.fp+self.tn)/(self.ap+self.an),   self.ap+self.an]]\n",
    "        colLabel = (\"ture_Speech\", \"true_Non-speech\", \"Total\")\n",
    "        rowLabel = (\"pred_Speech\", \"pred_Non-speech\", \"Total\")\n",
    "        axs.axis('tight')\n",
    "        axs.axis('off')\n",
    "        the_table = axs.table(cellText=clust_data,colLabels=colLabel,rowLabels=rowLabel,loc='center')\n",
    "\n",
    "#         axs[1].plot(clust_data[:,0],clust_data[:,1])\n",
    "        plt.title('confusion_matrix_percent')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def Classification_Accuracy_Metrics(self):\n",
    "        # precision = true positive accuracy\n",
    "        # recall = true positive rate\n",
    "        # fallout = false positive rate\n",
    "        # missRate = false negative rate\n",
    "        # inversePrecision = true negative accuracy\n",
    "        # inverseRecall = true negative rate\n",
    "        precision = self.tp/(self.tp+self.fp)\n",
    "        recall = self.tp/(self.tp+self.fn)\n",
    "        fallout = self.fp/(self.fp+self.tn)\n",
    "        missRate = self.fn/(self.fn+self.tp)\n",
    "        inversePrecision = self.tn/(self.tn+self.fn)\n",
    "        inverseRecall = self.tn/(self.tn+self.fp)\n",
    "        fig, axs =plt.subplots(1,1)\n",
    "        clust_data = [[precision],\n",
    "                      [recall],\n",
    "                      [fallout],\n",
    "                      [missRate],\n",
    "                      [inversePrecision],\n",
    "                      [inverseRecall]]\n",
    "        colLabel = (\"Metrics\", \"Accuracy\")\n",
    "        rowLabel = (\"precision\", \"recall\", \"fallout\",\"missRate\", \"inversePrecision\", \"inverseRecall\")\n",
    "        axs.axis('tight')\n",
    "        axs.axis('off')\n",
    "        the_table = axs.table(cellText=clust_data,colLabels=colLabel,rowLabels=rowLabel,loc='center')\n",
    "        plt.title('Classification_Accuracy_Metrics')\n",
    "        plt.show()\n",
    "        \n",
    "    def AUC(self):\n",
    "        all_tp = []\n",
    "        all_tn = []\n",
    "        all_fp = []\n",
    "        all_fn = [] \n",
    "        all_recall = [1]\n",
    "        all_fallout = [1]\n",
    "        for i in range(1,len(self.threshold_step)-1):\n",
    "            threshold = self.threshold_step[i]\n",
    "            print(threshold)\n",
    "            temp_tp = 0\n",
    "            temp_tn = 0\n",
    "            temp_fp = 0\n",
    "            temp_fn = 0\n",
    "            pred_label_mask = np.ma.masked_greater_equal(np.transpose(self.pred_label)[1], threshold).mask\n",
    "            for i in range(len(pred_label_mask)):\n",
    "                if pred_label_mask[i] == True and self.true_label[i+self.batch_size] == 1:\n",
    "                    temp_tp += 1 \n",
    "                elif pred_label_mask[i] == False and self.true_label[i+self.batch_size] == 0:\n",
    "                    temp_tn += 1\n",
    "                elif pred_label_mask[i] == True and self.true_label[i+self.batch_size] == 0:\n",
    "                    temp_fp += 1\n",
    "                elif pred_label_mask[i] == False and self.true_label[i+self.batch_size] == 1:\n",
    "                    temp_fn += 1\n",
    "            temp_recall = temp_tp/(temp_tp+temp_fn)\n",
    "            temp_fallout = temp_fp/(temp_fp+temp_tn)\n",
    "            all_tp.append(temp_tp)\n",
    "            all_tn.append(temp_tn)\n",
    "            all_fp.append(temp_fp)\n",
    "            all_fn.append(temp_fn)\n",
    "            all_recall.append(temp_recall)\n",
    "            all_fallout.append(temp_fallout)\n",
    "        all_recall.append(0)\n",
    "        all_fallout.append(0)\n",
    "#         print('all_recall = ',all_recall)\n",
    "#         print('all_fallout = ', all_fallout)\n",
    "        \n",
    "                \n",
    "        points = [(1,0)]\n",
    "        for i in range(len(all_recall)):\n",
    "            points.append((all_fallout[i],all_recall[i]))\n",
    "        polygon = Polygon(points)\n",
    "        print('AUC = ',polygon.area)\n",
    "        self.AUC = polygon.area\n",
    "        \n",
    "        \n",
    "        plt.plot(all_fallout,all_recall,'--r',linewidth=2)\n",
    "        plt.show()\n",
    "        \n",
    "        fig, ax =plt.subplots(1,1)\n",
    "        ax.add_patch(descartes.PolygonPatch(Polygon(points), linewidth=0, fc='blue', alpha=0.7))\n",
    "        plt.grid(color='grey', linestyle='--', linewidth=0.3)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(fontsize = 10)\n",
    "        plt.yticks(fontsize = 10)\n",
    "        plt.title('AUC = ' + str(polygon.area))\n",
    "        plt.savefig('AUC.png', dpi=1200)\n",
    "        plt.show()\n",
    "        \n",
    "def main():\n",
    "    # Set name of Shark object\n",
    "    sammy = Shark(\"Sammy\")\n",
    "    sammy.swim()\n",
    "    sammy.be_awesome()\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(685568, 2)\n",
      "(685845, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEICAYAAAAut+/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8TPfi//HXR4IkorGVVm3f7qW3qrf101uptq62tNRFY4+oIEIsIWgtDaXETuMKEnTTFsXXRXW1dbu+Kiiqra1iqRZJQ0Qk8vn9MSdzk8mo3NaW9v18PDwkmTNnzjmfM/OeOTPnPcZai4iIiECJq70AIiIi1wqFooiIiEOhKCIi4lAoioiIOBSKIiIiDoWiiIiIQ6EocpUYl/nGmFRjzKbfMZ9gY8y3l3LZrgXGmNPGmJuv9nLIn4vReYoiV4cxJhh4C7jDWptxtZfnSjHGrAPesNYmXu1luRYYYyxwm7V2z9VeFtErRZGrqSZw4M8UiEVhjPG92svg6VpcJrk8FIoiRWSMqW6MWWqM+dkYc8IYE2+MKWGMGW6M+cEY85Mx5jVjTJAzfS1jjDXGdDHGHDTGHDfGDHMu6wYkAg86hwlHGWPCjDGfetymNcbc6vzczBizyxhzyhhz2BgzyPn7I8aYQ/muc5cxZp0xJs0Ys9MY0yLfZQuMMTONMauc+fzbGHNLEdbdGmMijTHfO9d7yRhzizHmC2NMujFmkTGmlDNteWPMSmc7pTo/V3MuGwsEA/HOesfnm39vY8z3wPf5190YU8oYs9UYE+X83ccY85kxZuRFljnWGLPEGPOOs8xbjDF1811e1RjzrrOc+40xfb1c9w1jTDoQ5tzuC8aYvc78vjLGVHemv9MY86Ex5qQx5ltjTEhRtrkxZoMz2TZne7S92FjIZWat1T/907+L/AN8gG3AVKAM4Ac0BJ4D9gA3A4HAUuB15zq1AAvMBfyBukAWcJdzeRjwab7bKPC78zcL3Or8fBQIdn4uD9zn/PwIcMj5uaSzPC8ApYDHgFO4DtECLABOAvUBX+BN4O0irL8FVgDXAXWc9fjYWe8gYBfQxZm2ItAaCADKAouB5fnmtQ4I9zL/D4EKgL+Xdb8bSAXuAoYBXwI+F1nmWCAbaONsl0HAfufnEsBXwEhnO90M7AOe8LhuS2dafyAG+Bq4AzDOeFZ09ocUoKuzTe8DjgN1irLN86+n/l39f3qlKFI09YGqQIy1NsNae9Za+ynQEZhird1nrT0NPA+08zjcNspam2mt3YYrWOsWmnvRZAO1jTHXWWtTrbVbvEzTAFc4j7fWnrPWfgKsBNrnm2aptXaTtTYH1wP0vUW8/Thrbbq1diewA/jAWe9fgPeAegDW2hPW2nettWestaeAsUCjIsx/nLX2pLU20/MCa+0OYAywDFe4dbbWni/CPL+y1i6x1mYDU3A9mWkAPABcb60d7WynfbievLTLd90vrLXLrbW5zjKFA8Ottd9al23W2hPA07gOg8+31uY44/IurjDO81u3uVxhCkWRoqkO/OA8qOVXFfgh3+8/4Ho1UCXf337M9/MZXKH1W7QGmgE/GGPWG2Me9DJNVSDFWpvrsUw3XYLlOZbv50wvvwcCGGMCjDGznUPK6cAGoJwxxuci80+5yOWv4nr1vdpa+30Rl9k9T2ebHMK1jWoCVZ1DzGnGmDRcr66reLuuozqw18tt1AT+n8e8OgI35JvmUu0DcpkpFEWKJgWo4eUDF0dwPSjmqQHkUDAwiioD1yFHAIwx+R9Usdb+n7X2GaAysBxY5GUeR4Dqxpj89+0awOHfsDy/1UBchxj/n7X2OuBh5+/G+f9CH3m/2Efh/4nrVe8TxpiGRVyW6nk/ONukGq5tlALst9aWy/evrLW22a8sTwrg7f3XFGC9x7wCrbW9iriMcg1RKIoUzSZc7+mNN8aUMcb4GWMewnVKxQBjzP8YYwKBl4F3vLyiLIptQB1jzL3GGD9c72sB4HzYpKMxJsg5FJgOeDt8+G9c4TrYGFPSGPMI0Bx4+zcsz29VFtcrxzRjTAXgRY/Lj+F6D6/IjDGdgb/iet+1L/Cqs70v5q/GmFbOk5n+uN4L/RLXeKYbY4YYY/ydD9HcbYx54FfmlQi8ZIy5zbjcY4ypiCuobzfGdHa2eUljzAPGmLuKuHr/9faQy0ehKFIEzvtXzYFbgYO4DsO1BeYBr+M6RLgfOAtE/cbb+A4YDXyE6xOYn3pM0hk44BySjAA6eZnHOaAF0BTXhz3+CYRaa3f/lmX6jabh+mDKcVwBtMbj8ulAG+eTqTMuNjNjTA1nnqHW2tPW2oXAZlwferqY/8U1Tqm4tl8ra212vvG8F9e4HccVekG/Mq8puF6df4DrSUkSrg8FnQIex/V+5BFch0rjgNJFWD5wPfl51Tn0GnKxieXy0sn7IvKHZIyJxfWpzkJPHkQuRK8URUREHGppEJG8yrn3vF1mrb1mPylpjHkPVxmAp5ev9LLIH4MOn4qIiDh0+FRERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXH4Xu0FkN/H39//x7Nnz1a52ssh/z0/P7/cs2fP6olpMaXxK778/PyOZWZm3uDtMmOtvdLLI5eQMcZqDIsnYwwau+JL41d8OWNnvF2mZzkiIiIOhaKIiIhDoSiXxIIFC1i5cuWvTnOxQ03r1q0jJCSEPn36kJSUdMWW68/Aczvk5ub+1/OIjY3lH//4BwBr1qxhwYIFl2rxfrOwsDBOnz59tRej2MnMzCQiIoIWLVoQHBxMREQEe/fuLTBNmzZtLnj9X7usuNMHbeSS+PTTTzlz5gxhYWEcOHAAX19fIiIiiI2NpWPHjjRv3pz27dszdepUrLWULVuWMWPGFJjHsmXLGDVqFHfddRcABw4coGPHjoSEhPDtt98SHx/P4sWL+eKLL0hPT6dv375Ya1mwYAE5OTk8+OCDNG/enL59+1KpUiUaNWoEwKJFi1izZg1VqlRhxIgRV3zbXAvyj0+fPn34y1/+wltvvcWSJUvYvXs3b7/9NgMGDODFF1+84PgABAUFsWHDBvfvJ06coH///lx33XXcc8899OzZk7/85S+EhYXx1VdfkZSUhL+/PwA///wzvXv3platWjz99NPs27ePtWvXUrt2bXx8fBg0aBBjx47l559/5tSpU0ybNo3Vq1cXGO/c3FymTZtGpUqVCA8PByAuLo5vv/2W8PBwHn/88SuzQYs5f39/EhISWLduHTt27KB79+707NmToKAgrr/+elq0aMGuXbuIjY0lOjqaCRMmkJaWRr169ejWrdvVXvzLSq8U5ZJo2LAhHTp04Omnny50We3atRk6dCirV68mMzOT8uXLs2/fPs6dO1dguiFDhpCYmEjXrl1ZunQpAHfddRf9+vXjtttu49///jfx8fGUK1eOKlWqsGnTJqZMmUL58uW5/vrrSU5O5s0336RTp05MnjyZFi1aAPDEE08QHx/Pjh07Lv+GuEblH5/u3bvTunXrQtMsXLjwV8cHICoqivj4ePer/rfeeovnnnuOmTNn8tFHHwFQrVo1Bg4cyIMPPsjWrVvd1z179iw+Pj60bNmS4OBgAJo0acKQIUPYvHkzO3fuZMOGDZQrV45SpUrxzTffeB3vhIQEpkyZQu3atQGIiIhgzpw5vPvuu5d8u/1ZvPfeezRu3Jjp06ezZ88ebr31VmrXrk1sbCw+Pj5kZ2dToUIFFi1adLUX9bLTK0W5JEqUcD2/Kl26NDk5OWRlZbkvCwoKAlyH7J566il3WHmqWrUqkydPBqBp06bMmjWLnJwcALKzszHG4O/vT2xsrPs6H3/8Mf369aN8+fIAzJw5070snrdvjNcPm/0p5N8medsj728ZGRnAxccHXOPbvHlzFi1aRKNGjbDWFtquZcqUAaBkyZJkZWUxbdo0Dhw4wLhx45gxYwZLly7lww8/pGbNmgXGF6BOnToFxtdzvNevX1/o9oKCgvD19S2wz8l/x9s45v2+cuVK6tatS4cOHXjssceuxuJdUQpFuSTq1q3L2LFjadWqFSNGjOCmm24qNE2nTp3o06cPGzdu5Ny5c0yfPr3A5YmJiSQnJ2Ot5dFHHwXg+++/54UXXuDo0aMMGjSITp060aNHD/z9/XnqqacYMmQIUVFRVKlShVq1atGpUycGDBjAhx9+6H41Iv8Znw8++ID4+HjA9Qr6hRdeIDs7mzJlylx0fPJ07NiRadOm0ahRI9q3b8+AAQNYunSpe8w89e/fH4Cvv/6apKQkzp49y9///ndOnz7NBx98wPbt26lfvz516tShRIkSREdHk5mZyQsvvFBovAcMGEBkZCRVqlQhLCzssmyrP6OmTZsSERFBcnIyt9xyCwEBAVSqVImhQ4fSvn17xo0bR0pKym96L7q40XmKxdwf+TzFAwcOEB8fz6RJk672olwWf/bz3BYsWEClSpW8HnIvDv7s41ec/dp5igrFYq44h+Lbb7/N7t27AfDz82Po0KFXeYmurGv9QfXPPj4Xc62Pn1yYQvEPrDiH4p+dHlSLN41f8fVroXhNv6eoXs+L8/Pz+1N/gKQ409gVbxq/4svPz++Cb45e068U9Sro4vRstfjS2BVvGr/iS92nIiIiRaBQFBERcVzT7yn+Vt5ORM3j7WPg1loiIiLcv48dO5ZKlSr97uVo06YNS5Ys+d3zuRoyMjKIjIykVKlSPPLII3Ts2BGA1atXM2/ePEqUKEF4eDgNGzakR48eXHfddVStWpXhw4ezfPly1qxZQ0pKCiNGjKBBgwb07NmTzz//nK+//hqAHTt2MG7cOACef/55ateuTa9evcjMzCQgIICEhARWr15NfHw8zZo1o0+fPgAMHTqUM2fOEBAQwPjx44mNjeWbb76hfPnyjBw5krS0NGbMmAHAhx9+yN69e0lKSuLjjz/m7NmzjBw5khtvvJGoqCgqVarEPffcQ0REBGFhYfj6+uLr68v06dPZtGkTb775Jjk5OezatYvPP/+chQsXsnbtWrKyspg1axZlypQhIyODhx9+mFGjRl3VUwt+bbwutg2PHj3KuHHjsNbSrl07GjRoQOfOnSlTpgznzp1j/vz5LF68mFWrVlGyZEkGDRrEXXfdxbx580hOTiYoKIgxY8YUad8A1/mKjRs3Zt++fWzdupUFCxaQlpZGixYtCA0NZcqUKbzxxhu89tpr3H333Rdct6SkJF577TXWr19PYmIimzZt4ujRo7z00kvcfvvthW7bc185d+4cLVu2pEGDBjRp0sRry8/VcqF19hyrevXqFVpPz/tWyZIliYuL4+zZs9StW5chQ4YA8P7779O7d2/27NnjrnSrWLEijRs3pk2bNjRt2pSaNWsSGBjoPi3q6NGjPPTQQ6xYsYK7774bcDUK5U0zffp0tm/fjjGGMWPGUKVKFXr16oWPjw8333wzAwcOJCYmhpMnT5KWlsaCBQtISUkpdNue+8Dy5ctZtWoVP/30E7179+bxxx9nxYoVfPDBB5QsWZJx48bh5+d3SbZ9sQ7FBQsWFOhOnD9/Pp07d6ZVq1YkJSWRlZXF+fPnmTZtGsOGDcNa6+5IzC81NZVffvmFt99+2/23sLAwbrvtNk6ePEnTpk154IEHCvVCevY0Llu2jM8++wx/f38mTZrE8ePHGT58OFu3bmXOnDlUrVr1Sm+i32zp0qW0adOG5s2b07ZtW/ed8vPPP2fs2LFcd911TJs2jYoVK3LHHXcwYsQIevfuTUpKCi1btqRly5YkJyezYcMGGjRowOzZswuUCE+fPp2ZM2dijGHw4MHMnj2b2bNnA/Dcc8+Rm5tLs2bNCAgIcNezHTx4kOzsbGbMmEFMTAwpKSn4+vpSqlQpSpYsSbly5ahatSoJCQls3bqVChUqALBx40bmzp3L9u3b+eKLL6hcuTLNmzenc+fOtGnThm7duuHv709OTg7lypWjZMmSBAcHExwczPLly3nggQcAVzfr4sWLWblyJUuXLqVz587ExcUREhJyJYfGqwuNV1G24bRp0yhbtiynT5+mWrVqZGZmUrp0aebOnUvv3r3JyMjg3Xff5c033yQ1NZUXXniBl19+mXfeeYd69epx4403AkXbN2644QYSExNp2rQp4Kqfa9iwIQCtW7cmNDSU6Oho0tPTf3Xd9u/fz4kTJ7j++usBCA8PJzw8nOTkZP71r3/RrFmzQrftua/89NNPBAYGcubMGWrUqHHFxqooLjSekyZNKjBW33zzTaH19HbfmjdvHgCtWrUCIC0tjXXr1nHvvfcCrpq3qKgogoODadGiBW3atCEgIIDc3FyqVPnPZx0nTpzIs88+6/59yZIl3H///e5Td9atW8eyZcvYtGkTiYmJNGrUiDp16hAVFUXnzp05d+4cEydOBGDq1Kls3bqVTZs2Fbptz30g7zElNTWVQYMG0bhxY2bOnEndunW57rrrLlkgwh/g8Gn+7sSqVasydOhQ9u7dy4EDByhXrhynT5/m0KFD/Pjjj8TFxfHggw8WmkeFChVo0aIF3bt3p3fv3u7B6NKlCxMnTiQxMbFQL+S2bdsK9TQuW7aM2bNnM23aNHx9fSlZsiRjxowhPDyc9evXX+lN87scOnSI6tWrA+Dj4+P++z/+8Q/CwsJo2bIlHTt2pF69emRlZREdHc2RI0c4fPgw4LrzhIeH07hxY6/z/+WXXyhXrhxBQUGcOnUKgF27dtGqVSv8/f0LVbUBHD582L1MNWrU4NChQ7zwwgu8/vrrNGnShMTERPe0iYmJPPfccwCEhITw5JNPMmDAAFq0aEGzZs3YsmULAwcOJDU1lRMnTjBz5kzmzp1L1apVC3ybxMKFC2nfvj3wn9qrmjVrcujQIT766CNq165d4EHjarnQeHnytg137txJaGgosbGxvPTSSwQEBGCM4amnniIrK4uyZcsyaNAgoqKi+Oc//0lqair79u2jQoUKjB8/nh9++IG9e/cWad+YNGkSUVFRhY7kTJw4ka5duxZp3XJzc5k8ebK7KSdPTk4OM2bMICwszOtte+4rNWvW5NNPPyUhIYGXXnrpN2/7y+FC4+k5Vt7W09t9C1znneYVpo8dO5aYmBj3ZZ07d+btt98mJiaGEydOALB48WLmzJnD0aNH2b59O/Pnz6d169bugvdjx46RnJzM3//+d/d8evToQWRkJCtWrODQoUMF1qNy5cruef/4449s3ryZv/3tb15v+0LGjBlD79693S9GJkyYQPny5fnkk09+87b2VOxDMX934nXXXQe4OhwfeughYmNjmT9/vju4wNXd6E2HDh2YO3cuwcHBLFu2zD3v3Nxc97+nnnqK2NhYFi5ciK+vr7uncdasWdSvX99rJyO4Prpd3HoZq1WrxqFDh4CCXzM0btw41q9fz8aNGxk/fjwlSpRgzJgx7mLum2++GYCYmBjee+89d5epp6CgIH755RfS09MpW7Ys4CoOX7p0KdZafvjhh0LXuemmm9zLlJKSQrVq1dzhWblyZfdXCJ05c4YjR45w6623ApCQkMDGjRtZsmQJEydOxN/fn6lTpzJ58mQCAwOpXLmy1/kcPHiQoKAg936V5+DBg1SrVo21a9fy5ZdfsnDhQubOnXtVK7AuNF6evG3DatWqUb58eQIDAzl79ixbtmyhVq1arFq1ilq1arF161bq169PQkICnTp1onr16tx0003uV+J5Tz6Lsm9s3bqV+Ph4Nm3a5D4yMGXKFG644YYLHn72XLd9+/bx888/M3jwYLZt28bq1avJzs4mMjKS/v37U716da+37TnGeffXgICASzACl9aFxtNzrLytp7f71ttvv80PP/xAREQEGRkZ7Nmzh9GjR7Nt2zbeeOMNKleuzMyZMxk/frz7rSPP7bVp0yYWL17MmjVrmD17NuvXr+enn35i9OjRrF27lu+++46mTZvyz3/+k0cffZQ777yzwHr8/PPPVKxYkcOHDxMTE8PMmTPx8fHxetuerLUMGTKEpk2bct9991GhQgX3kbfy5csXCP/fq1gfPgUKdCf+3//9H+DqdIyIiCAmJoa0tDReeeUVbrzxRiZPnsxnn33mfrDMc/LkSWJiYggMDOTYsWNMnDiRtWvXMmfOHA4dOkT37t2pX79+oV5Iz57G5s2b07t3b8qUKcPLL798NTbHJdOqVSv69OnDqlWr3IcaX3/9dVq1akX37t2x1vLkk08CEBkZSXZ2Nvfffz+VK1cmMTGRbdu28csvv9C9e3cAhg0bRnJyMhEREUyfPp1+/fq5v/pp8ODBHDlyhHHjxpGbm4uvry/Vq1fniy++YMqUKaSmpnLjjTfSunVrSpYsSXR0NKVLl6Z69eq8/PLLpKSkcPz4cfd7ie+8806BQ7WNGjWie/fupKenEx4eTkZGBlFRUZw/f54uXbpQokQJBg4cSGZmJqmpqe5XnElJSQVevbRs2dL9vufMmTPdxdd571N7e3V7pVxovIqyDaOjoxk8eDDGGHr16kXt2rWZPHkykZGRHD9+nAEDBrB69Wr+9a9/cfr0aSZOnMgNN9xAhQoViI6O5ty5c9StW7dI+8Y777wDuN6e6NmzJytWrCAhIYHHHnuMgwcPMmzYMF599VVWrlzJN998w/Dhwwut26233uqez6FDh2jWrBnR0dF89913zJo1i8aNG/Pss88Wum3PfWXjxo28+uqrZGZm0qFDh6s2dt5caDw9xwoKb2PP+1ZycjKDBg3i6aefJjo6milTprif+B86dIhOnTpx4MABXn75ZTIyMtyvILt06UJAQAA5OTkMHjyYv/3tb4DrezXbtGnD3XffTUhIiLuO8fbbb3fvc1lZWcyYMYOAgAAWLlxIv379qFu3LqVKlaJFixbcfvvtDB06lF69ehEUFFTotj33gbVr1/LRRx/xyy+/sGfPHiIiInj44Yfp168fp06dYtasWZds2xfr8xQvZ3diWFgY8fHxBAYGXvJ5X0o6V6r40tgVbxq/4qvY1rxdrpP3f/zxRxISEty/P/nkkzRo0OCS386VoDtm8aWxK940fsWXQvEPTHfM4ktjV7xp/IqvYtt96ufnl2uMKfYfBrqc1L9YfGnsijeNX/Gl7tM/MD1bLb40dsWbxq/4UvepiIhIESgURUREHH/IULzYaRz5G0vy+Pr68tVXXwHQrl27y7ZsRXXgwAEGDRp0xW4vIyODLl260L17d958803333fs2EHHjh3p2LEjO3bs4MyZM3Tq1InIyEjGjBnjdRqAefPmERUV5e67jI2NpW3btkRERHDkyBEAdyFCfHw8AEOGDCEiIoJ69erx/vvvs2vXLkJCQujVq5e7Q3b48OH06NGDXr16cebMGTZu3EhERATh4eHu86jAdY5ho0aNANeYN2vWjIiICL7++muOHTtGREQEERER1KhRg/T09ELrcP78eTp06ED37t3p0qULubm5rF69mjZt2hASEsIHH3zgvq2IiIgrOlZFcaHxfOeddwgNDaVbt2588803gKsLtW/fvgwdOtTrNLm5ufTs2ZPQ0NACHcFff/21+8TuAwcOcO+99xIREcG7774LQNeuXenatStdunTh/PnzHDlyhI4dOxIaGsratWsBmDx5Mn369KFnz55Ya0lMTKRHjx40b96crVu3AnDLLbcQERHBnDlz3Lf9/vvvu883Xr58Od27d+eZZ57hgw8+cHcZR0REEB0djbWWTZs20bZt2wLjNGzYMG6//XZ3WcO15ELjt3z5ciIiInjqqaf48ssvgYuPn7f93fP+mLfNevfuXaBwI/92XrduHcHBwURERLBu3ToA6tevT0REhLtrtSiPF0lJSe51eP75572ug+d9zds6QMF98JKx1l6z/1yLd2Hz58+3oaGhdvz48XbixIm2du3adty4cfbbb7+1gwcPtv369bN9+vSxOTk5dsiQIXbw4MH2mWeesf/6178KzeuJJ56w7du3t9Za27ZtW2uttQkJCTYyMtJ26tTJHj9+3L744os2OjraDh482M6bN6/A9SdNmmSjoqLs8OHDrbXW/u1vf7NTp0614eHh9tChQ3bLli22b9++NjIy0r7++us2LS3N9uvXz/bt29cOGzbMWmvtoEGDbP/+/e2UKVPs/v377cMPP2xjYmJsu3btbG5urtdtcLFtVFSvvfaaXbFihbXW2pCQEPffw8PDbWpqqk1LS7M9evSwmzdvtqNHj7bWWhsZGWkPHjxYaJpjx47Zxx9/3A4ZMsTGx8dba6196aWXbKdOnWyfPn1sRkaGtdba6dOn25kzZ9pXXnmlwLI89dRTNicnx06aNMlu2LDBWmtt8+bNrbXWPvPMM9Zaa9955x37+uuvu6+zbNkym5CQYK21dt++fTYuLs62bt3aWmvtq6++alu3bm3Dw8PtsWPH3Nf56aefbGhoqNf1PHXqlA0LC3OvZ3p6uh02bJjdvXu3PXLkiB08eLC11trFixfbuXPn2oEDB/7X2/xSjZ03Fxr4HyShAAARoElEQVTPZ5991p47d84eO3bMduvWzf7www82OjraWuva/w4ePFhomvy6du1qz58/b8+dO2f79u1rQ0ND7alTp+z+/fvtQw89ZDt37mw3bdpU4Dp9+/a1Bw8etKNHj7bbt2+358+ft+3bt7dZWVm2Q4cO1lprX3nlFfdYW2vtli1b3PvZPffcY0NDQ+3q1auttdampqbaoUOHusc3z8mTJ+1zzz1njx8/bp977jlrrbVxcXF248aN1lpr9+/fX2icunTpYk+dOvUbtvDVGb88W7ZssdOmTfuvxi///u55f9ywYYOdMWOGtdbaTp062aysrELbed26dfbJJ5+0Xbp0sd9//7211tpHHnnEdu3a1S5YsMBaW7THizz9+/e3u3fv9roO3u5rnuvguQ/+N5yx85o7xf6V4qXoPgUIDAwkODiY1atXu//2/vvvM3PmTMLDw3nrrbcAV49mXFxcgVcK4Dr38f7776dv374A+Pv7079/fyIjI3nttdfcNUzXX389ycnJhbpUt2/fTqlSpZg6dSoDBgwA4Pbbb2fChAlUqVKFH3/88XJsPrcLdS169igWpWvRWy+mZ+/kzp07OX/+PLVr1y6wHJs2beK+++7Dx8fHaydiq1atiIqKYuPGje76KPhPR6m3XsxOnTqxZMkSoqKiGD9+vPvv8+fPp0uXLl7X01v/p2e3p7fux2vFhcbTs8PUWxeq5zRQuJfWs8P0Qj2iu3fvJisri+rVq7uXKa/5J3+hd16fLBTsMAVITk5m3rx5TJ8+HSjc25knrxezYsWK1KlTh/79+7Nz584C+0lx8Wtdtvl7hYs6flBwf/e8P3rrKPXczsHBwbz33nvExcXx4osvAvDxxx8zb948Vq9ezcmTJ4v0eAFw9uxZ9u/fzx133OF1HTzva97W4UI9ur9XsQ/FS9V9CtC9e3eSkpI4f/488J8C6PwbPa/ay1rL7t276d+/P8uWLSMuLo477riDrl27kp6eXmC5jDGcO3eOfv36ERsby+TJkwt1qfr4+BSqCbuS3akX6lr07FEsSteit15Mzx7Fjz76iL179xIfH8+7777L8ePHgYJF3t46EUNDQ3nllVe49957ufPOO4GCHaXeejG99Zpaa1m7di2PPvqo1/X01v/p2e3prfvxWnGh8fTWYerZheo5DRTupfXsMPXWI7pjxw4mTZrkrt/LW6a85alYsaJ73PP6ZD07TMHVwenj44Ofn5/X3k7r0YsJEB0dzbRp06hRowZ33HHHZdvOl8uvddnm7xUu6vh57u+e9wnPjtKAgIBC2znvOuXLl3c/HuX/29mzZ4v0eAGub9fI+8YOb+vgeV/ztg7eenQvhWv6PMWiuBTdp3l8fX3p2LGj+9lR48aN6du3L6mpqUydOtX93leeO++8k2nTpgEwfvx4jh8/ToUKFdxfuTJs2DC+//57pk6dyuOPP05UVBRVqlShVq1ahIaGFupSzczMJCYmhpo1a17x7+a7UNeiZ48iXLxrsXr16oV6MT17J/O+bmjdunXs2LGDSpUqcerUKU6cOEGtWrUAvPYxTps2je+++w4fHx/3K4f8HaXeejHnzJnDli1bOHHihPsZbt77I3kP5p7rcPPNNxfq//Ts9gwJCSnU/XituNB4eusw9exC9ZzGWy+tZ4epZ49obm4uTZo0oWnTpvTt25fhw4fTrVs3hg4diq+vL+Hh4ZQqVYr77ruPfv36kZWVRWRkJAMHDizQYXrPPfcQFxcHwCOPPEKZMmUK9XbOmDGjUC/miBEjOH78OJUrV6ZevXp89913jBo1ip07dzJnzhx69OjBlClT+OKLL+jfvz+jR4++pr7a7ULj59krXKNGjYuOHxTe3z3vjzfccEOBjtKgoKBC23np0qW8//77pKWl0adPH1JTU+nXrx9+fn7ugu6iPF6A6xs48r6qz9s6eOvR9VwHz33wUinW5ylezu7T3+tKfcGwzpUqvjR2xZvGr/hSzZsHdZ/KtUBjV7xp/IovheIfmO6YxZfGrnjT+BVf6j79A1P/YvGlsSveNH7Fl7pP/8D0bLX40tgVbxq/4kvdpyIiIkWgUBQREXFc0+8p/prMzEwGDBjAkSNHSE1NpU6dOsTExHDLLbe4p/m10yKu1CkTxVlGRgaRkZGUKlWKRx55xN0scfToUcaNG4e1lnbt2vHQQw8Brg7QwMBAJk2aBLgagXr37s2ePXtYt24dI0aMoE6dOrRr145HHnkEcJ3fuXnzZvdYfP311zRu3Jh9+/Zx5swZRowYwalTp7jtttsYNWoUEyZMYN++fezatYuOHTsSHBzsPjn8ww8/ZO/evSQlJfHxxx9z9uxZRo4cSd26denVqxc+Pj7cfPPNDBw4kLVr17JgwQJycnKYOHEi586do2XLljRo0IAmTZrQunVrhg8fzk8//YSPjw+TJ0/mq6++4s033yQnJ4ddu3bx+eef07RpU2rWrFlgva8VFxq/5cuXs2bNGlJSUhgxYgQNGjTglltuoUmTJtx333306NGDsLAwfH198fX1Zfr06ZQuXZqMjAwefvhhRo0axdNPP039+vW57777qFmzJs8//zyxsbF88803lC9fnpEjR1K1alUmTpxISkoK//M//8OAAQOIiYnh5MmTpKWlsWDBAj7++GNWrVrFTz/9RO/evXn88ccZNmwYixcvZsuWLQQGBrJ+/XoSEhIoW7YsnTp1Ijg4mF69egGusoDJkydjjOHo0aM89NBDrFixgrvvvpuhQ4dy5swZAgICGD9+POPHj2fv3r38+OOPzJo1i5ycnEJjLpdebm4uI0aMID09nfvvv5+DBw+yf/9+UlNTeeWVV9izZ0+hxwbPfcvzMadBgwZ07tyZMmXKcO7cOebPn1+o/OS3Krah6O/vT0JCgvvk7+7du9OzZ0+CgoK4/vrradGiBbt27SI2Npbo6GgmTJhAWloa9erVo1u3bld78YuFpUuX0qZNG5o3b07btm3dD6qTJk2ibNmy7iYMcDVU3H///ezevRuAtLQ01q1bx7333gu4juEHBgZy9uxZ93W+/PJL90n84Gr/SUxMpGnTpoCrbSOvqSKv8ivvhOD27dvTtm1bypUrR0JCAlu3bnW36GzcuJG5c+eyfft2vvjiC06dOkWdOnWIioqic+fOnDt3joSEBN566y127dpFUlISnTt3JjAwkDNnzlCjRg3A1ciyfPlyFi1axNKlS90PyMuXL+eBBx4AcBc1VKlS5fIMwu9wofFr2bIlLVu2JDk5mQ0bNtCgQQMCAwPJzMx0N6D4+/uTk5NDuXLlKFmyJABxcXGEhIS455/3gJR30ruvry+lSpWiZMmSlCtXjuTkZD777DPuuOMO9zjnnUw+depUtm7d6l6W1NRUBg0axOOPP87YsWPddWDg2rcmTJjAjTfeSEhICHXq1CE7O5ukpCQmTJjAZ599RsOGDZk4cSLPPvss4GrIyc7OZsaMGcTExJCSkuIum162bBlr164lODi40JjLpfe///u/HD58mAoVKlCtWjVWrlzJ4sWLeeutt/j6668JCAgo9NjguW95PuZkZmZSunRp5s6dS+/evcnIyKBs2bKXZHn/MIdP33vvPRo3bsz06dPZs2cPt956K7Vr1yY2NhYfHx+ys7OpUKECixYtutqLWmxcqH9x586dhIaGEhsby0svveS1A/RivYmZmZm8+eab7h5D8N5l+Omnn9KkSRPuuusu99+OHj2Kv78/5cqVc/8tfz1cSEgITz75JAMGDKBFixZeex2ttZQoUcLduemtu/NiPavgauaYM2cOR48eZfv27b9ja196Re3PhML9ojNnzmTu3LlUrVqVlStX8tFHH1G7du0C4e/Ze+nZp/ntt99y1113ERcXx6pVq8jMzARc5wlv3ry5wLea5PWWetO3b1/Gjh3LiBEjyMzM9NptOn/+fFq3bo2/vz+A1z5NgNOnT7No0SJatmx5wb5WubS+/fZbHnzwQaZMmcKsWbNo0qQJjz32GAkJCTRs2LBInaqejzneuokvlT9MKFprC308Ou/3lStXUrduXUaPHk12dvbVWLxi6UL9i9WqVaN8+fLuZ3eeHaDbtm27aG/i5s2b+eWXX+jfvz/btm3jyy+/9Npl2LBhQz788EM+/fRTdydtUlKS+5UjwJkzZzhy5Ii7vi8hIYGNGzeyZMkSJk6cWKjXsWLFipQoUYLc3Fx356a37s6L9axC4Q7Ja0lR+zOhYL9obm5uofVau3YtX375JQsXLmTu3LkFpsnrvfTWp1m+fHnAtV2zsrI4fPgwMTExzJw5Ex8fH6+9pZ5uu+02EhISeP75591HAzy7TTdt2sTixYtZs2YNs2fP9tqnmZ6eTq9evZgwYQJly5b1OuZy6eXfD3x8fFi5ciWffPIJY8eOJSkpqUidqp6POd66iS+VYn9KRt7h0/DwcCIiIqhYsSLlypVjxIgR9OrVi6CgINq3b8+4ceOoV68e7733HuvWrfvDvKd4OT8WnpGRQZ8+ffDz86Nhw4asWbOG119/nV27djFhwgSMMYSHh7vfU8zrAM3/3lreds7fm9irVy/3e4r5p8kTFhZGfHw8+/fvZ86cOZw/f57KlSsTGxuLtZamTZuyZs0a9/Tz58+ndOnSdOjQAXB9R9/u3btJT08nPDycv//97+731mrUqMHAgQP55JNPeOONN8jOziYuLo69e/e6uzubN29Ou3btCvWslihRghdffJEnnnjC/SqnS5cuBAQEkJOTw+zZs/+r9zUu90f6LzR+nv2ZlStXdveL3nPPPfTv35+BAweSmZlJamoqiYmJ7iL8vGrFhx56qEDv5fjx4732afbp04fSpUtTtmxZRo0axV//+lduv/12goKC6NWrF+vXr+fVV1/lgQcecH8f45QpU5g9ezbBwcGMHj2aw4cPk5SURHp6OiNHjuTOO+8s0G06atQo9zrHxsbSpk0b7r77bp5//nmysrIoXbo048aNo1WrVmRnZ3PTTTcREhJCyZIlC435f0OnZBTNmTNniIqKIiAggDvvvJOUlBTS0tL4+eefGTlyJHv37i3w2FC3bt1C+5bnY07e22Dly5fn+PHjzJ8/372PFoUabf7AdMcsvjR2xZvGr/jSeYoiIiJFoFAUERFxXNOnZPj5+R0zxlx7n3W/hqgftvjS2BVvGr/iy8/P79iFLrum31MUERG5kvQsR0RExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBwKRREREYdCUURExKFQFBERcSgURUREHApFERERh0JRRETEoVAUERFxKBRFREQcCkURERGHQlFERMShUBQREXEoFEVERBz/H/ZmQ/NKTU4SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35000000000000003\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41000000000000003\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47000000000000003\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.5700000000000001\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.6900000000000001\n",
      "0.7000000000000001\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.8200000000000001\n",
      "0.8300000000000001\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.9400000000000001\n",
      "0.9500000000000001\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n",
      "AUC =  0.9560470869729373\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result1 = result(test_predict_label_addr, test_label_addr, 0.01)\n",
    "print(result1.pred_label.shape)\n",
    "print(result1.true_label.shape)\n",
    "result1.confusion_matrix()\n",
    "# print(result1.tp)\n",
    "# print(result1.tn)\n",
    "# print(result1.fp)\n",
    "# print(result1.fn)\n",
    "# print(result1.ap)\n",
    "# print(result1.an)\n",
    "result1.Classification_Accuracy_Metrics()\n",
    "result1.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(685568, 2)\n",
      "(685845, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35000000000000003\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41000000000000003\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47000000000000003\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.5700000000000001\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.6900000000000001\n",
      "0.7000000000000001\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.8200000000000001\n",
      "0.8300000000000001\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.9400000000000001\n",
      "0.9500000000000001\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n",
      "AUC =  0.9630586655056885\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGVxJREFUeJzt3XmUVOWZx/Hvw6oCgkqLCYuAYBL0xKgdlBhEozGKCmpcQLM4MRCT0TmJmsTJwkRcYsToSdSjgqNJzFFDiEsfxOMYNyYqhFYJCoYRcaHjQhuxEQLI8swfbzVV3X2r6wrVdeve/n3OqdO3ql6qnkt1/fqtt977XnN3REQkW7okXYCIiJSfwl1EJIMU7iIiGaRwFxHJIIW7iEgGKdxFRDJI4S4ikkEKdxGRDFK4i4hkULeknrh///4+dOjQpJ5eRCSVnn322XfdvaZUu8TCfejQodTX1yf19CIiqWRmr8dpp2EZEZEMUriLiGSQwl1EJIMU7iIiGaRwFxHJoJLhbma3m9lqM3uxyP1mZr82sxVmtsTMDil/mSIi8lHE6bn/Bji+nftPAEbmLlOBm3e+LBER2Rkl57m7+3wzG9pOk4nA7zycr2+BmfUzs4+5+1tlqlFEKsEdtm3LX7ZuDbfvtlu+zbvvhtsL2zW33XNP6NcvtGtqgoaG6LbbtsFnPwtdu4a2zz8P772Xf5zCdh//ONTWhnYffABz57Z93ubtk04K7QHmz4dnn41+7j32gO98J79Pl10GGzZE79Mpp8Axx4R2CxfCzJnF9+m3v4Vddw1tf/Qj+NvfovfpC1+AadM65jUsUI6DmAYCqwquN+RuaxPuZjaV0LtnyJAhZXhqqWrusHEjrF8fLs2/5Fu3QrdusN9++bZLlsDmzdFvsGHDYPDg0O6tt8Kbptgb7JRToHv30Pbhh0P7qDfYiBHwpS+FdmvWwKxZ0Y+3bRucey4MHx7azp0Ljz8eXWf//nDFFfl9mjo17HdU269/HU49NbR74gm46qri+/TnP0OvXqHtlCnw179G79OJJ8KvfhXarVwJRxxRPAgffBDGjQttf/ADmDEj+jUcMQJefjl/ffjwELJRZsyASy4J2w89BJMnF//daGqC3XcP2xddFP4Popx1FtxzT9h++204++zij/nYY/lwr6uDX/4yut2IES3D/brrYO3a6LZDhuTD/dVX4fbbiz//rFn5cF+wIPyeRNlnn+KPUUblCHeLuC3yrNvuPhOYCVBbW6szc1ejLVvg/ffzl6am0NM5JPdVyurVcO214c2wbl0+uNevD9dvuy30ygAuvhiuvz76efbbD1asyF8fO7b4G+wXvwgBBKFHNmlS8frffx/69s3/u2JvsDPPzIf7P/8JP/xh8cccNy4f7vPnhzCIMnx4y3CfPTv8/0X53Ofy2++8A488Uvz5N2/Ob69cGf4QRnn77fz2tm0tr7e2ZUv07V26tLz07Nny/pqaEGCt23XpAn365Nv17QujRoXbu3Zt29YKYuPgg1veV9j+kIKv8Pr0Ca9968dqbl8YmmPHhj9mUXXutVfLfZo2LfwfR7U94oh8u9Gjw+93sX1qDnaAK68Mn0ai9mnvvYu/LmVUjnBvAAYXXB8EvFmGx5Wd5R56patXh8s770BjY/5y7bWwyy6h7Zlnht7WunVtH+fLX4Y5c8L2hg3Fe3kQPrY32223EA69eoXtbt3yv+StP7kddFD4AxH1Bhs4MN/uYx8LoVzsDdb8UR9CuyFDSofGHnvA978f/dxduoRPDs1OPBEGDIhu1/xHpdkttxQPmAMOyLcbNy58yii2T71759veemvL/6fC9oXhOnQovPlm8SAsDKKrrw5/CC2qn9bKK6+UbgNwwgnhEkexP5at7bMP3H13vLYTJ4ZLHBdfHK/d8OH5P/KljBkTr10HsjBUXqJRGHOf6+4HRtx3InABMB44DPi1u48u9Zi1tbWutWV20tq1oSf3+uvw2mvh59FHw8knh/vvvz//0T/K66/nQ/a00+C++8Ibv1+/cOnbN/wcOzaMS0IIlhtuCB+p+/QJwd2rVwigXr1Cj7w5ZNzjBYaIxGZmz7p7bal2JXvuZnY3cBTQ38wagP8CugO4+y3APEKwrwD+BfzbjpctbaxbF4L7wIK/qyefDM88E4YTWtuyJR/uAwaEEN5777A9YED4WN18Kezp3XZb+EKod+/2A7lXL7j00ni1K9hFEhNntkw734pAbpbMv5etos5q27bwkff558MXhi++GH6+/noYzli3Lj/+uWZNCPZddw1DBvvuGz6GDxnScix3zJjiY76t7bln2XdJRJKT2JK/nV5TU5hJMmBAuP6nP4Vx79Z69Ajf7q9enZ8xcscdodc9YIB6xyISSeFeKevXh6lajz0WZnAsWRKmY914Y7j/wAPDl4WHHgqf/nT+MnJk6LkXGjmy8vWLSKoo3DvavfeGubGPPhp66s26dWs5ZPLJT4bZDSIiZaBwL7cNG8LP5qlmTz0VDhiBME/2uOPCQRGHHdZyOpqGV0SkjBTu5fLcc+HQ5LvuCvPHp04Nt3/1q6FXftJJYdhFRKQCFO47wz2Mn19+ecvDp597Lr/9mc+Ei4hIBSncd9TTT4ejGp9+OlzffXf4xjfgm99sefShiEgCFO47atmyEOz9+8N3vwsXXND28HMRkYQo3OP64ANYtCgs1wlhVb+1a0NPvXl1OxGRKqHT7MWxcGFY2GrCBFiVW924e/ewVKmCXUSqkMK9lJkzw8JZr74aDh4qnKsuIlKlFO7FbNkSjiD91rfCWs8XXhh68Do6VERSQGPuUTZsCGd8uf/+sFjXrbeGMXYRkZRQuEdZvBjmzQtrmT/4YMuVFkVEUkDhHmXMmNBrHzy45TrqIiIpoXBvtnUrLF8ezvsI8U8RJiJShfSFarPvfQ9qa/OLfImIpJjCHcKa6jfcEHrvmrcuIhmgcH/qqbB8AIR118eOTbYeEZEy6Nzh/t57MHly6LFfcgmcc07SFYmIlEXnDXf3cIDSqlXhJBpXXZV0RSIiZdN5w33JEnjggXCi6bvvDmvFiIhkROedCnnQQfDCC2HNmOHDk65GRKSsOm+4A3ziE+EiIpIxnW9Y5o03YM6cMOYuIpJRnS/cf/pTOOMMuOyypCsREekwnSvcly6FO+8MX55qlUcRybDOFe7XXBOGY6ZMgWHDkq5GRKTDdJ5wb2iAu+6CLl3g4ouTrkZEpEN1nnC/8cZwdqUzztDURxHJvFjhbmbHm9lyM1thZpdG3D/EzB43s+fNbImZjS9/qTth/fpwLlQIqz+KiGRcyXnuZtYVuAn4ItAALDKzOndfVtDsJ8Bsd7/ZzEYB84ChHVDvjnGHH/wA6uvhsMOSrkZEpMPFOYhpNLDC3VcCmNk9wESgMNwdaF4rty/wZjmL3Gm9e8OlbT5wiIhkVpxwHwisKrjeALTu/v4M+B8zuxDoBRxblupERGSHxBlzt4jbWh/eORn4jbsPAsYDd5pZm8c2s6lmVm9m9Y2NjR+92h0xYwZceSW8805lnk9EpArECfcGYHDB9UG0HXY5D5gN4O7PALsA/Vs/kLvPdPdad6+tqanZsYo/ii1bQrj/5CdhaV8RkU4iTrgvAkaa2TAz6wFMAupatXkDOAbAzD5FCPcKdc3bMX8+NDaGxcEOPTTpakREKqZkuLv7FuAC4GHgJcKsmKVmNt3MJuSaXQxMMbO/AXcD57pXwcpc990Xfp52GljU6JKISDbFWvLX3ecRpjcW3jatYHsZcER5S9tJ27blw/3UU5OtRUSkwrJ7hGp9PfzjHzBoENTWJl2NiEhFZTfc7703/Dz1VA3JiEink91w/9znwrK+kycnXYmISMVl9zR7EyaEi4hIJ5TdnruISCeWzZ77vfeGxcKOOQb69Uu6GhGRistmz336dDj9dFi8OOlKREQSkb1wX7MGliyBHj20vK+IdFrZC/e//CUMyRx2GOy6a9LViIgkInvh/uST4eeRRyZbh4hIgrIb7uPGJVuHiEiCshXua9fCc89Bt27hICYRkU4qW1MhX3kF9t8fhg+HXr2SrkZEJDHZCveDD4aXXkq6ChGRxGVrWEZERICshXsVnB9ERKQaZCvcr74aevcOJ8QWEenEshXua9fC+vVav11EOr3shTvA7rsnW4eISMKyFe4ffBB+9umTbB0iIgnLVrir5y4iAmQt3NVzFxEBshbu6rmLiABZO0L1kkvgjTdg6NCkKxERSVS2wv2ss5KuQESkKmRrWEZERIAshfvWrTBrFsyenXQlIiKJy86wzLp1MHVqWH7gzDOTrkZEJFHZ6blrpoyIyHbZCffmOe4KdxGRDIV7c89dBzCJiMQLdzM73syWm9kKM7u0SJszzWyZmS01s7vKW2YM6rmLiGxX8gtVM+sK3AR8EWgAFplZnbsvK2gzEvhP4Ah3X2Nme3dUwUVpzF1EZLs4PffRwAp3X+nuHwL3ABNbtZkC3OTuawDcfXV5y4xh/frwU8MyIiKxwn0gsKrgekPutkL7A/ub2VNmtsDMjo96IDObamb1Zlbf2Ni4YxUX87WvwYcfws03l/dxRURSKE64R53WqPXJSrsBI4GjgMnAbWbWr80/cp/p7rXuXltTU/NRay2te3fYbbfyP66ISMrECfcGYHDB9UHAmxFtHnD3ze7+KrCcEPYiIpKAOOG+CBhpZsPMrAcwCahr1eZ+4GgAM+tPGKZZWc5CS7riChgzBupalyYi0vmUDHd33wJcADwMvATMdvelZjbdzCbkmj0M/NPMlgGPA9939392VNGR/v53WLAAmpoq+rQiItUo1toy7j4PmNfqtmkF2w5clLskQwcxiYhsl50jVHUQk4jIdtkJd/XcRUS2y164q+cuIpKhcNewjIjIdtk5WcfZZ8O770LfvklXIiKSuOyE+3XXJV2BiEjVyM6wjIiIbJeNcN+wARYuhJdfTroSEZGqkI1wf+UVOPxwOOWUpCsREakK2Qj35pkymuMuIgJkJdw1x11EpIVshLvmuIuItJCNcNfSAyIiLWQj3NVzFxFpIRvhrp67iEgL2ThC9fzzYfx46IjzsoqIpFA2wr2mRsEuIlIgG8MyIiLSQjbCfcYMmDoVli1LuhIRkaqQjXCfOxdmzYLGxqQrERGpCtkId82WERFpIVvhrnnuIiJAVsJdC4eJiLSQjXBXz11EpIX0h/uHH8KmTdC1K+yyS9LViIhUhfQfxLRpE4wdG7bNkq1FRKRKpD/c+/SB+fOTrkJEpKqkf1hGRETaSH+4b94M69eDe9KViIhUjfSH+yOPQO/eYVVIEREBYoa7mR1vZsvNbIWZXdpOu9PNzM2stnwlltA8x71374o9pYhItSsZ7mbWFbgJOAEYBUw2s1ER7foA/wEsLHeR7dIcdxGRNuL03EcDK9x9pbt/CNwDTIxodzlwDbCxjPWVplPsiYi0ESfcBwKrCq435G7bzswOBga7+9wy1haPFg0TEWkjTrhHHRm0fWqKmXUBrgcuLvlAZlPNrN7M6hvLtTyveu4iIm3ECfcGYHDB9UHAmwXX+wAHAk+Y2WvA4UBd1Jeq7j7T3WvdvbamXKfF05i7iEgbcY5QXQSMNLNhwD+AScDZzXe6exPQv/m6mT0BXOLu9eUttYgpU+Dzn4cxYyrydCIiaVAy3N19i5ldADwMdAVud/elZjYdqHf3uo4usl2jR4eLiIhsF2ttGXefB8xrddu0Im2P2vmyRERkZ6R/4bBZs2DjRjjnHNhzz6SrERGpCukP95//HF59NSw/oHAXEQGysLaMZsuIiLSRnXDXQUwiItulO9w3bQpL/nbvDj17Jl2NiEjVSHe4F/badYo9EZHtshHuGm8XEWkh3eG+cSP06wd77JF0JSIiVSXdUyEPOADWrNEp9kREWkl3z72ZxttFRFrIRriLiEgL6Q733/8e9tsPpk9PuhIRkaqS7nB/+21YuRKampKuRESkqqQ73JvPwqSjU0VEWkh3uGueu4hIpHSHu3ruIiKR0h3u6rmLiERKd7g399wV7iIiLaT7CNUzzoBRo2DEiKQrERGpKukO9298I+kKRESqUrqHZUREJFK6w/2hh+DJJ2Hr1qQrERGpKukdlnGHk08Owb5pE3TtmnRFIiJVI709940bQ7D37Ak9eiRdjYhIVUlvuGuOu4hIUekNd81xFxEpKr3hXnhybBERaSH94a6eu4hIG+kNdy0aJiJSVHqnQh53HKxapfOniohESG+49+wJgwYlXYWISFWKNSxjZseb2XIzW2Fml0bcf5GZLTOzJWb2qJntW/5SRUQkrpLhbmZdgZuAE4BRwGQzG9Wq2fNArbt/GpgDXFPuQtu44w449VS4//4OfyoRkbSJ03MfDaxw95Xu/iFwDzCxsIG7P+7u/8pdXQB0/HjJ4sUh2F97rcOfSkQkbeKE+0BgVcH1htxtxZwHPBR1h5lNNbN6M6tvbGyMX2UUzXMXESkqTrhHTUfxyIZmXwFqgRlR97v7THevdffampqa+FVG0Tx3EZGi4syWaQAGF1wfBLzZupGZHQv8GBjn7pvKU147tPyAiEhRcXrui4CRZjbMzHoAk4C6wgZmdjBwKzDB3VeXv8wIGpYRESmqZLi7+xbgAuBh4CVgtrsvNbPpZjYh12wG0Bv4o5ktNrO6Ig9XPuq5i4gUFesgJnefB8xrddu0gu1jy1xXaWPHwsCBsNdeFX9qEZFql94jVG+5JekKRESqVnoXDhMRkaLSGe5bt4aDl957L+lKRESqUjrDffVqGDYMRrVeBUFERCCt4a613EVE2pXOcNfRqSIi7Up3uKvnLiISKZ3hrgOYRETalc5wV89dRKRd6Q539dxFRCKl8wjVCRPCVMiB7S0rLyLSeaUz3AcPDhcREYmUzmEZERFpVzrD/a674Kc/hRdfTLoSEZGqlM5wnzMHrrgCli9PuhIRkaqUznDXbBkRkXalM9y1toyISLvSGe7quYuItCud4a6eu4hIu9IZ7uq5i4i0K30HMbnDxz8OTU3Qu3fS1YiIVKX0hbsZ/P3vSVchIlLV0jksIyIi7VK4i4hkUPrCfdGi8EXq+PFJVyIiUrXSF+5NTWEq5KZNSVciIlK10hfumuMuIlJS+sJdc9xFREpKX7ir5y4iUlL6wl09dxGRkmKFu5kdb2bLzWyFmV0acX9PM/tD7v6FZja03IVu19xzV7iLiBRV8ghVM+sK3AR8EWgAFplZnbsvK2h2HrDG3UeY2STgF8BZHVEwJ5wA/frBuHEd8vAiIlkQZ/mB0cAKd18JYGb3ABOBwnCfCPwstz0HuNHMzN29jLUGRx4ZLiIiUlScYZmBwKqC6w252yLbuPsWoAnYqxwFiojIRxcn3C3ittY98jhtMLOpZlZvZvWNjY1x6hMRkR0QJ9wbgMEF1wcBbxZrY2bdgL7Ae60fyN1nunutu9fW1NTsWMUiIlJSnHBfBIw0s2Fm1gOYBNS1alMHfD23fTrwWIeMt4uISCwlv1B19y1mdgHwMNAVuN3dl5rZdKDe3euA/wbuNLMVhB77pI4sWkRE2hfrZB3uPg+Y1+q2aQXbG4EzyluaiIjsqPQdoSoiIiUp3EVEMkjhLiKSQQp3EZEMUriLiGSQJTUd3cwagdd38J/3B94tYzlpoH3uHLTPncPO7PO+7l7yKNDEwn1nmFm9u9cmXUclaZ87B+1z51CJfdawjIhIBincRUQyKK3hPjPpAhKgfe4ctM+dQ4fvcyrH3EVEpH1p7bmLiEg7qjrcq+rE3BUSY58vMrNlZrbEzB41s32TqLOcSu1zQbvTzczNLPUzK+Lss5mdmXutl5rZXZWusdxi/G4PMbPHzez53O/3+CTqLBczu93MVpvZi0XuNzP7de7/Y4mZHVLWAty9Ki+E5YVfAYYDPYC/AaNatfkOcEtuexLwh6TrrsA+Hw3sltv+dmfY51y7PsB8YAFQm3TdFXidRwLPA3vkru+ddN0V2OeZwLdz26OA15Kueyf3+UjgEODFIvePBx4inMnucGBhOZ+/mnvu20/M7e4fAs0n5i40EfhtbnsOcIyZRZ3yLy1K7rO7P+7u/8pdXUA4M1aaxXmdAS4HrgE2VrK4DhJnn6cAN7n7GgB3X13hGsstzj47sHtuuy9tz/iWKu4+n4gz0hWYCPzOgwVAPzP7WLmev5rDvTOemDvOPhc6j/CXP81K7rOZHQwMdve5lSysA8V5nfcH9jezp8xsgZkdX7HqOkacff4Z8BUzayCcP+LCypSWmI/6fv9IYp2sIyFlOzF3isTeHzP7ClALjOvQijpeu/tsZl2A64FzK1VQBcR5nbsRhmaOInw6+18zO9Dd3+/g2jpKnH2eDPzG3X9pZmMIZ3c70N23dXx5iejQ/KrmnnvZTsydInH2GTM7FvgxMMHdN1Woto5Sap/7AAcCT5jZa4SxybqUf6ka93f7AXff7O6vAssJYZ9Wcfb5PGA2gLs/A+xCWIMlq2K933dUNYd7Zzwxd8l9zg1R3EoI9rSPw0KJfXb3Jnfv7+5D3X0o4XuGCe5en0y5ZRHnd/t+wpfnmFl/wjDNyopWWV5x9vkN4BgAM/sUIdwbK1plZdUBX8vNmjkcaHL3t8r26El/o1zi2+bxwP8RvmX/ce626YQ3N4QX/4/ACuCvwPCka67APv8ZeAdYnLvUJV1zR+9zq7ZPkPLZMjFfZwOuA5YBLwCTkq65Avs8CniKMJNmMXBc0jXv5P7eDbwFbCb00s8DzgfOL3iNb8r9f7xQ7t9rHaEqIpJB1TwsIyIiO0jhLiKSQQp3EZEMUriLiGSQwl1EJIMU7iIiGaRwFxHJIIW7iEgG/T9AzQTEJ4wr2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result2 = result(test_predict_label_addr2, test_label_addr, 0.01)\n",
    "print(result2.pred_label.shape)\n",
    "print(result2.true_label.shape)\n",
    "result2.confusion_matrix()\n",
    "result2.Classification_Accuracy_Metrics()\n",
    "result2.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1878784, 2)\n",
      "(1879087, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35000000000000003\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41000000000000003\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47000000000000003\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.5700000000000001\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.6900000000000001\n",
      "0.7000000000000001\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.8200000000000001\n",
      "0.8300000000000001\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.9400000000000001\n",
      "0.9500000000000001\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n",
      "AUC =  0.9662581305651756\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGUpJREFUeJzt3X+U1XWdx/Hnm2EGIhEyBkVgGCjcw2iUOqm55WJYC6wLtfkD0mOZxW5m6zl6dg9mq0nnVOK2lobbssnJ2qODuu02x0hPmaYZmIMUAaWNgDrKyqiAIgkM894/Pne8d2bunfsF7r3f+/3e1+Oce77fe+9nvvf9nZn7ms987uf7/Zq7IyIi6TIs7gJERKT0FO4iIimkcBcRSSGFu4hICincRURSSOEuIpJCCncRkRRSuIuIpJDCXUQkhYbH9cLjxo3z5ubmuF5eRCSR1q1b97K7NxZrF1u4Nzc309HREdfLi4gkkpk9G6WdhmVERFJI4S4ikkIKdxGRFFK4i4ikkMJdRCSFioa7ma00sx1mtrHA82Zmt5hZp5ltMLNTSl+miIgciig99+8Dc4Z4fi4wPXNbDPz7kZclIiJHoug8d3d/xMyah2iyAPiBh+v1rTWzsWY2wd23l6hGEalF7uE2LNMHPXgQXnsNenrCek9P//VJk+Dtbw9tn3sOXnih//N9y5EjYfbs7OvcdRe8+ebgdgcPwoc+BO9/f2i3YQOsWpW/XU8P3HJL2DbAV74CGzcOrrGnB84+G667ruzfvlIcxDQReD7nflfmsUHhbmaLCb17mpqaSvDSIinQ2wv79xcOjYkTswG3devggOtbNjbCSSeFdnv2wAMP5G938CCce24IQ4CHHoI1a/IH0THHwLXXZmu9/PKw7XwBe+ml8IlPhHY//3kIsHwh3NMD69fD6NGh7d/9Hfzyl4PbHTwIF14IbW2h3TPPwF/8ReHv4y9+EYITQtB+85v5273rXdDZmb3/938Pr7+ev+2yZdlw37wZvva1wq9/003ZcH/kkfB9zWf8+MLbKKFShLvleSzvVbfdfQWwAqC1tVVX5q4V7oMDxh3Gjs22efbZwgE3cWK4Afzf/4VgKBSECxdCQ0No+z//E7abLzRmzIBFi0K77m644YbCPcIvfxlmzgxtv/e90HvLF1jHHw/33Zfdp5kzYefO/NtduhSuvjq0u+eeUHchO3dmv1ef/WwIsXwuuCDUBvDSS3DeeYW3+e53Z8P9/vtDiOUzbVr/cL/zTti9O3/bD32of81r1hR+/QMHsuuvvQavvpq/XW9vdr2+Pnwf6upg+PBwy10fMSLbtrkZzjgj+3zu8vjj+7/GJz8Zeu4D2w0fDqeemm03cyZ89av5X7uuLhvsEP6wXX55/lqPPbbw96WEShHuXcDknPuTgBdLsF2JU28v7NoV3nTDh4c3C4T7//mf4bmdOwcvf/CD8KYC+NKX4MYb+79B+zQ3h15on/e9L2wjn69/HZYsCeuPPhpCrJBzzw29TYDbbgs9yHw+8YlsuO/ZA8uXF97mZz+bDfdnnim8zZ07+99/4YXCobVvX3a9vj78QcoXLnV14Q9hn3e9C15+OX+7GTOy7UaPho9/PP82hw/vH3Bnn52/TV0dvOMd/etevjz8ccrXPvf1zz4bfvWrwkE4Zky27T33hN+RgW3q6rL/sQBMnTr4e1zIFVeEWxTf/W60di0t4RbFrFnR2pVRKcK9HbjCzNqA04HdGm+vUnv3wvbtofe7fTvMn5/t5d5wQ/g3srs7hMcrr4Q3McDHPhZ6wRCCsC9o89mxI7tulg32vjdr3xv3qKP6f920aaFHmC+Mcns6EybAX/914dCor8+2/fjH4cQT8/eecoNo3Di49db87erqssEOYejhwx/O/9pve1v/fXryyfA9yFdr3/cdwrBEbtgPZcWKaO3Gj4cf/Sha2zlzwi2Kiy6K1m7cuHCLYuAfECkJcx96dMTM7gJmAeOAl4DrgXoAd/+umRnwHcKMmr3Ape5e9Ixgra2trhOHldCePeFDpFGjsr3sjg645prQg3zhhfDvb65t22DKlLB+/vlw7739nx8zJvSCzzknGyp798L114c35Nixg5dNTaEGCEMQZqH3ZflG70TkUJnZOndvLdYuymyZRUWed+ALh1CbHKk77wyf3Hd2hoDeti30tAGuvBK+9a2w3tPTfwihoQGOOy70fidM6P/v/pIl8PnPh95WYyO88539e5d9Ro0KHxxFMTy2k46K1Dy9+6pNb2/4EHDjxvDp/ObN8Kc/hU/f+8Ly1lth7dr+XzdyZOg1535IeeKJsHp19gPJY44p3IPO/eBIRBJP4V4t1q6Fq66C3/8+DLEMtHUrTJ8e1i+9FObNCx+uTZsWhmGOPXZwcI8eDXPnlr10Eak+CvdKeuUVeOyxEOSPPw5nnRXGryH0vPumjh17bJivfOKJ4dbSkp22BrB4ceVrF5FEUbiX28MPQ3t7mJv8u9/1f27fvmy4t7TAz34G731vGPMWETkCCvdS6+oKH0b2TYv74Q9h5cqwPmJEmAN+5plw+unh1qehIcxKEREpAYV7KTz/fDgy8N57w3BLW1s4bBrCvOAJE8Lc6A98YPBcaBGRMlC4H659++DHP4bbbw/DKX3TCkeNCvPN+3z4w+EmIlJBCvfD9dGPhumJEIZb/vZvw/lB5s7NHsQjIhIThXtU3d3ZExdBOGR8165w3pGLLsqez0REpAroMnvFvPFGODfz1KnhJFh9vvAF+O1v4YtfVLCLSNVRz70Q93DipSuvDOdlAdiyJTzedzIoEZEqpZ57Ptu3h2GX884LwX7qqWF8fdUqnQBLRBJB3c+Btm0LV155+eVw+P6NN4YrtQzT30ERSQ6F+0BTpsAHPxjO77JyJUyeXPxrRESqjMIdwoemb7wRLnBgBv/1X+FgI/XWRSShFO67d4czLO7fHy7SO2pU9grqIiIJVdtd0127YPZs+PWvwwWFcy8RJyKSYLUb7nv3hospr1sXzon+6KPZy9OJiCRcbQ7LHDwIl1wSzq0+eXK4MHRTU9xViYiUTG323L/0Jfjv/w4XgL7/fgW7iKRO7YV7b284SKmuLhyB2tISd0UiIiVXe+E+bBjccQd0dOhUvCKSWrUT7r29Yawdwlz2970v3npERMqodsL99tvDZe3WrYu7EhGRsquNcH/1VbjmmhDsnZ1xVyMiUna1Ee433ACvvAKzZsEFF8RdjYhI2aU/3J9+Gm67LYyzf+tbOmWviNSE9If70qXQ0wOXXgrvfW/c1YiIVES6w33LFrjrrnDVpH/5l7irERGpmHSffmDjxnDBjQULdN4YEakpkcLdzOYA3wbqgO+5+zcGPN8E3AGMzbRZ4u6rS1zroZs/H557LpwkTESkhhQdljGzOmA5MBdoARaZ2cBj9r8M3O3uJwMLgdtKXehhO/poOO64uKsQEamoKGPupwGd7r7F3fcDbcCCAW0cODqzPgZ4sXQlHgZ3aGtTj11EalaUcJ8IPJ9zvyvzWK6vABebWRewGvhiSao7XGvWwKJF0Noagl5EpMZECfd8E8MHJuYi4PvuPgmYB/zQzAZt28wWm1mHmXV0d3cferVR3XFHWJ57rua1i0hNihLuXcDknPuTGDzschlwN4C7rwFGAuMGbsjdV7h7q7u3NjY2Hl7FxfT0hFP5Alx8cXleQ0SkykUJ9yeA6WY21cwaCB+Ytg9o8xwwG8DMZhDCvYxd8yH86lfw8stwwgnwnvfEUoKISNyKhru79wBXAA8AfyDMitlkZkvNbH6m2dXA58zsd8BdwKfdYxrsvu++sFywQEMyIlKzIs1zz8xZXz3gsety1jcDf1na0g7TT34Sln/zN/HWISISo3SdfmDPHhg5MsxtP/PMuKsREYlNuk4/cNRRsH49vP461NfHXY2ISGzS1XPvM3p03BWIiMQqPeHuDlu36qAlERHSFO5/+hNMmwbvf3/clYiIxC494f7II2E5dWq8dYiIVIH0hPvjj4elZsmIiKQo3H/zm7A8/fR46xARqQLpCPc33ghXXaqrg5NPjrsaEZHYpSPcn3wSenth5kx429virkZEJHbpCPf168Py1FPjrUNEpEqk4wjVT30KTjkFxoyJuxIRkaqQjnAfMwY++MG4qxARqRrpGJYREZF+kh/uL7wQzt1+441xVyIiUjWSH+4bNkB7O9x/f9yViIhUjeSH+x//GJYtLfHWISJSRZIf7tu2heW0abGWISJSTZIf7lu3hqVOGCYi8pb0hHtzc6xliIhUk2SHu3t2WEY9dxGRtyT7IKZ9+2DePNixA8aOjbsaEZGqkexwHzkSVq2KuwoRkaqT7GEZERHJK9k9966ucKrfiRPDudxFRARIes/9a1+DKVPg1lvjrkREpKokO9w1U0ZEJK9kh3t3d1ged1y8dYiIVJlkh/v+/WE5YkS8dYiIVJl0hHtDQ7x1iIhUmWSH+4EDYalwFxHpJ1K4m9kcM3vKzDrNbEmBNheY2WYz22Rmd5a2zALUcxcRyavoPHczqwOWAx8BuoAnzKzd3TfntJkOXAP8pbvvNLPx5Sq4n7Y22LMHxlfm5UREkiLKQUynAZ3uvgXAzNqABcDmnDafA5a7+04Ad99R6kLzOvPMiryMiEjSRBmWmQg8n3O/K/NYrhOAE8zsMTNba2Zz8m3IzBabWYeZdXT3TWMUEZGSixLulucxH3B/ODAdmAUsAr5nZoNO0+juK9y91d1bGxsbD7XWwf75n+Gaa+DgwSPflohIikQJ9y5gcs79ScCLedr82N0PuPtW4ClC2JePO9x0E3zjGzAs2ZN+RERKLUoqPgFMN7OpZtYALATaB7T5X+BsADMbRxim2VLKQgfpmwZZXw+W758LEZHaVTTc3b0HuAJ4APgDcLe7bzKzpWY2P9PsAeAVM9sMPAT8k7u/Uq6iAU2DFBEZQqRT/rr7amD1gMeuy1l34KrMrTIU7iIiBSV3sFrhLiJSkMJdRCSFkn0lpunTYcKEuKsQEak6yQ33piZ4+um4qxARqUrJHZYREZGCFO4iIimU3HBfswaOPhrmzYu7EhGRqpPccH/zTXj99bAUEZF+khvu+/aFpaZCiogMktxw1zx3EZGCFO4iIimkcBcRSSGFu4hICiX3CNVTT4Vly6ClJe5KRESqTnLD/T3vCTcRERkkucMyIiJSUHLDfeNGaGsLSxER6Se54f6jH8GiRbBqVdyViIhUneSGu2bLiIgUpHAXEUkhhbuISAolN9wPHAhLhbuIyCDJDXf13EVEClK4i4ikUHLDfcUK2LULLrww7kpERKpOck8/MGJEuImIyCDJ7bmLiEhByQ33q6+G2bNh3bq4KxERqTrJHZZ58kl4+GF47bW4KxERqTrJ7blrtoyISEGRwt3M5pjZU2bWaWZLhmh3npm5mbWWrsQCFO4iIgUVDXczqwOWA3OBFmCRmQ26/JGZjQb+EXi81EXmpXAXESkoSs/9NKDT3be4+36gDViQp91XgWXAmyWsrzCFu4hIQVHCfSLwfM79rsxjbzGzk4HJ7n5fCWsbWl+419dX7CVFRJIiymwZy/OYv/Wk2TDgZuDTRTdkthhYDNDU1BStwkLOPRe2b4ejjz6y7YiIpFCUcO8CJufcnwS8mHN/NHAS8LCZARwHtJvZfHfvyN2Qu68AVgC0trY6R+Lb3z6iLxcRSbMowzJPANPNbKqZNQALgfa+J919t7uPc/dmd28G1gKDgl1ERCqnaLi7ew9wBfAA8AfgbnffZGZLzWx+uQss6I9/hGeegd7e2EoQEalW5n5koyOHq7W11Ts6jqBz39AQLtixb59mzIhIzTCzde5e9FiiZB6h6p69EpNmy4iIDJLMcM8Ndss3mUdEpLYlM9x1AJOIyJAU7iIiKaRwFxFJIYW7iEgKJfNiHePHwy9/CcOS+bdJRKTckhnuI0fCWWfFXYWISNVS11dEJIWSGe7btsFVV8F3vhN3JSIiVSmZ4d7VBTffDG1tcVciIlKVkhnumi0jIjKkZIe7zisjIpJXssNdPXcRkbwU7iIiKaRwFxFJoWSG+6hRcMIJMHFi3JWIiFSlZB6h+rGPhZuIiOSVzJ67iIgMSeEuIpJCyQz3f/1XePvb4frr465ERKQqJTPc//xn2LsXenvjrkREpColM9w1FVJEZEgKdxGRFFK4i4ikkMJdRCSFFO4iIimUzCNUL7gAZsyAM86IuxIRkaqUzHD/yEfCTURE8krmsIyIiAwpmT33Bx+El16CWbPg+OPjrkZEpOpE6rmb2Rwze8rMOs1sSZ7nrzKzzWa2wcweNLMppS81x7JlcNFFsGFDWV9GRCSpioa7mdUBy4G5QAuwyMxaBjRbD7S6+0zgXmBZqQvtR7NlRESGFKXnfhrQ6e5b3H0/0AYsyG3g7g+5+97M3bXApNKWOYDCXURkSFHCfSLwfM79rsxjhVwG/DTfE2a22Mw6zKyju7s7epUDKdxFRIYUJdwtz2Oet6HZxUArcFO+5919hbu3untrY2Nj9CoHUriLiAwpymyZLmByzv1JwIsDG5nZOcC1wF+5+77SlFfAgQNhqXAXEckrSs/9CWC6mU01swZgIdCe28DMTgb+A5jv7jtKX+YA6rmLiAypaM/d3XvM7ArgAaAOWOnum8xsKdDh7u2EYZijgHvMDOA5d59ftqo3bQoBP2pU2V5CRCTJIh3E5O6rgdUDHrsuZ/2cEtc1tBEjwk1ERPLS6QdERFIomeE+e3Y49cCf/xx3JSIiVSmZ55Z59NEwY6auLu5KRESqUvJ67u7ZqZD19fHWIiJSpZIX7rnBbvmOrxIRkeSFu+a4i4gUpXAXEUkhhbuISAolb7ZMQwNccgkcdVTclYiIVK3khfsxx8Add8RdhYhIVUvesIyIiBSVvHDfuzecOOzZZ+OuRESkaiUv3DduhJNOgvPPj7sSEZGqlbxw12wZEZGikhfuugqTiEhRyQt39dxFRIpSuIuIpJDCXUQkhRTuIiIplLwjVM85J1ys45hj4q5ERKRqJS/cGxvDTURECkresIyIiBSVvHB/8EG48kpob4+7EhGRqpW8cH/iCbjlFvj1r+OuRESkaiUv3DVbRkSkKIW7iEgKJTfc6+vjrUNEpIolN9zVcxcRKUjhLiKSQskL92OPhRkzYNy4uCsREalakcLdzOaY2VNm1mlmS/I8P8LMVmWef9zMmktd6Fuuvx42b4YLLyzbS4iIJF3RcDezOmA5MBdoARaZWcuAZpcBO9393cDNwI2lLlRERKKL0nM/Deh09y3uvh9oAxYMaLMAuCOzfi8w28ysdGWKiMihiBLuE4Hnc+53ZR7L28bde4DdwDtLUeAgn/kMNDXBz35Wls2LiKRBlLNC5uuB+2G0wcwWA4sBmpqaIrx0HitXHt7XiYjUkCg99y5gcs79ScCLhdqY2XBgDPDqwA25+wp3b3X31kadtldEpGyihPsTwHQzm2pmDcBCYOApGduBT2XWzwN+4e6Deu4iIlIZRYdl3L3HzK4AHgDqgJXuvsnMlgId7t4O3A780Mw6CT32heUsWkREhhbpSkzuvhpYPeCx63LW3wTOL21pIiJyuJJ3hKqIiBSlcBcRSSGFu4hICincRURSSOEuIpJCFtd0dDPrBp49zC8fB7xcwnKSQPtcG7TPteFI9nmKuxc9CjS2cD8SZtbh7q1x11FJ2ufaoH2uDZXYZw3LiIikkMJdRCSFkhruK+IuIAba59qgfa4NZd/nRI65i4jI0JLacxcRkSFUdbhX1YW5KyTCPl9lZpvNbIOZPWhmU+Kos5SK7XNOu/PMzM0s8TMrouyzmV2Q+VlvMrM7K11jqUX43W4ys4fMbH3m93teHHWWipmtNLMdZraxwPNmZrdkvh8bzOyUkhbg7lV5I5xe+BlgGtAA/A5oGdDmcuC7mfWFwKq4667APp8NjMqsf74W9jnTbjTwCLAWaI277gr8nKcD64F3ZO6Pj7vuCuzzCuDzmfUWYFvcdR/hPp8FnAJsLPD8POCnhCvZnQE8XsrXr+aeey1emLvoPrv7Q+6+N3N3LeHKWEkW5ecM8FVgGfBmJYsrkyj7/DlgubvvBHD3HRWusdSi7LMDR2fWxzD4im+J4u6PkOeKdDkWAD/wYC0w1swmlOr1qzncq+vC3JURZZ9zXUb4y59kRffZzE4GJrv7fZUsrIyi/JxPAE4ws8fMbK2ZzalYdeURZZ+/AlxsZl2E60d8sTKlxeZQ3++HJNLFOmJSsgtzJ0jk/TGzi4FW4K/KWlH5DbnPZjYMuBn4dKUKqoAoP+fhhKGZWYT/zh41s5PcfVeZayuXKPu8CPi+u3/TzD5AuLrbSe7eW/7yYlHW/KrmnnvJLsydIFH2GTM7B7gWmO/u+ypUW7kU2+fRwEnAw2a2jTA22Z7wD1Wj/m7/2N0PuPtW4ClC2CdVlH2+DLgbwN3XACMJ52BJq0jv98NVzeFeixfmLrrPmSGK/yAEe9LHYaHIPrv7bncf5+7N7t5M+Jxhvrt3xFNuSUT53f5fwofnmNk4wjDNlopWWVpR9vk5YDaAmc0ghHt3RausrHbgksysmTOA3e6+vWRbj/sT5SKfNs8DniZ8yn5t5rGlhDc3hB/+PUAn8BtgWtw1V2Cffw68BPw2c2uPu+Zy7/OAtg+T8NkyEX/OBvwbsBn4PbAw7porsM8twGOEmTS/BT4ad81HuL93AduBA4Re+mXAPwD/kPMzXp75fvy+1L/XOkJVRCSFqnlYRkREDpPCXUQkhRTuIiIppHAXEUkhhbuISAop3EVEUkjhLiKSQgp3EZEU+n9C0BMqVD6lnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result3 = result(train_predict_label_addr, train_label_addr, 0.01)\n",
    "print(result3.pred_label.shape)\n",
    "print(result3.true_label.shape)\n",
    "result3.confusion_matrix()\n",
    "result3.Classification_Accuracy_Metrics()\n",
    "result3.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1878784, 2)\n",
      "(1879087, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35000000000000003\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41000000000000003\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47000000000000003\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.5700000000000001\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.6900000000000001\n",
      "0.7000000000000001\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.8200000000000001\n",
      "0.8300000000000001\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.9400000000000001\n",
      "0.9500000000000001\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n",
      "AUC =  0.9754716392547088\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXu8JGV5579P9TlnhlEBFY0KKBjBFY0KYcnFJJpoArob2KyXgFExUdlc1Gw07odsdpVgspvouuYiRiESjSai6Epm/UCIF1hFQBlBBxhFRxAZUJEZGC4z59b17B9v9Tw1PX1O90ydc6q66/f9fPpzuqveqnrqW3Xern7fp94yd0cIIcTkk9UdgBBCiLVBFb4QQrQEVfhCCNESVOELIURLUIUvhBAtQRW+EEK0BFX4QgjRElThTwBmdqWZ3Wtm6wZMf23ftOeZ2bbSZzOzN5rZTWb2kJltM7OLzewnVjjGR5nZp4pt3G5mL1+m7KFm9iEzu7t4nVOa90Qze7Dv5Wb25tL+5X3zzxywjWPMbNbMPlKa9l/7lttdrOuwYv7LzOxqM9tlZlcOWGfHzP7UzO4yswfM7AYzO7SYd7qZ3WJmO4t9+pCZHVxa9mlm9vli/lYz+7XSvJ82s8+Y2Q4z+1FxfB5fmn+OmS30xf7kAfGdWbh6bWnaZX3LzZvZjaX5R5nZFcU+f9PMXlCaZ8X+3lnEfaWZPb3vmH/MzO4pXv9Y3mex9qjCH3PM7Cjg5wEHTj2AVfwV8PvAG4FHAccClwD/bmUi3MN5wDzwY8BvAH9brhz6eDewATgKOAl4pZn9JoC7f8/dH957AT8B5MAnS8vfVS7j7h9aIp7ryhPc/X/0rfsvgCvd/Z6iyA7gL4E/XyLuPwF+FvgZ4GDglcBsMe9LwHPc/RDgycAU8KcAZjYF/DPwadIxOAv4iJkdWyz7SOD8wseTgAeAv+/b9sf69vnW8kwzeyTwR8DNffv8wr59vhq4uFTko8ANwKOBPwY+YWaPKea9FPgt0vn3KOAa4MOlZf+0iP3JwI+Tjv05S7gTa4Aq/PHnVcC1wAeBfa5kl8PMjgF+DzjD3T/v7nPuvsvd/9Hdl6rU9hszexjwYuC/u/uD7n4VsJFUIQ7iV4F3FLF8F/gAqWIZxKuALxTlRo3ndOA+4HPLlLEivj1fFu7+WXf/OHDXgPKPBP4z8Dp3v90TN7n7bLHsHaUvDoAu8JTi/b8BngC829277v550hfEK4tlL3P3i939fnffBbwHeM6o+1vwP4G/Bu5ZqkDp4uHDxedjgROAt7n7bnf/JHAj6VgCHA1c5e63unsX+AhwXGmVRwOXFHHvBD4FLPUlL9YAVfjjz6uAfyxeJ5vZj+3Hss8Htrn7V0ZdwMzea2b3LfHavMRixwJdd/9WadrXWf6f3/reP2OJcq+iVCkXPNbMfmhmt5nZu4svnF78BwPnAm9eZtuQKr4fY+9fDsvxE8Ai8BIz+4GZfcvMfq9cwMx+zsx2kq7QX0z6tQB77yulaUvt8y/Qd6UO/GrR5HOzmf1O33ZPAk4E3jdkH14FfNHdbys+Px241d0fKJUpH7eLgKeY2bFmNk264PiXUtnzgH9vZo8svhBfDFw2JAaxiqjCH2PM7OdIP/E/7u5fBb4DLNk2PoBHA9/fn226+++6+6FLvJ65xGIPB3b2TdsJPGKJ8v8CnG1mjzCzp5Cu7jf0FzKzXqX8idLkbwLPBh4P/BLwk8D/Ls1/O/ABd79j+T3lTOAT7v7gkHI9jgAOIX25HQ28BDjHzH65V8DdryqadI4A3gl8txTz3cBbzGzazH4FeC6D9/mZwFuBt5Qmfxx4GvAY4HXAW83sjKJ8B3gv8AZ3z4fsw6tIvxR7DDtu3we+CNwC7CY18fxBqez1wAywvXh1i1hETajCH2/OBP611FTwT+zdrLMITPctMw0sFO+3kyrG1eZBUpt2mYNJV7qDeCOpAvk2qW37o8C2AeXOBD5ZrpTd/QfuvsXd8+JK9b+QKl/M7NnAC0h9BEtiZgeRKq9Bbf9Lsbv4e27R/LGZdAX8ov6C7n4n6UvtouLzAvAfSP0mPyD9+vg4fftcfPldBvy+u3+xtL4t7n5X0Rx0Nalf5iXF7N8FNrv7NUP2+eeAx7H3l+ew4/Y24N8CRwLrSX0Ynzez3hfVxcC3SF8QB5MuSD6CqA1V+GNKUSm9DHhu0YTwA9LV1bPM7FlFse+ROvrKHA3cXrz/HHCEmZ24H9t9n+2bJdN79Tcz9PgWMFX0GfR4Fvs2SwDg7jvc/Tfc/XHu/nTSebpXs9N+VMpONJk8j+Tje4WvPwRebGbX9y3zH0kdtFcOWXeZXnPWqMPPTpE6MtNC7pvd/bnu/mh3P5nU0blnn83sScBngbe7+4f3WdvelPf5+cCvlc6RnwXeZWbv6VvmTOD/9P2iuRl4spmVf4mVj9uzSJ3F29x90d0/SOqkPa40//3u/lCx3vcx4AtQrCHurtcYvoAzSJXSE0lXZr3XF4B3FWVOJjUVnESqAI4FvgH8dmk9f0O6kn4e6ef3euB04OwVjvci0pX6w0gdjjuBpy9R9sdJzU0d4IWkjsan95V5OemLy/qmP69wYqQrzyuAvy/mbehz9b9IV7SP6VvHv5Ku1Pvj6hR+frvwvB6YLs3/AvB+YB2pieVu4PnFvN8oxfUk4P+RKtjess8s1reB9EV0G7CumHc46er4LUv4Oo1U0VpxrO8EzizmHdq3z1cDbwIOKS1/EKkT+5cGrPvawtN64NeKco8p5r0NuIrUrJaROpkfAg4t5l9RnF8HFa/3Al+q+3+nza/aA9DrAA9cahJ414DpLyM1C0wVn3+LdEV2P7AVOBvISuWNlJZ5M7CrqCw+tlRlXCHeR5HSPR8i/fJ4eWnezwMP9u3DXUU8XwNOHrC+y0lXu/3T31Tswy7gjqLCecQSMZ0DfKRv2uGkprCnDCj/atLVc/n1wb5l/4XUFHIr8J9K8/6M1ETzUPH3fODRpfnvBO4tlr2svP2iYvVi3p5Xaf5HSc1zD5L6A964zHG4Enht37QzGPDlWcw7qlhmN6mt/gWleetJHbPfL86v64FTSvOPBv5vEduOws0xdf/vtPllxYERQggx4agNXwghWsLQCt/MLrR0K/hNS8w3M/trS7eDbzazE1Y+TCGEEFUZ5Qr/g8Apy8x/IXBM8ToL+NvqYQkhhFhphlb47v4FUofLUpwG/IMnrgUOLQ/sJIQQohlMrcA6DidlQ/TYVkzb5w5OMzuL9CuADRs2/ORTn/rUXm8+ZpZ6kS2lD+d5Tpal76Pe9HLZ8vze+6rz+7e1FrH0es87nc5+x7JS+92UY1A6T1q134O2VU6mGCUWd3DPMctKy9qeaf2xuFNal2GW5vfK7r0tK6b11p/eZ1lGno8WS29+rH9QrHsfg4i7W0y3Utm915WuXffeb8gw23t+7Hdv2t77Fbcw7L3fcW2c3u997uy9rkGxxPoHxVK+baK3/v5YKI7TInfcceM97t4bwG6/WIkKf9A4IANTf9z9fFI6Gscff7xff33//S7tY+fOdOf6IYccUsv2u12Ym0uvhYX4u7AA8/Pxvve5f9rCAuQ5xT/5vi9I8yF9Lr/vf3W7O3GHLDtkpGV66x60nVHjGVauyjLLlR0Wz9RUcjE/f8iS624L09Ppf2RhoZ7/kSbx+Mfv5IILDr19eMnBrESFv410g0uPIxgwmqBYGWZn4aGHYNeu9Lf3ftcu2L1739fsbPo7N5fez86mSrtXyXe7de9RMF0MArGwsHy5NiAXYjVYiQp/I/B6M7sI+Clgp7sPHZCr99OtzbjD7Ow0990Ht98O992XXjt3wv33x6v3+aGHYHGx7qhXD/f+YX/ai1wEchFUdTG0wjezj5JuVz/M0pOS3kYxIJe7vw+4lDQ+xlbS3Y2/OcqGy222k8ru3fDDH8Ldd8ffe+5Jr+3b4d57odNJz8eYm9tnYMTW0XOxuCgXchHIRZBls8MLLcPQCt/dzxgy30kP0dgveh1R487CAtx5J2zblv5+//tw113ptbN/YNmBrBtepCV0u3LRQy4CuQjyvJqLlWjSaQ0LC/Cd78Att8C3vgW33ZYq+Rb8WBFCTAC1Vfjj0KQzNwc33ww33gg33QRbt658G3r8XH3YkJKTj1wEchHIRbDqTTqrRVObdO6+G669Fq67LlX2q50l0e0etLobGCPkIpCLQC6CPK/morYKv0mjdD7wAFx1FXzuc6m5Zi0xm+C0m/1ELgK5COQiqOqi1W343/wmfPrTcPXV9eU7mzW/aWutkItALgK5KFPNRW0Vfl15+O5wzTXwyU+mjte6yfOZukNoDHIRyEUgF4F7NRetatL5ylfgIx9J2TVNIcvminfKMZaLQC4CuQjM5oYXWoZWXOHffjucfz5s3jy87FqT57qLsIdcBHIRyEWw6nfajjPz8/DRj8KnPtWsMWP2ppnZSvUgF4FcBHIRVHMxsXn43/kOvPOd6caoJtPp7AZ02zjIRRm5COQiyLLdlZafyDz8yy6DCy4Yj5EGlWMcyEUgF4FcBGObh78auMOFF8Ill9QdyeiYNbatac2Ri0AuArkIqrqYmCydPIe/+iv4/OdXdLWrjm4qCeQikItALsqM6Y1XK5ml4w5/8zfjV9lD9dHvJgm5COQikIvAvZqL2rq/V/IK/8Mfhs9+dsVWt6Zk2TxZNl93GI1ALgK5COQiMKvmYeyv8D/7Wbj44hVZVS24d+oOoTHIRSAXgVyUqeZirDttv/1teO97646iGjqZA7kI5CKQi6Cqi9qadKrm4T/0EPz5n49H6uVydDq79+QZtx25COQikIugtXn473lPGrt+3FGOcSAXgVwEchG0Mg//6qvT+PWTgYZ+DeQikItALoIxHR75QJt0ZmfTXbSTQqeTet1X+tGJ44hcBHIRyEVQNVtp7Jp0PvlJuOeeFQ6mRrrd9XWH0BjkIpCLQC6CPK/mYqzutN2xI418OUlk2Zj3Oq8gchHIRSAXgVk1F2PVhn/RRTBXbfz/xuFez5O/mohcBHIRyEWZai7G5sarH/4QPvOZVQqmRtzH6jt3VZGLQC4CuQiquhibPPxLLpnMThvlGAdyEchFIBdBK/LwH3xwMq/uQR1SZeQikItALoKqnbZj8eywz3xm8truhRBirWl8k447XHrpKgdTI53OLJ3ObN1hNAK5COQikIsgy6p5aHyTzte/Dj/4wSoHUyO6bTyQi0AuArkIxnZohVHz8MfxoSb7Q9W82klCLgK5COQimOg8/NlZuOaauqNYXVbwwV9jj1wEchHIxcrR6Dz8r3wlVfqTTJ5P1x1CY5CLQC4CuQjcq7lodKftl7+8BoHUjDqkArkI5CKQi2BiO227Xbj++jUKpka63Zm6Q2gMchHIRSAXQZ5XczHSFb6ZnWJmt5jZVjM7e8D8J5rZFWZ2g5ltNrMXVYoK+MY30g1Xk0/GmNwOsQbIRSAXgVwE1TwMXdrMOsB5wAuB44AzzOy4vmL/Dfi4ux8PnA4MfdLssCadTZuGrWEy0G3jgVwEchHIRbAWQyucBGx191sBzOwi4DRgS6mMAwcX7w8B7hq20jzP2bVrF4uLi+R5zszMDHNzc0xNTdHpdNi8eTdTUwdh1sVskTxfR5bN457hPkWns7vIz83pdObpdteTZQu4p46NND/dhtzpzNLtHoTZImZOnk8X02aAbM+60ra65PkMWTaH+xTunb22lWULS8TS29Yc3e76PelTKZZZut11A2LJAafTmWdq6sEBsZT3u7NPLHvvtw2IpbetBcxYZr9TLEvt98ofg7St/ljAMcuZmbm32NbgYxDbWuoYNHW/Rz/3zLq4Z6xbt33Vzr3etlKnaHPPvbRfM8zM3Ldq596++93Mc69qhT/K74PDgTtKn7cV08qcA7zCzLYBlwJvGLQiMzvLzDaZ2aYdO3YsucEHH4Q77lhy9kTRO7BCLsqkCqFbdxiNQC6Cqh5s2A1QZvZS4GR3f23x+ZXASe7+hlKZNxXrepeZ/QzwAeAZ7r5ku83xxx/vN9xww8B5114Lf/Zn+70vY8nMzH0AzM8fWnMk9SMXgVwEchE8/vH3ccEFj/yqu594IMuP0qSzDTiy9PkI9m2yeQ1wCoC7X2Nm64HDgLuXWulyefibN48Q1YRQtdd9kpCLQC4CuQjcVz9L5zrgGDM72sxmSJ2yG/vKfA94PoCZPQ1YD/xouZUu98vixhtHiGpCyLI5skxDgYJclJGLQC4Cs2oehl7hu/uimb0euBzoABe6+81mdi6wyd03Am8GLjCzPyB14L7ah7QVLXWFf//9cPvt+7kXY4ye5hPIRSAXgVwEVV2MtLS7X0rqjC1Pe2vp/RbgOZUiKfj2t9OQyG3BvVN3CI1BLgK5COSiTDUXjRta4bbb1jiQmlGOcSAXgVwEchFM3CMOv/vdtY2jbjTWdyAXgVwEchGM7Xj4S9G2K3zlFwdyEchFIBdlqrlo1ANQFhbgzjtrCKZGdKNRIBeBXARyEVR10ajx8O+4I42S2SbyfF3dITQGuQjkIpCLwL2ai9o6bQdd4betOQcgy+bJsvm6w2gEchHIRSAXgVk1D41qw29jhe+uYV97yEUgF4FclKnmolFNOm3L0AHdVFJGLgK5COQiqOqiUXn4bbzCV45xIBeBXARyEUxMHv6OHWlYhbahHONALgK5COQimJg8/B/+sO4I6mL4w9zbg1wEchHIRVDNRW0Vfn+TzlxLB8PrdFKv+6JSjeWihFwEchFUzVZqTJPOwkJNgdRM79FoQi7KyEUgF0GeV3PRmDtt21rhZ1lLd3wAchHIRSAXQe95xQdKY9rw51t6X0WbhoIehlwEchHIxcrRmDz8tl7hu0/XHUJjkItALgK5CKq6aEweflsrfOUYB3IRyEUgF8HE5OG3tUlHHVKBXARyEchFULXTtjGDVLS1whdCiLVCTTo10+nM0unM1h1GI5CLQC4CuQiyrJqHxjTptLXC123jgVwEchHIRTC2QysoDz+hp/kEchHIRSAXwdg+8aqftrbhm2mckB5yEchFIBdlxnQsHeXhJ/J8pu4QGoNcBHIRyEXgXs2FOm1rRh1SgVwEchHIRTAxnbZtbdLpdnX10kMuArkI5CKo+munMW34bb3Cb9CtEA1ALgK5COQiqOZCTTo1o9vGA7kI5CKQi0BDK4w5yjEO5CKQi0AugrHNw++nrRW+WbfuEBqDXARyEchFUNWFbryqGd1UEshFIBeBXJQZ0xuvlIefyPN1dYfQGOQikItALgL3ai50hV8zWdZ7evuGWuNoAnIRyEUgF4HZ3PBCy9CYK/y2tuG7N6YbpXbkIpCLQC7KVHMxUlqmmZ1iZreY2VYzO3uJMi8zsy1mdrOZ/dP+BtLWK3z3Du6dusNoBHIRyEUgF0FVD0O/LsysA5wH/DKwDbjOzDa6+5ZSmWOAPwKe4+73mtljh61XefiJXn7x4qJ+rspFIBeBXARrkYd/ErDV3W8FMLOLgNOALaUyrwPOc/d7Adz97mErzfOcXbt2sbi4yOJijtkMMzNzuE/h3qHT2U23exBmXcwWyfN1ZNk87hnuU3vmQ06nM0+3u54sW8A9Peg3zU+PA+t0Zot1LWLm5Pl0MW0GyPq21SXPZ8iyfWOBnCxbWCKW3rbm6HbXY5a+wVIss3S76wbEkgNOpzPP1NSDA2Ip73dnn1j23m8bEEtvWwuYscx+p1iW2u+VPwZpW/2xgGOWMzNzb7GtwccgtrXUMWjqfo9+7pl1cc9Yt277qp17vW3l+XSjz720XzPMzNy3aufevvvdzHOvaoU/SpPO4cAdpc/bimlljgWONbMvmdm1ZnbKoBWZ2VlmtsnMNu3YsWPP9LZe3Sdyqg55OjnIRSAXgVwE1TxYf7bMPgXMXgqc7O6vLT6/EjjJ3d9QKvNpYAF4GXAE8EXgGe5+31Lrffazn+1f+9rXAHjgAXj5yyvtx9iybl364pube1TNkdSPXARyEchF8IQn7OD88x/9VXc/8UCWH6VJZxtwZOnzEcBdA8pc6+4LwG1mdgtwDHDdUistD63Q5iv83k9BIRdl5CKQiyDPq7kYpUnnOuAYMzvazGaA04GNfWUuAX4RwMwOIzXx3LrcSsu/LNqakgmQZfNkWYsFlJCLQC4CuQjMqnkYWuG7+yLweuBy4BvAx939ZjM718xOLYpdDmw3sy3AFcBb3H37qEG0+Qo/dcZo+FeQizJyEchFmWoeRsrid/dLgUv7pr219N6BNxWvkSjfeNXuCl83lfSQi0AuArkIqrpoxHj4ba7wNdZ3IBeBXARyEUzEePhtbsNXh1QgF4FcBHIRrEWn7arT5gpfCCHWikY06Sy2eLjrTmeWTqfak+gnBbkI5CKQiyDLqnlQk07N6PFtgVwEchHIRTC2jzhUHn6iN+6JkIsychHIRVDVRSPyndqcpdP3WIBWIxeBXARysXI04gEoba7w00iFAuSijFwEchG4V3PRiE7bNjfpqEMqkItALgK5CCai07bNV/hpnGwBclFGLgK5CPK8mgu14ddOI26FaAhyEchFIBdBNReNaNJpc4Wv28YDuQjkIpCLQEMrjDnKMQ7kIpCLQC4C5eGPOWYtvs24D7kI5CKQi6Cqi0a04bd5aIX0IHMBclFGLgK5KFPNRSPy8Nt8hV+1132SkItALgK5CNzHNEtHTTqJLJsr3m2oNY4mIBeBXARyEZjNDS+0DI24wm9zlo6e5hPIRSAXgVwEVV00wmS7K/xO3SE0BrkI5CKQizLVXCgPv2aUYxzIRSAXgVwEysMfc5RjHMhFIBeBXARjm4dfps1X+GbdukNoDHIRyEUgF2WquWhElk67K/wW34TQh1wEchHIRTC2N14pDz+R5+vqDqExyEUgF4FcBO7VXNTWaasr/ESWzZNlLf7GKyEXgVwEchGYVfOgNvyacdfQrz3kIpCLQC7KVHPRiCaddlf4jfjObQRyEchFIBdBVRe15+EvLECpdad1KMc4kItALgK5CMY+D7/NV/egHOMychHIRSAXwdjn4be9wq863OlkIReBXARyEYzp8MjlJp020+mkXvc2PxOgh1wEchHIRVA1W6n2Jp025+ADdLvr6w6hMchFIBeBXAR5Xs1F7Xfatr3Cz7KW/8QpIReBXARyEZhVc6E2/Jppc4ZSP3IRyEUgFytH7Xn4qvCn6w6hMchFIBeBXARVXYyUh29mp5jZLWa21czOXqbcS8zMzezEYevsddq2vUlHOcaBXARyEchFsOp5+GbWAc4DfhnYBlxnZhvdfUtfuUcAbwS+PMqGlYefUIdUIBeBXARyEaxFp+1JwFZ3vxXAzC4CTgO29JV7O/AO4A9H2fDi4iK7du3igQcWmZnJyfMZsmwO9yncO3Q6u+l2D8Ksi9kieb6OLJvHPcN9as98yOl05ul215NlC7innz1pfpLT6cwW61rEzMnz6WLaDJD1bau7ZCyQk2ULS8TS29Yc3e76PZ0rKZZZut11A2LJMVso0s58QCzl/e7sE8ve+20DYultawEzltnvFMvaHYO0rf5YIMcsJ8vmi20NPgaxraWOQVP3e/Rzr7e93vKrce71tpXn040+96an76fbnSm2tTrn3r773cxzb2pqxyjV65KM0qRzOHBH6fO2YtoezOx44Eh3//RyKzKzs8xsk5lt2r59O6Ar/E5njiyr9iT6SaHTmaPTma07jEbQ6czS6ei8AMiyObkoMKvmYZQrfBswbU+/uZllwLuBVw9bkbufD5wPcMIJJ/iGDRvIsnI7/oY9ZRcXN+y7giXmD7ohY+/5Dxsyf/RtDY9l2Lb2np/nM3uVGd/9rh7L1NSuvunDtnWgx6BZ+z1oW70x4Ffz3FtuXcO3tW/J1ToGvYeYL7X8WsZS97m3sPCEAfNHZ5QKfxtwZOnzEcBdpc+PAJ4BXFlk3jwO2Ghmp7r7pqVW2svDb/sVvp7mE8hFIBeBXARr8cSr64BjzOxo4E7gdODlvZnuvhM4LAKyK4E/XK6yL6MKX0nGPeQikItALspUczG0wnf3RTN7PXA50AEudPebzexcYJO7bzyQDffy8Nuelpk6zATIRRm5COQiqJqHP9KNV+5+KXBp37S3LlH2eaOsU3n4iV4n5eC2u3YhF4FcBHIRZFm1pIbaB09r+wh4KV1LgFyUkYtALoJekseBUvtYOm2/wq/xoWMNRC4CuQjkIqjmovZHHLa9wtdt44FcBHIRyEWgRxyOOXp8WyAXgVwEchHoEYdjjlm37hAag1wEchHIRVDVRe0PQFGFr5O5h1wEchHIRZkxrfCVh5+o2us+SchFIBeBXATuY5qloyv8RAycphxjuQjkIpCLYC0GT1sV9MSrhHvt3SiNQS4CuQjkokw1F7WbbHuTTm8kQCEXZeQikIugqgvl4deMcowDuQjkIpCLYOzz8DW0gnKMe8hFIBeBXARjn4ff9it8yOsOoEHIRSAXgVwE1VzUnqXT9go/y1rea11CLgK5COQi6D2v+EBRlk7N9B5lJ+SijFwEchG4V3NRW6et8vATWTZPlrX8Z06BXARyEchFYFbNQ61t+O7Qbfld0+4a+rWHXARyEchFmWouam3SaXv7PeimkjJyEchFIBdBVRe15uGrwleOcRm5COQikItgrPPw295+D9Dtrq87hMYgF4FcBHIR5Hk1F7U2jqnCF0KItUNNOjXT6czR6VQbAW9SkItALgK5CGLk0ANDTTo1o5+rgVwEchHIRVC1SafWO21V4Ve/c26SkItALgK5CKq6qLUNX006QgixdtSah68rfHCfrjuExiAXgVwEchFUdVFrp60qfOh0Zul0ZusOoxHIRSAXgVwEWVbNgzpta6bb1cBQPeQikItALoKqA8mpDV8IIVqC8vBrRj9XA7kI5CKQi0BNOmOOHt8WyEUgF4FcBGP7iEPl4SfMWv5Q3xJyEchFIBdBVRe1jjuqJh0w0/M6e8hFIBeBXJQZ02faKg8/keczdYfQGOQikItALgL3ai5G6rQ1s1PM7BYz22pmZw+Y/yYz22Jmm83sc2b2pGHrVJNOIsvmKg+INCnIRSAXgVwEZtU8DK3wzawDnAe8EDgOOMPMjutghcqDAAAKMklEQVQrdgNwors/E/gE8I4R1qsmHSDPp8lz3UkIclFGLgK5CKreaTtKk85JwFZ3vxXAzC4CTgO2RBB+Ran8tcArhq10cXGRubldTE8vYpaT5zNk2RzuU7h36HR20+0ehFkXs0XyfB1ZNo97hvvUnvmQ0+nM0+2uJ8sWcE9S0vw0slynM1usaxEzJ8+ni2kzQNa3re6SsUBOli0sEUtvW3N0u+v3DHKUYpndc/PI3rHkmC3S6fS++fpjKe93Z59Y9t5vGxBLb1sLmLHMfq/1MUjb6o8FHLOy48HHILa11DFo6n6Pfu6l9WXF+bQ6515vW6kybe65Nz39AN3uDFm2uGrn3r773cxzb2rqvmFV67KM0qRzOHBH6fO2YtpSvAa4bNAMMzvLzDaZ2abt27erSYd0glTNrZ0UUr61HmUHvcf66byAlHsuFwmzah7M3YdswF4KnOzury0+vxI4yd3fMKDsK4DXA89192Ubm0444QT/9V+/nquuOuDYJ4KpqV0ALC5uqDmS+pGLQC4CuQge97hd/N3fPeyr7n7igSw/SpPONuDI0ucjgLv6C5nZC4A/ZoTKvofa8MGsW3cIjUEuArkI5CKo6mKUJp3rgGPM7GgzmwFOBzbuHYQdD7wfONXd7x5lw+6uCh+KtjzdWAJyUUYuArkos8o3Xrn7opm9Hrgc6AAXuvvNZnYusMndNwLvBB4OXGxmAN9z91OXW6+ZsahjWHn0u0lCLgK5COQicK/mYqQbr9z9UuDSvmlvLb1/wf5uWFf4iSzrSVD7pFwEchHIRWBWrdKs9U5bVfjg3qk7hMYgF4FcBHJRppqLWsfSUVqmTuYychHIRSAXQVUXesRhzaR8a+Weg1yUkYtALoIsq+ZB4+HXjMb6DuQikItALoKxHQ8flIef0NCvgVwEchHIRTCmwyOrSSfRG0dHKapyUUYuArkIImPpwFCTTs30Bl0SclFGLgK5CPK8mosar/CdIcP4tIIs07deD7kI5CKQi6A3EuqBUuMzbevacrNwt7pDaAxyEchFIBdlqrmosdNWBxHAvdZ+80YhF4FcBHIRVHVRax6+UI5xGbkI5CKQi2Bs8/Br/K5pFOqQCuQikItALoKqnba11bpqwxdCiLVFTTo1kx7rp8e3gVyUkYtALoKqj0NVk07N6LbxQC4CuQjkIhjboRWGPUu3LVTNq50k5CKQi0AuAuXhjzmm7NQ9yEUgF4FcrBw1Vvg6igB5Pl13CI1BLgK5COQicK/mQp22NaMOqUAuArkI5CJQp+2Y0+3O1B1CY5CLQC4CuQjyvJoLteHXjr74ArkI5CKQi6CaixpvvFKTDui28TJyEchFIBfB2A6t4K5vbVCOcRm5COQikItAefhjjpke49NDLgK5COQiqOpCbfg1Y6amrR5yEchFIBdlxvSZtsrDT1TtdZ8k5CKQi0AuAvexzdLRJT5Als0V7zbUGkcTkItALgK5CMzmhhdaBl3h14ye5hPIRSAXgVwEVV2oDb9m3Dt1h9AY5CKQi0AuylRzoTz8mlGOcSAXgVwEchEoD3/MUY5xIBeBXARyEYxxHn5dW24WZt26Q2gMchHIRSAXZaq5UJZOzeimkkAuArkI5CIY4xuvlKUDkOfr6g6hMchFIBeBXATu1VzU2GmrK3yALJsny+brDqMRyEUgF4FcBGbVPIxU4ZvZKWZ2i5ltNbOzB8xfZ2YfK+Z/2cyOGrZO1fcJ90wd2AVyEchFIBdlVnl4ZDPrAOcBLwSOA84ws+P6ir0GuNfdnwK8G/iLYetVk07CfUo3lhTIRSAXgVwEa3Hj1UnAVne/FcDMLgJOA7aUypwGnFO8/wTwHjMzX6bdZmZmgac8ZRewiFmO+wxmc8UOdTDbjftBQBezRdzXYTZffNNPlebnxfT1QO+J7tPF/PWkmGeLsouAF/Nni3Epsr5tdZeJJcdsYYlYetuaGxDL7J62t3IsZjlmDwLz5PljB8RS3u/OgFjK+20DYultqz+W/v1e22MQ29o7FrPdQBf3g4ttLXUM0raWPgbN3O/9OffM7gc6uK/euRf7Pd3ocy/LfgTM4L56596++93Mc+/QQ++iCqNU+IcDd5Q+bwN+aqky7r5oZjuBRwP3lAuZ2VnAWcXHub/8y4fddCBBTyCH0eeqxchFIBeBXARPPdAFR6nwB7W99F+5j1IGdz8fOB/AzDa5+4kjbH/ikYtALgK5COQiMLNNB7rsKD0A24AjS5+PAPp/V+wpY2ZTwCHAjgMNSgghxMozSoV/HXCMmR1tZjPA6cDGvjIbgTOL9y8BPr9c+70QQoi1Z2iTTtEm/3rgctJQbRe6+81mdi6wyd03Ah8APmxmW0lX9qePsO3zK8Q9achFIBeBXARyERywC9OFuBBCtAPdzSCEEC1BFb4QQrSEVa/wV2NYhnFlBBdvMrMtZrbZzD5nZk+qI861YJiLUrmXmJmb2cSm5I3iwsxeVpwbN5vZP611jGvFCP8jTzSzK8zshuL/5EV1xLnamNmFZna3mQ28V8kSf1142mxmJ4y0YndftRepk/c7wJOBGeDrwHF9ZX4XeF/x/nTgY6sZU12vEV38IrCheP87bXZRlHsE8AXgWuDEuuOu8bw4BrgBeGTx+bF1x12ji/OB3yneHwd8t+64V8nFLwAnADctMf9FwGWke6B+GvjyKOtd7Sv8PcMyuPs80BuWocxpwIeK958Anm9mkzjQzlAX7n6Fu+8qPl5LuudhEhnlvAB4O/AOYHYtg1tjRnHxOuA8d78XwN3vXuMY14pRXDhwcPH+EPa9J2gicPcvsPy9TKcB/+CJa4FDzezxw9a72hX+oGEZDl+qjLsvAr1hGSaNUVyUeQ3pG3wSGerCzI4HjnT3T69lYDUwynlxLHCsmX3JzK41s1PWLLq1ZRQX5wCvMLNtwKXAG9YmtMaxv/UJsPoPQFmxYRkmgJH308xeAZwIPHdVI6qPZV2YWUYadfXVaxVQjYxyXkyRmnWeR/rV90Uze4a737fKsa01o7g4A/igu7/LzH6GdP/PM9w9X/3wGsUB1ZurfYWvYRmCUVxgZi8A/hg41d3n1ii2tWaYi0cAzwCuNLPvktooN05ox+2o/yP/7O4L7n4bcAvpC2DSGMXFa4CPA7j7NcB60sBqbWOk+qSf1a7wNSxDMNRF0YzxflJlP6nttDDEhbvvdPfD3P0odz+K1J9xqrsf8KBRDWaU/5FLSB36mNlhpCaeW9c0yrVhFBffA54PYGZPI1X4P1rTKJvBRuBVRbbOTwM73f37wxZa1SYdX71hGcaOEV28E3g4cHHRb/09dz+1tqBXiRFdtIIRXVwO/IqZbQG6wFvcfXt9Ua8OI7p4M3CBmf0BqQnj1ZN4gWhmHyU14R1W9Fe8DZgGcPf3kfovXgRsBXYBvznSeifQlRBCiAHoTlshhGgJqvCFEKIlqMIXQoiWoApfCCFagip8IYRoCarwhRCiJajCF0KIlvD/AX3F9TCbwLmVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result4 = result(train_predict_label_addr2, train_label_addr, 0.01)\n",
    "print(result4.pred_label.shape)\n",
    "print(result4.true_label.shape)\n",
    "result4.confusion_matrix()\n",
    "result4.Classification_Accuracy_Metrics()\n",
    "result4.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  label\n",
      "(685568, 2)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File(test_predict_label_addr, 'r')\n",
    "variables = h5f.items()\n",
    "for var in variables:\n",
    "    name = var[0]\n",
    "    data = var[1]\n",
    "    print (\"Name \", name)  # Name\n",
    "    if type(data) is h5py.Dataset:\n",
    "        test = data.value\n",
    "h5f.close()\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening data\n",
    "h5f = h5py.File(train_data_addr, 'r')\n",
    "variables = h5f.items()\n",
    "for var in variables:\n",
    "    name = var[0]\n",
    "    data = var[1]\n",
    "    print (\"Name \", name)  # Name\n",
    "    if type(data) is h5py.Dataset:\n",
    "        train_data = data.value\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File(train_silence_add_clean_addr, 'r')\n",
    "variables = h5f.items()\n",
    "for var in variables:\n",
    "    name = var[0]\n",
    "    data = var[1]\n",
    "    print (\"Name \", name)  # Name\n",
    "    if type(data) is h5py.Dataset:\n",
    "        train_silence_add_clean = data.value\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] run place\n",
    "# opening data\n",
    "h5f = h5py.File(train_data_addr, 'r')\n",
    "variables = h5f.items()\n",
    "for var in variables:\n",
    "    name = var[0]\n",
    "    data = var[1]\n",
    "    print (\"Name \", name)  # Name\n",
    "    if type(data) is h5py.Dataset:\n",
    "        train_data = data.value\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File(train_label_addr, 'r')\n",
    "variables = h5f.items()\n",
    "for var in variables:\n",
    "    name = var[0]\n",
    "    data = var[1]\n",
    "    print (\"Name \", name)  # Name\n",
    "    if type(data) is h5py.Dataset:\n",
    "        train_label = data.value\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "1207379.0\n"
     ]
    }
   ],
   "source": [
    "print(train_label)\n",
    "print(np.sum(train_label)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length silence add data train = 1879087\n",
    "# length silence add data test = 685845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_and_name_extraction(timit_addr, label_directory, name_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-491c89a499c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "w = np.empty((0,0))\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = [[  66386,  174296,   75131,  577908,   32015],\n",
    "        [  58230,  381139,   78045,   99308,  160454],\n",
    "        [  89135,   80552,  152558,  497981,  603535],\n",
    "        [  78415,   81858,  150656,  193263,   69638],\n",
    "        [ 139361,  331509,  343164,  781380,   52269]]\n",
    "\n",
    "columns = ('Freeze', 'Wind', 'Flood', 'Quake', 'Hail')\n",
    "rows = ['%d year' % x for x in (100, 50, 20, 10, 5)]\n",
    "\n",
    "values = np.arange(0, 2500, 500)\n",
    "value_increment = 1000\n",
    "\n",
    "# Get some pastel shades for the colors\n",
    "colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))\n",
    "n_rows = len(data)\n",
    "\n",
    "index = np.arange(len(columns)) + 0.3\n",
    "bar_width = 0.4\n",
    "\n",
    "# Initialize the vertical-offset for the stacked bar chart.\n",
    "y_offset = np.array([0.0] * len(columns))\n",
    "\n",
    "# Plot bars and create text labels for the table\n",
    "cell_text = []\n",
    "for row in range(n_rows):\n",
    "    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])\n",
    "    y_offset = y_offset + data[row]\n",
    "    cell_text.append(['%1.1f' % (x/1000.0) for x in y_offset])\n",
    "# Reverse colors and text labels to display the last value at the top.\n",
    "colors = colors[::-1]\n",
    "cell_text.reverse()\n",
    "\n",
    "# Add a table at the bottom of the axes\n",
    "the_table = plt.table(cellText=cell_text,\n",
    "                      rowLabels=rows,\n",
    "                      rowColours=colors,\n",
    "                      colLabels=columns,\n",
    "                      loc='bottom')\n",
    "\n",
    "# Adjust layout to make room for the table:\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2)\n",
    "\n",
    "plt.ylabel(\"Loss in ${0}'s\".format(value_increment))\n",
    "plt.yticks(values * value_increment, ['%d' % val for val in values])\n",
    "plt.xticks([])\n",
    "plt.title('Loss by Disaster')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
